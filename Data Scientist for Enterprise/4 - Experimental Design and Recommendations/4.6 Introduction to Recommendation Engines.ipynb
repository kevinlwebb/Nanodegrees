{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations are being used to recommend everything from movies to music to friends to new destinations. There are three main methods for implementing recommendations that you will become familiar with throughout this lesson:\n",
    "- Knowledge Based Recommendations\n",
    "- Collaborative Filtering Based Recommendations\n",
    "- Content Based Recommendations\n",
    "\n",
    "After completing this lesson, you will be ready for the upcoming lessons where you will\n",
    "- Learn about more advanced techniques.\n",
    "- Deploy your recommendations in a web application.\n",
    "\n",
    "These three lessons will aim to be extremely practical. The lessons will require that you write code to implement a number of different recommendation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Example Recommendation Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Recommendations\n",
    "- LinkedIn and Facebook\n",
    "\n",
    "Both LinkedIn and Facebook have recommendations for connections (business of friends) similar to what is shown below.\n",
    "\n",
    "- AirBnB Experiences and Destinations\n",
    "\n",
    "AirBnB uses recommendations to determine experiences and destinations for their users.\n",
    "\n",
    "- Walmart, Amazon, and Other Retailers\n",
    "\n",
    "As humans on the Internet, we all get pinged with constant recommendations from retailers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 What's Ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Recommendations\n",
    "In this lesson, you will be working with the MovieTweetings data to apply each of the three methods of recommendations:\n",
    "1. Knowledge Based Recommendations\n",
    "2. Collaborative Filtering Based Recommendations\n",
    "3. Content Based Recommendations\n",
    "\n",
    "Within Collaborative Filtering, there are two main branches:\n",
    "1. Model Based Collaborative Filtering\n",
    "2. Neighborhood Based Collaborative Filtering\n",
    "\n",
    "In this lesson, you will implement Neighborhood Based Collaborative Filtering. In the next lesson, you will implement Model Based Collaborative Filtering.\n",
    "\n",
    "### Similarity Metrics\n",
    "In order to implement Neighborhood Based Collaborative Filtering, you will learn about some common ways to measure the similarity between two users (or two items) including:\n",
    "1. Pearson's correlation coefficient\n",
    "2. Spearman's correlation coefficient\n",
    "3. Kendall's Tau\n",
    "4. Euclidean Distance\n",
    "5. Manhattan Distance\n",
    "\n",
    "You will learn why sometimes one metric works better than another by looking at a specific situation where one metric provides more information than another.\n",
    "\n",
    "### Business Cases For Recommendations\n",
    "Finally, you will look at the four ideas needed for businesses to implement successful recommendations to drive revenue, which include:\n",
    "1. Relevance\n",
    "2. Novelty\n",
    "3. Serendipity\n",
    "4. Increased Diversity\n",
    "\n",
    "At the end of this lesson, you will have gained a ton of skills to build upon or to start creating your own recommendations in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Introduction to MovieTweetings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MovieTweetings\n",
    "If you would like additional information about the MovieTweetings data, you can find more information at the links provided here:\n",
    "- [The MovieTweetings white paper.](http://crowdrec2013.noahlab.com.hk/papers/crowdrec2013_Dooms.pdf)\n",
    "- [A Github account set up for MovieTweetings](https://github.com/sidooms/MovieTweetings)\n",
    "- [A slide deck by Simon Doom about MovieTweetings.](https://www.slideshare.net/simondooms/movie-tweetings-a-movie-rating-dataset-collected-from-twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 MovieTweeting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations with MovieTweetings: Getting to Know The Data\n",
    "\n",
    "Throughout this lesson, you will be working with the [MovieTweetings Data](https://github.com/sidooms/MovieTweetings/tree/master/recsyschallenge2014).  To get started, you can read more about this project and the dataset from the [publication here](http://crowdrec2013.noahlab.com.hk/papers/crowdrec2013_Dooms.pdf).\n",
    "\n",
    "**Note:** There are solutions to each of the notebooks available by hitting the orange jupyter logo in the top left of this notebook.  Additionally, you can watch me work through the solutions on the screencasts that follow each workbook. \n",
    "\n",
    "To get started, read in the libraries and the two datasets you will be using throughout the lesson using the code below.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tests as t\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('https://raw.githubusercontent.com/sidooms/MovieTweetings/master/latest/movies.dat', delimiter='::', header=None, names=['movie_id', 'movie', 'genre'], dtype={'movie_id': object}, engine='python')\n",
    "reviews = pd.read_csv('https://raw.githubusercontent.com/sidooms/MovieTweetings/master/latest/ratings.dat', delimiter='::', header=None, names=['user_id', 'movie_id', 'rating', 'timestamp'], dtype={'movie_id': object, 'user_id': object, 'timestamp': object}, engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Take a Look At The Data \n",
    "\n",
    "Take a look at the data and use your findings to fill in the dictionary below with the correct responses to show your understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of movies is 35216.\n",
      "The number of ratings is 854283.\n",
      "The number of unique users is 66438.\n",
      "The number of missing reviews is 0.\n",
      "The average, minimum, and max ratings given are 7.0, 0, and 10, respectively.\n"
     ]
    }
   ],
   "source": [
    "# number of movies\n",
    "print(\"The number of movies is {}.\".format(movies.shape[0]))\n",
    "\n",
    "# number of ratings\n",
    "print(\"The number of ratings is {}.\".format(reviews.shape[0]))\n",
    "\n",
    "# unique users\n",
    "print(\"The number of unique users is {}.\".format(reviews.user_id.nunique()))\n",
    "\n",
    "# missing ratings\n",
    "print(\"The number of missing reviews is {}.\".format(int(reviews.rating.isnull().mean()*reviews.shape[0])))\n",
    "\n",
    "# the average, min, and max ratings given\n",
    "print(\"The average, minimum, and max ratings given are {}, {}, and {}, respectively.\".format(np.round(reviews.rating.mean(), 0), reviews.rating.min(), reviews.rating.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of genres is 28.\n"
     ]
    }
   ],
   "source": [
    "# number of different genres\n",
    "genres = []\n",
    "for val in movies.genre:\n",
    "    try:\n",
    "        genres.extend(val.split('|'))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "# we end up needing this later\n",
    "genres = set(genres)\n",
    "print(\"The number of genres is {}.\".format(len(genres)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The number of movies in the dataset': 31245, 'The number of ratings in the dataset': 712337, 'The number of different genres': 28, 'The number of unique users in the dataset': 53968, 'The number missing ratings in the reviews dataset': 0, 'The average rating given across all ratings': 7, 'The minimum rating given across all ratings': 0, 'The maximum rating given across all ratings': 10}\n"
     ]
    }
   ],
   "source": [
    "# Use your findings to match each variable to the correct statement in the dictionary\n",
    "a = 53968\n",
    "b = 10\n",
    "c = 7\n",
    "d = 31245\n",
    "e = 15\n",
    "f = 0\n",
    "g = 4\n",
    "h = 712337\n",
    "i = 28\n",
    "\n",
    "dict_sol1 = {\n",
    "'The number of movies in the dataset': d, \n",
    "'The number of ratings in the dataset': h,\n",
    "'The number of different genres': i, \n",
    "'The number of unique users in the dataset': a, \n",
    "'The number missing ratings in the reviews dataset': f, \n",
    "'The average rating given across all ratings': c,\n",
    "'The minimum rating given across all ratings': f,\n",
    "'The maximum rating given across all ratings': b\n",
    "}\n",
    "\n",
    "print(dict_sol1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Cleaning\n",
    "\n",
    "Next, we need to pull some additional relevant information out of the existing columns. \n",
    "\n",
    "For each of the datasets, there are a couple of cleaning steps we need to take care of:\n",
    "\n",
    "#### Movies\n",
    "* Pull the date from the title and create new column\n",
    "* Dummy the date column with 1's and 0's for each century of a movie (1800's, 1900's, and 2000's)\n",
    "* Dummy column the genre with 1's and 0's\n",
    "\n",
    "#### Reviews\n",
    "* Create a date out of time stamp\n",
    "\n",
    "You can check your results against the header of my solution by running the cell below with the **show_clean_dataframes** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull date if it exists\n",
    "create_date = lambda val: val[-5:-1] if val[-1] == ')' else np.nan\n",
    "\n",
    "# apply the function to pull the date\n",
    "movies['date'] = movies['movie'].apply(create_date)\n",
    "\n",
    "# Return century of movie as a dummy column\n",
    "def add_movie_year(val):\n",
    "    if val[:2] == yr:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# Apply function\n",
    "for yr in ['18', '19', '20']:\n",
    "    movies[str(yr) + \"00's\"] = movies['date'].apply(add_movie_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split and return values for columns\n",
    "def split_genres(val):\n",
    "    try:\n",
    "        if val.find(gene) >-1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except AttributeError:\n",
    "        return 0\n",
    "\n",
    "# Apply function for each genre\n",
    "for gene in genres:        \n",
    "    movies[gene] = movies['genre'].apply(split_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie</th>\n",
       "      <th>genre</th>\n",
       "      <th>date</th>\n",
       "      <th>1800's</th>\n",
       "      <th>1900's</th>\n",
       "      <th>2000's</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>...</th>\n",
       "      <th>Action</th>\n",
       "      <th>Music</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Family</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000008</td>\n",
       "      <td>Edison Kinetoscopic Record of a Sneeze (1894)</td>\n",
       "      <td>Documentary|Short</td>\n",
       "      <td>1894</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000010</td>\n",
       "      <td>La sortie des usines Lumière (1895)</td>\n",
       "      <td>Documentary|Short</td>\n",
       "      <td>1895</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000012</td>\n",
       "      <td>The Arrival of a Train (1896)</td>\n",
       "      <td>Documentary|Short</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>The Oxford and Cambridge University Boat Race ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1895</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000091</td>\n",
       "      <td>Le manoir du diable (1896)</td>\n",
       "      <td>Short|Horror</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  movie_id                                              movie  \\\n",
       "0  0000008      Edison Kinetoscopic Record of a Sneeze (1894)   \n",
       "1  0000010                La sortie des usines Lumière (1895)   \n",
       "2  0000012                      The Arrival of a Train (1896)   \n",
       "3       25  The Oxford and Cambridge University Boat Race ...   \n",
       "4  0000091                         Le manoir du diable (1896)   \n",
       "\n",
       "               genre  date  1800's  1900's  2000's  Documentary  Biography  \\\n",
       "0  Documentary|Short  1894       1       0       0            1          0   \n",
       "1  Documentary|Short  1895       1       0       0            1          0   \n",
       "2  Documentary|Short  1896       1       0       0            1          0   \n",
       "3                NaN  1895       1       0       0            0          0   \n",
       "4       Short|Horror  1896       1       0       0            0          0   \n",
       "\n",
       "   Sci-Fi  ...  Action  Music  Film-Noir  Musical  Adult  Family  Fantasy  \\\n",
       "0       0  ...       0      0          0        0      0       0        0   \n",
       "1       0  ...       0      0          0        0      0       0        0   \n",
       "2       0  ...       0      0          0        0      0       0        0   \n",
       "3       0  ...       0      0          0        0      0       0        0   \n",
       "4       0  ...       0      0          0        0      0       0        0   \n",
       "\n",
       "   Thriller  Comedy  Western  \n",
       "0         0       0        0  \n",
       "1         0       0        0  \n",
       "2         0       0        0  \n",
       "3         0       0        0  \n",
       "4         0       0        0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head() #Check what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "change_timestamp = lambda val: datetime.datetime.fromtimestamp(int(val)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "reviews['date'] = reviews['timestamp'].apply(change_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0114508</td>\n",
       "      <td>8</td>\n",
       "      <td>1381006850</td>\n",
       "      <td>2013-10-05 16:00:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0358273</td>\n",
       "      <td>9</td>\n",
       "      <td>1579057827</td>\n",
       "      <td>2020-01-14 21:10:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10039344</td>\n",
       "      <td>5</td>\n",
       "      <td>1578603053</td>\n",
       "      <td>2020-01-09 14:50:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>6751668</td>\n",
       "      <td>9</td>\n",
       "      <td>1578955697</td>\n",
       "      <td>2020-01-13 16:48:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>7131622</td>\n",
       "      <td>8</td>\n",
       "      <td>1579559244</td>\n",
       "      <td>2020-01-20 16:27:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  movie_id  rating   timestamp                 date\n",
       "0       1   0114508       8  1381006850  2013-10-05 16:00:50\n",
       "1       2   0358273       9  1579057827  2020-01-14 21:10:27\n",
       "2       2  10039344       5  1578603053  2020-01-09 14:50:53\n",
       "3       2   6751668       9  1578955697  2020-01-13 16:48:17\n",
       "4       2   7131622       8  1579559244  2020-01-20 16:27:24"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now reviews and movies are the final dataframes with the necessary columns\n",
    "# reviews.to_csv('./reviews_clean.csv')\n",
    "# movies.to_csv('./movies_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Solution MovieTweeting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Ways to Recommend: Knowledge Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Based Recommendations\n",
    "A knowledge based recommendation is one in which knowledge about the item or user preferences are used to make a recommendation.\n",
    "\n",
    "Knowledge based recommendations are pretty common when purchasing luxury items. Take a look at the filters available on Zillow in the image below. This is an example of building in a knowledge based recommendation, as users can add their own preferences to the items that are provided.\n",
    "\n",
    "![Zillow](https://video.udacity-data.com/topher/2018/August/5b6a4153_screen-shot-2018-08-07-at-6.02.41-pm/screen-shot-2018-08-07-at-6.02.41-pm.png)\n",
    "\n",
    "Often a rank based algorithm is provided along with knowledge based recommendations to bring the most popular items in particular categories to the user's attention.\n",
    "\n",
    "In the next concept, you will get some practice implementing this type of recommendation for the MovieTweetings dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Notebook: Knowledge Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations with MovieTweetings: Most Popular Recommendation\n",
    "\n",
    "Now that you have created the necessary columns we will be using throughout the rest of the lesson on creating recommendations, let's get started with the first of our recommendations.\n",
    "\n",
    "To get started, read in the libraries and the two datasets you will be using throughout the lesson using the code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How To Find The Most Popular Movies\n",
    "\n",
    "For this notebook, we have a single task.  The task is that no matter the user, we need to provide a list of the recommendations based on simply the most popular items.\n",
    "\n",
    "For this task, we will consider what is \"most popular\" based on the following criteria:\n",
    "\n",
    "* A movie with the highest average rating is considered best\n",
    "* With ties, movies that have more ratings are better\n",
    "* A movie must have a minimum of 5 ratings to be considered among the best movies\n",
    "* If movies are tied in their average rating and number of ratings, the ranking is determined by the movie that is the most recent rating\n",
    "\n",
    "With these criteria, the goal for this notebook is to take a **user_id** and provide back the **n_top** recommendations.  Use the function below as the scaffolding that will be used for all the future recommendations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Unnamed: 0'\n",
      "'Unnamed: 0'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tests as t\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the datasets\n",
    "# movies = pd.read_csv('movies_clean.csv')\n",
    "# reviews = pd.read_csv('reviews_clean.csv')\n",
    "\n",
    "try:\n",
    "    del movies['Unnamed: 0']\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    del reviews['Unnamed: 0']\n",
    "except Exception as e:\n",
    "    print(e)#### 1. How To Find The Most Popular Movies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How To Find The Most Popular Movies\n",
    "\n",
    "For this notebook, we have a single task.  The task is that no matter the user, we need to provide a list of the recommendations based on simply the most popular items.\n",
    "\n",
    "For this task, we will consider what is \"most popular\" based on the following criteria:\n",
    "\n",
    "* A movie with the highest average rating is considered best\n",
    "* With ties, movies that have more ratings are better\n",
    "* A movie must have a minimum of 5 ratings to be considered among the best movies\n",
    "* If movies are tied in their average rating and number of ratings, the ranking is determined by the movie that is the most recent rating\n",
    "\n",
    "With these criteria, the goal for this notebook is to take a **user_id** and provide back the **n_top** recommendations.  Use the function below as the scaffolding that will be used for all the future recommendations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ranked_df(movies, reviews):\n",
    "        '''\n",
    "        INPUT\n",
    "        movies - the movies dataframe\n",
    "        reviews - the reviews dataframe\n",
    "        \n",
    "        OUTPUT\n",
    "        ranked_movies - a dataframe with movies that are sorted by highest avg rating, more reviews, \n",
    "                        then time, and must have more than 4 ratings\n",
    "        '''\n",
    "        \n",
    "        # Pull the average ratings and number of ratings for each movie\n",
    "        movie_ratings = reviews.groupby('movie_id')['rating']\n",
    "        avg_ratings = movie_ratings.mean()\n",
    "        num_ratings = movie_ratings.count()\n",
    "        last_rating = pd.DataFrame(reviews.groupby('movie_id').max()['date'])\n",
    "        last_rating.columns = ['last_rating']\n",
    "\n",
    "        # Add Dates\n",
    "        rating_count_df = pd.DataFrame({'avg_rating': avg_ratings, 'num_ratings': num_ratings})\n",
    "        rating_count_df = rating_count_df.join(last_rating)\n",
    "\n",
    "        # merge with the movies dataset\n",
    "        movie_recs = movies.set_index('movie_id').join(rating_count_df)\n",
    "\n",
    "        # sort by top avg rating and number of ratings\n",
    "        ranked_movies = movie_recs.sort_values(['avg_rating', 'num_ratings', 'last_rating'], ascending=False)\n",
    "\n",
    "        # for edge cases - subset the movie list to those with only 5 or more reviews\n",
    "        ranked_movies = ranked_movies[ranked_movies['num_ratings'] > 4]\n",
    "        \n",
    "        return ranked_movies\n",
    "    \n",
    "\n",
    "def popular_recommendations(user_id, n_top, ranked_movies):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id (str) of the individual you are making recommendations for\n",
    "    n_top - an integer of the number recommendations you want back\n",
    "    ranked_movies - a pandas dataframe of the already ranked movies based on avg rating, count, and time\n",
    "\n",
    "    OUTPUT:\n",
    "    top_movies - a list of the n_top recommended movies by movie title in order best to worst\n",
    "    '''\n",
    "\n",
    "    top_movies = list(ranked_movies['movie'][:n_top])\n",
    "\n",
    "    return top_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usint the three criteria above, you should be able to put together the above function.  If you feel confident in your solution, check the results of your function against our solution. On the next page, you can see a walkthrough and you can of course get the solution by looking at the solution notebook available in this workspace.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 movies recommended for id 1\n",
    "ranked_movies = create_ranked_df(movies, reviews) # only run this once - it is not fast\n",
    "\n",
    "recs_20_for_1 = popular_recommendations('1', 20, ranked_movies)\n",
    "\n",
    "# Top 5 movies recommended for id 53968\n",
    "recs_5_for_53968 = popular_recommendations('53968', 5, ranked_movies)\n",
    "\n",
    "# Top 100 movies recommended for id 70000\n",
    "recs_100_for_70000 = popular_recommendations('70000', 100, ranked_movies)\n",
    "\n",
    "# Top 35 movies recommended for id 43\n",
    "recs_35_for_43 = popular_recommendations('43', 35, ranked_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MSG 2 the Messenger (2015)', 'Avengers: Age of Ultron Parody (2015)', 'Five Minutes (2017)', 'Selam (2013)', 'Let There Be Light (2017)', \"Quiet Riot: Well Now You're Here, There's No Way Back (2014)\", 'Crawl Bitch Crawl (2012)', 'Chasing Happiness (2019)', 'Make Like a Dog (2015)', 'Pandorica (2016)', 'Third Contact (2011)', 'Romeo Juliet (2009)', 'Be Somebody (2016)', 'Birlesen Gonuller (2014)', 'Kitbull (2019)', 'Agnelli (2017)', 'Sátántangó (1994)', 'Foster (2011)', 'CM101MMXI Fundamentals (2013)', 'Crystal Lake Memories: The Complete History of Friday the 13th (2013)'] \n",
      "\n",
      "['MSG 2 the Messenger (2015)', 'Avengers: Age of Ultron Parody (2015)', 'Five Minutes (2017)', 'Selam (2013)', 'Let There Be Light (2017)'] \n",
      "\n",
      "['MSG 2 the Messenger (2015)', 'Avengers: Age of Ultron Parody (2015)', 'Five Minutes (2017)', 'Selam (2013)', 'Let There Be Light (2017)', \"Quiet Riot: Well Now You're Here, There's No Way Back (2014)\", 'Crawl Bitch Crawl (2012)', 'Chasing Happiness (2019)', 'Make Like a Dog (2015)', 'Pandorica (2016)', 'Third Contact (2011)', 'Romeo Juliet (2009)', 'Be Somebody (2016)', 'Birlesen Gonuller (2014)', 'Kitbull (2019)', 'Agnelli (2017)', 'Sátántangó (1994)', 'Foster (2011)', 'CM101MMXI Fundamentals (2013)', 'Crystal Lake Memories: The Complete History of Friday the 13th (2013)', 'Hans Zimmer Live on Tour (2017)', 'Kirik Party (2016)', 'Chasing Asylum (2016)', 'Beyond the Sea (2004)', 'Blood Brother (2013)', 'Poshter Girl (2016)', 'A Matter of Life and Death (1946)', 'The Sky Is Pink (2019)', 'Bridegroom (2013)', 'Mad As Hell (2014)', 'Nashville (1975)', 'The Shawshank Redemption (1994)', 'Dag II (2016)', 'Take the Ball, Pass the Ball (2018)', 'Chasing Coral (2017)'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(recs_20_for_1,\"\\n\")\n",
    "print(recs_5_for_53968,\"\\n\")\n",
    "print(recs_35_for_43,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** This wasn't the only way we could have determined the \"top rated\" movies.  You can imagine that in keeping track of trending news or trending social events, you would likely want to create a time window from the current time, and then pull the articles in the most recent time frame.  There are always going to be some subjective decisions to be made.  \n",
    "\n",
    "If you find that no one is paying any attention to your most popular recommendations, then it might be time to find a new way to recommend, which is what the next parts of the lesson should prepare us to do!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Adding Filters\n",
    "\n",
    "Now that you have created a function to give back the **n_top** movies, let's make it a bit more robust.  Add arguments that will act as filters for the movie **year** and **genre**.  \n",
    "\n",
    "Use the cells below to adjust your existing function to allow for **year** and **genre** arguments as **lists** of **strings**.  Then your ending results are filtered to only movies within the lists of provided years and genres (as `or` conditions).  If no list is provided, there should be no filter applied.\n",
    "\n",
    "You can adjust other necessary inputs as necessary to retrieve the final results you are looking for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popular_recs_filtered(user_id, n_top, ranked_movies, years=None, genres=None):\n",
    "    '''\n",
    "    REDO THIS DOC STRING\n",
    "    \n",
    "    INPUT:\n",
    "    user_id - the user_id (str) of the individual you are making recommendations for\n",
    "    n_top - an integer of the number recommendations you want back\n",
    "    ranked_movies - a pandas dataframe of the already ranked movies based on avg rating, count, and time\n",
    "    years - a list of strings with years of movies\n",
    "    genres - a list of strings with genres of movies\n",
    "    \n",
    "    OUTPUT:\n",
    "    top_movies - a list of the n_top recommended movies by movie title in order best to worst\n",
    "    '''\n",
    "    # Filter movies based on year and genre\n",
    "    if years is not None:\n",
    "        ranked_movies = ranked_movies[ranked_movies['date'].isin(years)]\n",
    "\n",
    "    if genres is not None:\n",
    "        num_genre_match = ranked_movies[genres].sum(axis=1)\n",
    "        ranked_movies = ranked_movies.loc[num_genre_match > 0, :]\n",
    "            \n",
    "            \n",
    "    # create top movies list \n",
    "    top_movies = list(ranked_movies['movie'][:n_top])\n",
    "\n",
    "    return top_movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 movies recommended for id 1 with years=['2015', '2016', '2017', '2018'], genres=['History']\n",
    "recs_20_for_1_filtered = popular_recs_filtered('1', 20, ranked_movies, years=['2015', '2016', '2017', '2018'], genres=['History'])\n",
    "\n",
    "# Top 5 movies recommended for id 53968 with no genre filter but years=['2015', '2016', '2017', '2018']\n",
    "recs_5_for_53968_filtered = popular_recs_filtered('53968', 5, ranked_movies, years=['2015', '2016', '2017', '2018'])\n",
    "\n",
    "# Top 100 movies recommended for id 70000 with no year filter but genres=['History', 'News']\n",
    "recs_100_for_70000_filtered = popular_recs_filtered('70000', 100, ranked_movies, genres=['History', 'News'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hillary's America: The Secret History of the Democratic Party (2016)\", 'I Believe in Miracles (2015)', 'O.J.: Made in America (2016)', 'Ayla: The Daughter of War (2017)', 'Hacksaw Ridge (2016)', 'They Shall Not Grow Old (2018)', 'Namhansanseong (2017)', 'The Farthest (2017)', 'Kono sekai no katasumi ni (2016)', 'Sado (2015)', 'Silicon Cowboys (2016)', '13th (2016)', 'Ethel &amp; Ernest (2016)', 'Paul, Apostle of Christ (2018)', 'Kincsem (2017)', 'LA 92 (2017)', 'Straight Outta Compton (2015)', 'Nise - O Coração da Loucura (2015)', 'Under sandet (2015)', 'Only the Dead (2015)'] \n",
      "\n",
      "['MSG 2 the Messenger (2015)', 'Avengers: Age of Ultron Parody (2015)', 'Five Minutes (2017)', 'Let There Be Light (2017)', 'Make Like a Dog (2015)'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(recs_20_for_1_filtered,\"\\n\")\n",
    "print(recs_5_for_53968_filtered,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Solution: Knowledge Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 More Personalized Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we need to be able to send recommendations without a user telling us exactly what they want or in a more personalized way than simply the top items. Imagine you want to send an email of recommendations or place recommendations within a web page (the side of a blog or as a banner advertisement); in these cases, it is often useful to implement information that we know about users or items to make these recommendations. This leads to some additional recommendation methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 Ways to Recommend: Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "- A method of making recommendations based on using the collaboration of user-item interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering and Content Based Recommendations\n",
    "**Collaborative filtering** is a method of making recommendations based only on the interactions between users and items.\n",
    "\n",
    "Alternatively, **content based recommendations** are when we use information about the users or items to assist in our recommendations.\n",
    "\n",
    "### Examples\n",
    "- When a user is inputting her/his information (location input), this is an example of knowledge based recommending.\n",
    "- When we use connections between users and items (connecting Mike and Pradeep as similar), this is an example of collaborative filtering. \n",
    "- When we use information about the items or users to recommend new items (items related to robotics), this is an example of content based recommending."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 Measuring Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "There are two main ways to implement collaborative filtering:\n",
    "1. Model Based Collaborative Filtering\n",
    "2. Neighborhood Based Collaborative Filtering\n",
    "\n",
    "In this lesson, we will cover Neighborhood Based Collaborative Filtering, which is used to identify items or users that are \"neighbors\" with one another.\n",
    "\n",
    "There are a number of ways we might go about finding an individual's closest neighbors - the metrics we will take a closer look at include:\n",
    "1. Pearson's correlation coefficient\n",
    "2. Spearman's correlation coefficient\n",
    "3. Kendall's Tau\n",
    "4. Euclidean Distance\n",
    "5. Manhattan Distance\n",
    "\n",
    "On the next page, you will work through a few examples to get more familiar with how each of these metrics is computed, and why you might use one over another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 Notebook: Measuring Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Find Your Neighbor?\n",
    "\n",
    "In neighborhood based collaborative filtering, it is incredibly important to be able to identify an individual's neighbors. Let's look at a small dataset in order to understand how we can use different metrics to identify close neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "play_data = pd.DataFrame({'x1': [-3, -2, -1, 0, 1, 2, 3], \n",
    "               'x2': [9, 4, 1, 0, 1, 4, 9],\n",
    "               'x3': [1, 2, 3, 4, 5, 6, 7],\n",
    "               'x4': [2, 5, 15, 27, 28, 30, 31]\n",
    "})\n",
    "\n",
    "#create play data dataframe\n",
    "play_data = play_data[['x1', 'x2', 'x3', 'x4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Measures of Similarity\n",
    "\n",
    "The first metrics we will look at have similar characteristics:\n",
    "\n",
    "1. Pearson's Correlation Coefficient\n",
    "2. Spearman's Correlation Coefficient\n",
    "3. Kendall's Tau\n",
    "\n",
    "Let's take a look at each of these individually.\n",
    "\n",
    "### Pearson's Correlation\n",
    "\n",
    "First, **Pearson's correlation coefficient** is a measure related to the strength and direction of a **linear** relationship.  \n",
    "\n",
    "If we have two vectors x and y, we can compare their individual elements in the following way to calculate Pearson's correlation coefficient:\n",
    "\n",
    "\n",
    "$$CORR(\\textbf{x}, \\textbf{y}) = \\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum\\limits_{i=1}^{n}(x_i-\\bar{x})^2}\\sqrt{\\sum\\limits_{i=1}^{n}(y_i-\\bar{y})^2}} $$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i$$\n",
    "\n",
    "1. Write a function that takes in two vectors and returns the Pearson correlation coefficient.  You can then compare your answer to the built in function in NumPy by using the assert statements in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    corr - the pearson correlation coefficient for comparing x and y\n",
    "    '''\n",
    "    # Compute Mean Values\n",
    "    mean_x, mean_y = np.sum(x)/len(x), np.sum(y)/len(y) \n",
    "    \n",
    "    x_diffs = x - mean_x\n",
    "    y_diffs = y - mean_y\n",
    "    numerator = np.sum(x_diffs*y_diffs)\n",
    "    denominator = np.sqrt(np.sum(x_diffs**2))*np.sqrt(np.sum(y_diffs**2))\n",
    "        \n",
    "    corr = numerator/denominator\n",
    "                            \n",
    "    return corr                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, it looks like you are all set!  Nice job coding up Pearson's correlation coefficient!\n"
     ]
    }
   ],
   "source": [
    "# This cell will test your function against the built in numpy function\n",
    "assert pearson_corr(play_data['x1'], play_data['x2']) == np.corrcoef(play_data['x1'], play_data['x2'])[0][1], 'Oops!  The correlation between the first two columns should be 0, but your function returned {}.'.format(pearson_corr(play_data['x1'], play_data['x2']))\n",
    "assert round(pearson_corr(play_data['x1'], play_data['x3']), 2) == np.corrcoef(play_data['x1'], play_data['x3'])[0][1], 'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'.format(np.corrcoef(play_data['x1'], play_data['x3'])[0][1], pearson_corr(play_data['x1'], play_data['x3']))\n",
    "assert round(pearson_corr(play_data['x3'], play_data['x4']), 2) == round(np.corrcoef(play_data['x3'], play_data['x4'])[0][1], 2), 'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'.format(np.corrcoef(play_data['x3'], play_data['x4'])[0][1], pearson_corr(play_data['x3'], play_data['x4']))\n",
    "print(\"If this is all you see, it looks like you are all set!  Nice job coding up Pearson's correlation coefficient!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"If when x increases, y always increases, Pearson's correlation will be always be 1.\": False, \"If when x increases by 1, y always increases by 3, Pearson's correlation will always be 1.\": True, \"If when x increases by 1, y always decreases by 5, Pearson's correlation will always be -1.\": True, \"If when x increases by 1, y increases by 3 times x, Pearson's correlation will always be 1.\": False} \n",
      "\n",
      "That's right!  Pearson's correlation relates to a linear relationship.  The second and third cases are examples of perfect linear relationships, where we would receive values of 1 and -1.  Only having an increase or decrease that are directly related will not lead to a Pearson's correlation coefficient of 1 or -1.  You can see this by testing out your function using the examples above without using assert statements.\n"
     ]
    }
   ],
   "source": [
    "a = True\n",
    "b = False\n",
    "c = \"We can't be sure.\"\n",
    "\n",
    "\n",
    "pearson_dct = {\"If when x increases, y always increases, Pearson's correlation will be always be 1.\": b,\n",
    "               \"If when x increases by 1, y always increases by 3, Pearson's correlation will always be 1.\": a,\n",
    "               \"If when x increases by 1, y always decreases by 5, Pearson's correlation will always be -1.\": a,\n",
    "               \"If when x increases by 1, y increases by 3 times x, Pearson's correlation will always be 1.\": b\n",
    "}\n",
    "\n",
    "print(pearson_dct, \"\\n\")\n",
    "print(\"That's right!  Pearson's correlation relates to a linear relationship.  The second and third cases are examples of perfect linear relationships, where we would receive values of 1 and -1.  Only having an increase or decrease that are directly related will not lead to a Pearson's correlation coefficient of 1 or -1.  You can see this by testing out your function using the examples above without using assert statements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman's Correlation\n",
    "\n",
    "Now, let's look at **Spearman's correlation coefficient**.  Spearman's correlation is what is known as a [non-parametric](https://en.wikipedia.org/wiki/Nonparametric_statistics) statistic, which is a statistic who's distribution doesn't depend parameters (statistics that follow normal distributions or binomial distributions are examples of parametric statistics).  \n",
    "\n",
    "Frequently non-parametric statistics are based on the ranks of data rather than the original values collected.  This happens to be the case with Spearman's correlation coefficient, which is calculated similarly to Pearson's correlation.  However, instead of using the raw data, we use the rank of each value.\n",
    "\n",
    "You can quickly change from the raw data to the ranks using the **.rank()** method as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ranked values for the variable x1 are: [1. 2. 3. 4. 5. 6. 7.]\n",
      "The raw data values for the variable x1 are: [-3 -2 -1  0  1  2  3]\n"
     ]
    }
   ],
   "source": [
    "print(\"The ranked values for the variable x1 are: {}\".format(np.array(play_data['x1'].rank())))\n",
    "print(\"The raw data values for the variable x1 are: {}\".format(np.array(play_data['x1'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we map each of our data to ranked data values as shown above:\n",
    "\n",
    "$$\\textbf{x} \\rightarrow \\textbf{x}^{r}$$\n",
    "$$\\textbf{y} \\rightarrow \\textbf{y}^{r}$$\n",
    "\n",
    "Here, we let the **r** indicate these are ranked values (this is not raising any value to the power of r).  Then we compute Spearman's correlation coefficient as:\n",
    "\n",
    "$$SCORR(\\textbf{x}, \\textbf{y}) = \\frac{\\sum\\limits_{i=1}^{n}(x^{r}_i - \\bar{x}^{r})(y^{r}_i - \\bar{y}^{r})}{\\sqrt{\\sum\\limits_{i=1}^{n}(x^{r}_i-\\bar{x}^{r})^2}\\sqrt{\\sum\\limits_{i=1}^{n}(y^{r}_i-\\bar{y}^{r})^2}} $$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\bar{x}^r = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x^r_i$$\n",
    "\n",
    "`3.` Write a function that takes in two vectors and returns the Spearman correlation coefficient.  You can then compare your answer to the built in function in scipy stats by using the assert statements in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_spearman(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    corr - the spearman correlation coefficient for comparing x and y\n",
    "    '''\n",
    "    # Change each vector to ranked values\n",
    "    x = x.rank()\n",
    "    y = y.rank()\n",
    "    \n",
    "    # Compute Mean Values\n",
    "    mean_x, mean_y = np.sum(x)/len(x), np.sum(y)/len(y) \n",
    "    \n",
    "    x_diffs = x - mean_x\n",
    "    y_diffs = y - mean_y\n",
    "    numerator = np.sum(x_diffs*y_diffs)\n",
    "    denominator = np.sqrt(np.sum(x_diffs**2))*np.sqrt(np.sum(y_diffs**2))\n",
    "        \n",
    "    corr = numerator/denominator\n",
    "                            \n",
    "    return corr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, it looks like you are all set!  Nice job coding up Spearman's correlation coefficient!\n"
     ]
    }
   ],
   "source": [
    "# This cell will test your function against the built in scipy function\n",
    "assert corr_spearman(play_data['x1'], play_data['x2']) == spearmanr(play_data['x1'], play_data['x2'])[0], 'Oops!  The correlation between the first two columns should be 0, but your function returned {}.'.format(compute_corr(play_data['x1'], play_data['x2']))\n",
    "assert round(corr_spearman(play_data['x1'], play_data['x3']), 2) == spearmanr(play_data['x1'], play_data['x3'])[0], 'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'.format(np.corrcoef(play_data['x1'], play_data['x3'])[0][1], compute_corr(play_data['x1'], play_data['x3']))\n",
    "assert round(corr_spearman(play_data['x3'], play_data['x4']), 2) == round(spearmanr(play_data['x3'], play_data['x4'])[0], 2), 'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'.format(np.corrcoef(play_data['x3'], play_data['x4'])[0][1], compute_corr(play_data['x3'], play_data['x4']))\n",
    "print(\"If this is all you see, it looks like you are all set!  Nice job coding up Spearman's correlation coefficient!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now that you have computed **Spearman's correlation coefficient**, use the below dictionary to identify statements that are true about **this** measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"If when x increases, y always increases, Spearman's correlation will be always be 1.\": True, \"If when x increases by 1, y always increases by 3, Pearson's correlation will always be 1.\": True, \"If when x increases by 1, y always decreases by 5, Pearson's correlation will always be -1.\": True, \"If when x increases by 1, y increases by 3 times x, Pearson's correlation will always be 1.\": True} \n",
      "\n",
      "That's right!  Unlike Pearson's correlation, Spearman's correlation can have perfect relationships (1 or -1 values) that aren't linear relationships.  You will notice that neither Spearman or Pearson correlation values suggest a relation when there are quadratic relationships.\n"
     ]
    }
   ],
   "source": [
    "a = True\n",
    "b = False\n",
    "c = \"We can't be sure.\"\n",
    "\n",
    "\n",
    "spearman_dct = {\"If when x increases, y always increases, Spearman's correlation will be always be 1.\": a,\n",
    "               \"If when x increases by 1, y always increases by 3, Pearson's correlation will always be 1.\": a,\n",
    "               \"If when x increases by 1, y always decreases by 5, Pearson's correlation will always be -1.\": a,\n",
    "               \"If when x increases by 1, y increases by 3 times x, Pearson's correlation will always be 1.\": a\n",
    "}\n",
    "\n",
    "print(spearman_dct, \"\\n\")\n",
    "print(\"That's right!  Unlike Pearson's correlation, Spearman's correlation can have perfect relationships (1 or -1 values) that aren't linear relationships.  You will notice that neither Spearman or Pearson correlation values suggest a relation when there are quadratic relationships.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kendall's Tau\n",
    "\n",
    "Kendall's tau is quite similar to Spearman's correlation coefficient.  Both of these measures are nonparametric measures of a relationship.  Specifically both Spearman and Kendall's coefficients are calculated based on ranking data and not the raw data.  \n",
    "\n",
    "Similar to both of the previous measures, Kendall's Tau is always between -1 and 1, where -1 suggests a strong, negative relationship between two variables and 1 suggests a strong, positive relationship between two variables.\n",
    "\n",
    "Though Spearman's and Kendall's measures are very similar, there are statistical advantages to choosing Kendall's measure in that Kendall's Tau has smaller variability when using larger sample sizes.  However Spearman's measure is more computationally efficient, as Kendall's Tau is O(n^2) and Spearman's correlation is O(nLog(n)). You can find more on this topic in [this thread](https://www.researchgate.net/post/Does_Spearmans_rho_have_any_advantage_over_Kendalls_tau).\n",
    "\n",
    "Let's take a closer look at exactly how this measure is calculated.  Again, we want to map our data to ranks:\n",
    "\n",
    "$$\\textbf{x} \\rightarrow \\textbf{x}^{r}$$\n",
    "$$\\textbf{y} \\rightarrow \\textbf{y}^{r}$$\n",
    "\n",
    "Then we calculate Kendall's Tau as:\n",
    "\n",
    "$$TAU(\\textbf{x}, \\textbf{y}) = \\frac{2}{n(n -1)}\\sum_{i < j}sgn(x^r_i - x^r_j)sgn(y^r_i - y^r_j)$$\n",
    "\n",
    "Where $sgn$ takes the the sign associated with the difference in the ranked values.  An alternative way to write \n",
    "\n",
    "$$sgn(x^r_i - x^r_j)$$ \n",
    "\n",
    "is in the following way:\n",
    "\n",
    "$$\n",
    " \\begin{cases} \n",
    "      -1  & x^r_i < x^r_j \\\\\n",
    "      0 & x^r_i = x^r_j \\\\\n",
    "      1 & x^r_i > x^r_j \n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore the possible results of \n",
    "\n",
    "$$sgn(x^r_i - x^r_j)sgn(y^r_i - y^r_j)$$\n",
    "\n",
    "are only 1, -1, or 0, which are summed to give an idea of the propotion of times the ranks of **x** and **y** are pointed in the right direction.\n",
    "\n",
    "`5.` Write a function that takes in two vectors and returns Kendall's Tau.  You can then compare your answer to the built in function in scipy stats by using the assert statements in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendalls_tau(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    tau - the kendall's tau for comparing x and y\n",
    "    '''    \n",
    "    # Change each vector to ranked values\n",
    "    x = x.rank()\n",
    "    y = y.rank()\n",
    "    n = len(x)\n",
    "     \n",
    "    sum_vals = 0\n",
    "    # Compute Mean Values\n",
    "    for i, (x_i, y_i) in enumerate(zip(x, y)):\n",
    "        for j, (x_j, y_j) in enumerate(zip(x, y)):\n",
    "            if i < j:\n",
    "                sum_vals += np.sign(x_i - x_j)*np.sign(y_i - y_j)\n",
    "                        \n",
    "    tau = 2*sum_vals/(n*(n-1))\n",
    "    \n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, it looks like you are all set!  Nice job coding up Kendall's Tau!\n"
     ]
    }
   ],
   "source": [
    "# This cell will test your function against the built in scipy function\n",
    "assert kendalls_tau(play_data['x1'], play_data['x2']) == kendalltau(play_data['x1'], play_data['x2'])[0], 'Oops!  The correlation between the first two columns should be 0, but your function returned {}.'.format(kendalls_tau(play_data['x1'], play_data['x2']))\n",
    "assert round(kendalls_tau(play_data['x1'], play_data['x3']), 2) == kendalltau(play_data['x1'], play_data['x3'])[0], 'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'.format(kendalltau(play_data['x1'], play_data['x3'])[0][1], kendalls_tau(play_data['x1'], play_data['x3']))\n",
    "assert round(kendalls_tau(play_data['x3'], play_data['x4']), 2) == round(kendalltau(play_data['x3'], play_data['x4'])[0], 2), 'Oops!  The correlation between the first and third columns should be {}, but your function returned {}.'.format(kendalltau(play_data['x3'], play_data['x4'])[0][1], kendalls_tau(play_data['x3'], play_data['x4']))\n",
    "print(\"If this is all you see, it looks like you are all set!  Nice job coding up Kendall's Tau!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` Use your functions (and/or your knowledge of each of the above coefficients) to accurately identify each of the below statements as True or False.  **Note:** There may be some rounding differences due to the way numbers are stored, so it is recommended that you consider comparisons to 4 or fewer decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"For all columns of play_data, Spearman and Kendall's measures match.\": True, \"For all columns of play_data, Spearman and Pearson's measures match.\": False, \"For all columns of play_data, Pearson and Kendall's measures match.\": False} \n",
      "\n",
      "That's right!  Pearson does not match the other two measures, as it looks specifically for linear relationships.  However, Spearman and Kenall's measures are exactly the same to one another in the cases related to play_data.\n"
     ]
    }
   ],
   "source": [
    "a = True\n",
    "b = False\n",
    "c = \"We can't be sure.\"\n",
    "\n",
    "\n",
    "corr_comp_dct = {\"For all columns of play_data, Spearman and Kendall's measures match.\": a,\n",
    "                \"For all columns of play_data, Spearman and Pearson's measures match.\": b, \n",
    "                \"For all columns of play_data, Pearson and Kendall's measures match.\": b}\n",
    "\n",
    "print(corr_comp_dct, \"\\n\")\n",
    "print(\"That's right!  Pearson does not match the other two measures, as it looks specifically for linear relationships.  However, Spearman and Kenall's measures are exactly the same to one another in the cases related to play_data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Measures\n",
    "\n",
    "Each of the above measures are considered measures of correlation.  Similarly, there are distance measures (of which there are many).  [This is a great article](http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/) on some popular distance metrics.  In this notebook, we will be looking specifically at two of these measures.  \n",
    "\n",
    "1. Euclidean Distance\n",
    "2. Manhattan Distance\n",
    "\n",
    "Different than the three measures you built functions for, these two measures take on values between 0 and potentially infinity.  Measures that are closer to 0 imply that two vectors are more similar to one another.  The larger these values become, the more dissimilar two vectors are to one another.\n",
    "\n",
    "Choosing one of these two `distance` metrics vs. one of the three `similarity` above is often a matter of personal preference, audience, and data specificities.  You will see in a bit a case where one of these measures (euclidean or manhattan distance) is optimal to using Pearson's correlation coefficient.\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "Euclidean distance can also just be considered as straight-line distance between two vectors.\n",
    "\n",
    "For two vectors **x** and **y**, we can compute this as:\n",
    "\n",
    "$$ EUC(\\textbf{x}, \\textbf{y}) = \\sqrt{\\sum\\limits_{i=1}^{n}(x_i - y_i)^2}$$\n",
    "\n",
    "### Manhattan Distance\n",
    "\n",
    "Different from euclidean distance, Manhattan distance is a 'manhattan block' distance from one vector to another.  Therefore, you can imagine this distance as a way to compute the distance between two points when you are not able to go through buildings.\n",
    "\n",
    "Specifically, this distance is computed as:\n",
    "\n",
    "$$ MANHATTAN(\\textbf{x}, \\textbf{y}) = \\sqrt{\\sum\\limits_{i=1}^{n}|x_i - y_i|}$$\n",
    "\n",
    "Using each of the above, write a function for each to take two vectors and compute the euclidean and manhattan distances.\n",
    "\n",
    "\n",
    "<img src=\"https://view3f484599.udacity-student-workspaces.com/notebooks/images/distances.png\">\n",
    "\n",
    "You can see in the above image, the **blue** line gives the **Manhattan** distance, while the **green** line gives the **Euclidean** distance between two points.\n",
    "\n",
    "`7.` Use the below cell to complete a function for each distance metric.  Then test your functions against the built in values using the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucl_dist(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    euc - the euclidean distance between x and y\n",
    "    '''  \n",
    "    return np.linalg.norm(x - y)\n",
    "    \n",
    "def manhat_dist(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    manhat - the manhattan distance between x and y\n",
    "    '''  \n",
    "    return sum(abs(e - s) for s, e in zip(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.966629547095765"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your functions\n",
    "eucl_dist(play_data['x1'], play_data['x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manhat_dist(play_data['x2'], play_data['x3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 Solution: Measuring Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 Identifying Recommendation Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Recommendations\n",
    "Finalizing our neighborhood based recommendations, we need to use the ratings from our neighbors to influence the ratings we provide to other users.\n",
    "\n",
    "There are a few ways to do this, but a simple method would be to:\n",
    "1. Remove movies our user has already seen.\n",
    "2. Find ratings of the neighbors that are high.\n",
    "3. Recommend movies to each user where both 1 and 2 above hold.\n",
    "\n",
    "Other methods for making recommendations using collaborative filtering are based on weighting of the neighbors' ratings based on the 'closeness' of the neighbors.\n",
    "\n",
    "You can use each of the following two papers to learn more about this technique:\n",
    "1. [Domino Data Lab Paper](https://blog.dominodatalab.com/recommender-systems-collaborative-filtering/)\n",
    "2. [Semantic Scholar Paper On Weighted Ratings](https://pdfs.semanticscholar.org/3e9e/bcd9503ef7375c7bb334511804d1e45127e9.pdf)\n",
    "\n",
    "In the next notebook, you will implement the three-step process above to make recommendations for every user in the dataset. For computational reasons, you will notice that iterating this approach through all users has been done for you. But you will go through the process of implementing for individual pairs of users, which could easily be extended via looping to all users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17 Notebook: Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations with MovieTweetings: Collaborative Filtering\n",
    "\n",
    "One of the most popular methods for making recommendations is **collaborative filtering**.  In collaborative filtering, you are using the collaboration of user-item recommendations to assist in making new recommendations.  \n",
    "\n",
    "There are two main methods of performing collaborative filtering:\n",
    "\n",
    "1. **Neighborhood-Based Collaborative Filtering**, which is based on the idea that we can either correlate items that are similar to provide recommendations or we can correlate users to one another to provide recommendations.\n",
    "\n",
    "2. **Model Based Collaborative Filtering**, which is based on the idea that we can use machine learning and other mathematical models to understand the relationships that exist amongst items and users to predict ratings and provide ratings.\n",
    "\n",
    "\n",
    "In this notebook, you will be working on performing **neighborhood-based collaborative filtering**.  There are two main methods for performing collaborative filtering:\n",
    "\n",
    "1. **User-based collaborative filtering:** In this type of recommendation, users related to the user you would like to make recommendations for are used to create a recommendation.\n",
    "\n",
    "2. **Item-based collaborative filtering:** In this type of recommendation, first you need to find the items that are most related to each other item (based on similar ratings).  Then you can use the ratings of an individual on those similar items to understand if a user will like the new item.\n",
    "\n",
    "In this notebook you will be implementing **user-based collaborative filtering**.  However, it is easy to extend this approach to make recommendations using **item-based collaborative filtering**.  First, let's read in our data and necessary libraries.\n",
    "\n",
    "**NOTE**: Because of the size of the datasets, some of your code cells here will take a while to execute, so be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_id  movie_id  rating   timestamp                 date\n",
      "0       1   0114508       8  1381006850  2013-10-05 16:00:50\n",
      "1       2   0358273       9  1579057827  2020-01-14 21:10:27\n",
      "2       2  10039344       5  1578603053  2020-01-09 14:50:53\n",
      "3       2   6751668       9  1578955697  2020-01-13 16:48:17\n",
      "4       2   7131622       8  1579559244  2020-01-20 16:27:24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tests as t\n",
    "from scipy.sparse import csr_matrix\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the datasets\n",
    "# movies = pd.read_csv('movies_clean.csv')\n",
    "# reviews = pd.read_csv('reviews_clean.csv')\n",
    "\n",
    "# del movies['Unnamed: 0']\n",
    "# del reviews['Unnamed: 0']\n",
    "\n",
    "print(reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Similarity\n",
    "\n",
    "When using **neighborhood** based collaborative filtering, it is important to understand how to measure the similarity of users or items to one another.  \n",
    "\n",
    "There are a number of ways in which we might measure the similarity between two vectors (which might be two users or two items).  In this notebook, we will look specifically at two measures used to compare vectors:\n",
    "\n",
    "* **Pearson's correlation coefficient**\n",
    "\n",
    "Pearson's correlation coefficient is a measure of the strength and direction of a linear relationship. The value for this coefficient is a value between -1 and 1 where -1 indicates a strong, negative linear relationship and 1 indicates a strong, positive linear relationship. \n",
    "\n",
    "If we have two vectors x and y, we can define the correlation between the vectors as:\n",
    "\n",
    "\n",
    "$$CORR(x, y) = \\frac{\\text{COV}(x, y)}{\\text{STDEV}(x)\\text{ }\\text{STDEV}(y)}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\text{STDEV}(x) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\text{COV}(x, y) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})$$\n",
    "\n",
    "where n is the length of the vector, which must be the same for both x and y and $\\bar{x}$ is the mean of the observations in the vector.  \n",
    "\n",
    "We can use the correlation coefficient to indicate how alike two vectors are to one another, where the closer to 1 the coefficient, the more alike the vectors are to one another.  There are some potential downsides to using this metric as a measure of similarity.  You will see some of these throughout this workbook.\n",
    "\n",
    "\n",
    "* **Euclidean distance**\n",
    "\n",
    "Euclidean distance is a measure of the straightline distance from one vector to another.  Because this is a measure of distance, larger values are an indication that two vectors are different from one another (which is different than Pearson's correlation coefficient).\n",
    "\n",
    "Specifically, the euclidean distance between two vectors x and y is measured as:\n",
    "\n",
    "$$ \\text{EUCL}(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$$\n",
    "\n",
    "Different from the correlation coefficient, no scaling is performed in the denominator.  Therefore, you need to make sure all of your data are on the same scale when using this metric.\n",
    "\n",
    "**Note:** Because measuring similarity is often based on looking at the distance between vectors, it is important in these cases to scale your data or to have all data be in the same scale.  In this case, we will not need to scale data because they are all on a 10 point scale, but it is always something to keep in mind!\n",
    "\n",
    "------------\n",
    "\n",
    "### User-Item Matrix\n",
    "\n",
    "In order to calculate the similarities, it is common to put values in a matrix.  In this matrix, users are identified by each row, and items are represented by columns.  \n",
    "\n",
    "\n",
    "![alt text](https://view3f484599.udacity-student-workspaces.com/notebooks/images/userxitem.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above matrix, you can see that **User 1** and **User 2** both used **Item 1**, and **User 2**, **User 3**, and **User 4** all used **Item 2**.  However, there are also a large number of missing values in the matrix for users who haven't used a particular item.  A matrix with many missing values (like the one above) is considered **sparse**.\n",
    "\n",
    "Our first goal for this notebook is to create the above matrix with the **reviews** dataset.  However, instead of 1 values in each cell, you should have the actual rating.  \n",
    "\n",
    "The users will indicate the rows, and the movies will exist across the columns. To create the user-item matrix, we only need the first three columns of the **reviews** dataframe, which you can see by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854283"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_items = reviews[['user_id', 'movie_id', 'rating']]\n",
    "user_items.head()\n",
    "len(user_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the User-Item Matrix\n",
    "\n",
    "In order to create the user-items matrix (like the one above), I personally started by using a [pivot table](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html). \n",
    "\n",
    "However, I quickly ran into a memory error (a common theme throughout this notebook).  I will help you navigate around many of the errors I had, and achieve useful collaborative filtering results! \n",
    "\n",
    "_____\n",
    "\n",
    "`1.` Create a matrix where the users are the rows, the movies are the columns, and the ratings exist in each cell, or a NaN exists in cells where a user hasn't rated a particular movie. If you get a memory error (like I did), [this link here](https://stackoverflow.com/questions/39648991/pandas-dataframe-pivot-memory-error) might help you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unstacked DataFrame is too big, causing int32 overflow\n"
     ]
    }
   ],
   "source": [
    "# Create user-by-item matrix\n",
    "try:\n",
    "    user_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert movies.shape[0] == user_by_movie.shape[1], \"Oh no! Your matrix should have {} columns, and yours has {}!\".format(movies.shape[0], user_by_movie.shape[1])\n",
    "# assert reviews.user_id.nunique() == user_by_movie.shape[0], \"Oh no! Your matrix should have {} rows, and yours has {}!\".format(reviews.user_id.nunique(), user_by_movie.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have a matrix of users by movies, use this matrix to create a dictionary where the key is each user and the value is an array of the movies each user has rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with users and corresponding movies seen\n",
    "\n",
    "def movies_watched(user_id):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id of an individual as int\n",
    "    OUTPUT:\n",
    "    movies - an array of movies the user has watched\n",
    "    '''\n",
    "    movies = user_by_movie.loc[user_id][user_by_movie.loc[user_id].isnull() == False].index.values\n",
    "\n",
    "    return movies\n",
    "\n",
    "\n",
    "def create_user_movie_dict():\n",
    "    '''\n",
    "    INPUT: None\n",
    "    OUTPUT: movies_seen - a dictionary where each key is a user_id and the value is an array of movie_ids\n",
    "    \n",
    "    Creates the movies_seen dictionary\n",
    "    '''\n",
    "    n_users = user_by_movie.shape[0]\n",
    "    movies_seen = dict()\n",
    "\n",
    "    for user1 in range(1, n_users+1):\n",
    "        \n",
    "        # assign list of movies to each user key\n",
    "        movies_seen[user1] = movies_watched(user1)\n",
    "    \n",
    "    return movies_seen\n",
    "    \n",
    "movies_seen = create_user_movie_dict()`2.` Now that you have a matrix of users by movies, use this matrix to create a dictionary where the key is each user and the value is an array of the movies each user has rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : [ 68646 113277]\n",
      "2 : [ 422720  454876  790636  816711 1091191 1103275 1322269 1390411 1398426\n",
      " 1431045 1433811 1454468 1535109 1675434 1798709 2017038 2024544 2294629\n",
      " 2361509 2381249 2726560 2883512 3079380]\n",
      "3 : [1790864 2170439 2203939]\n",
      "4 : [1300854]\n"
     ]
    }
   ],
   "source": [
    "for user in [1,2,3,4]:\n",
    "    print(user,\":\",movies_seen[user])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` If a user hasn't rated more than 2 movies, we consider these users \"too new\".  Create a new dictionary that only contains users who have rated more than 2 movies.  This dictionary will be used for all the final steps of this workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove individuals who have watched 2 or fewer movies - don't have enough data to make recs\n",
    "\n",
    "def create_movies_to_analyze(movies_seen, lower_bound=2):\n",
    "    '''\n",
    "    INPUT:  \n",
    "    movies_seen - a dictionary where each key is a user_id and the value is an array of movie_ids\n",
    "    lower_bound - (an int) a user must have more movies seen than the lower bound to be added to the movies_to_analyze dictionary\n",
    "\n",
    "    OUTPUT: \n",
    "    movies_to_analyze - a dictionary where each key is a user_id and the value is an array of movie_ids\n",
    "    \n",
    "    The movies_seen and movies_to_analyze dictionaries should be the same except that the output dictionary has removed \n",
    "    \n",
    "    '''\n",
    "    movies_to_analyze = dict()\n",
    "\n",
    "    for user, movies in movies_seen.items():\n",
    "        if len(movies) > lower_bound:\n",
    "            movies_to_analyze[user] = movies\n",
    "    return movies_to_analyze\n",
    "\n",
    "movies_to_analyze = create_movies_to_analyze(movies_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tests below to check that your movies_to_analyze matches the solution\n",
    "# assert len(movies_to_analyze) == 23512, \"Oops!  It doesn't look like your dictionary has the right number of individuals.\"\n",
    "# assert len(movies_to_analyze[2]) == 23, \"Oops!  User 2 didn't match the number of movies we thought they would have.\"\n",
    "# assert len(movies_to_analyze[7])  == 3, \"Oops!  User 7 didn't match the number of movies we thought they would have.\"\n",
    "# print(\"If this is all you see, you are good to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating User Similarities\n",
    "\n",
    "Now that you have set up the **movies_to_analyze** dictionary, it is time to take a closer look at the similarities between users. Below is the pseudocode for how I thought about determining the similarity between users:\n",
    "\n",
    "```\n",
    "for user1 in movies_to_analyze\n",
    "    for user2 in movies_to_analyze\n",
    "        see how many movies match between the two users\n",
    "        if more than two movies in common\n",
    "            pull the overlapping movies\n",
    "            compute the distance/similarity metric between ratings on the same movies for the two users\n",
    "            store the users and the distance metric\n",
    "```\n",
    "\n",
    "However, this took a very long time to run, and other methods of performing these operations did not fit on the workspace memory!\n",
    "\n",
    "Therefore, rather than creating a dataframe with all possible pairings of users in our data, your task for this question is to look at a few specific examples of the correlation between ratings given by two users.  For this question consider you want to compute the [correlation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) between users.\n",
    "\n",
    "`4.` Using the **movies_to_analyze** dictionary and **user_by_movie** dataframe, create a function that computes the correlation between the ratings of similar movies for two users.  Then use your function to compare your results to ours using the tests below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(user1, user2):\n",
    "    '''\n",
    "    INPUT\n",
    "    user1 - int user_id\n",
    "    user2 - int user_id\n",
    "    OUTPUT\n",
    "    the correlation between the matching ratings between the two users\n",
    "    '''\n",
    "    # Pull movies for each user\n",
    "    movies1 = movies_to_analyze[user1]\n",
    "    movies2 = movies_to_analyze[user2]\n",
    "    \n",
    "    \n",
    "    # Find Similar Movies\n",
    "    sim_movs = np.intersect1d(movies1, movies2, assume_unique=True)\n",
    "    \n",
    "    # Calculate correlation between the users\n",
    "    df = user_by_movie.loc[(user1, user2), sim_movs]\n",
    "    corr = df.transpose().corr().iloc[0,1]\n",
    "    \n",
    "    return corr #return the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function against the solution\n",
    "# assert compute_correlation(2,2) == 1.0, \"Oops!  The correlation between a user and itself should be 1.0.\"\n",
    "# assert round(compute_correlation(2,66), 2) == 0.76, \"Oops!  The correlation between user 2 and 66 should be about 0.76.\"\n",
    "# assert np.isnan(compute_correlation(2,104)), \"Oops!  The correlation between user 2 and 104 should be a NaN.\"\n",
    "\n",
    "# print(\"If this is all you see, then it looks like your function passed all of our tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the NaN's?\n",
    "\n",
    "If the function you wrote passed all of the tests, then you have correctly set up your function to calculate the correlation between any two users.  \n",
    "\n",
    "`5.` But one question is, why are we still obtaining **NaN** values?  As you can see in the code cell above, users 2 and 104 have a correlation of **NaN**. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think and write your ideas here about why these NaNs exist, and use the cells below to do some coding to validate your thoughts. You can check other pairs of users and see that there are actually many NaNs in our data - 2,526,710 of them in fact. These NaN's ultimately make the correlation coefficient a less than optimal measure of similarity between two users.\n",
    "\n",
    "```\n",
    "In the denominator of the correlation coefficient, we calculate the standard deviation for each user's ratings.  The ratings for user 2 are all the same rating on the movies that match with user 104.  Therefore, the standard deviation is 0.  Because a 0 is in the denominator of the correlation coefficient, we end up with a **NaN** correlation coefficient.  Therefore, a different approach is likely better for this particular situation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{454876, 816711, 1454468, 1535109}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which movies did both user 2 and user 104 see?\n",
    "set_2 = set(movies_to_analyze[2])\n",
    "set_104 = set(movies_to_analyze[104])\n",
    "set_2.intersection(set_104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_id\n",
      "454876     8.0\n",
      "1454468    8.0\n",
      "1535109    8.0\n",
      "816711     8.0\n",
      "Name: 2, dtype: float64\n",
      "movie_id\n",
      "454876     9.0\n",
      "1454468    7.0\n",
      "1535109    9.0\n",
      "816711     7.0\n",
      "Name: 104, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# What were the ratings for each user on those movies?\n",
    "print(user_by_movie.loc[2, set_2.intersection(set_104)])\n",
    "print(user_by_movie.loc[104, set_2.intersection(set_104)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` Because the correlation coefficient proved to be less than optimal for relating user ratings to one another, we could instead calculate the euclidean distance between the ratings.  I found [this post](https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy) particularly helpful when I was setting up my function.  This function should be very similar to your previous function.  When you feel confident with your function, test it against our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_euclidean_dist(user1, user2):\n",
    "    '''\n",
    "    INPUT\n",
    "    user1 - int user_id\n",
    "    user2 - int user_id\n",
    "    OUTPUT\n",
    "    the euclidean distance between user1 and user2\n",
    "    '''\n",
    "    # Pull movies for each user\n",
    "    movies1 = movies_to_analyze[user1]\n",
    "    movies2 = movies_to_analyze[user2]\n",
    "    \n",
    "    \n",
    "    # Find Similar Movies\n",
    "    sim_movs = np.intersect1d(movies1, movies2, assume_unique=True)\n",
    "    \n",
    "    # Calculate euclidean distance between the users\n",
    "    df = user_by_movie.loc[(user1, user2), sim_movs]\n",
    "    dist = np.linalg.norm(df.loc[user1] - df.loc[user2])\n",
    "    \n",
    "    return dist #return the euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in solution euclidean distances\"\n",
    "import pickle\n",
    "df_dists = pd.read_pickle(\"data/Term2/recommendations/lesson1/data/dists.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Nearest Neighbors to Make Recommendations\n",
    "\n",
    "In the previous question, you read in **df_dists**. Therefore, you have a measure of distance between each user and every other user. This dataframe holds every possible pairing of users, as well as the corresponding euclidean distance.\n",
    "\n",
    "Because of the **NaN** values that exist within the correlations of the matching ratings for many pairs of users, as we discussed above, we will proceed using **df_dists**. You will want to find the users that are 'nearest' each user.  Then you will want to find the movies the closest neighbors have liked to recommend to each user.\n",
    "\n",
    "I made use of the following objects:\n",
    "\n",
    "* df_dists (to obtain the neighbors)\n",
    "* user_items (to obtain the movies the neighbors and users have rated)\n",
    "* movies (to obtain the names of the movies)\n",
    "\n",
    "`7.` Complete the functions below, which allow you to find the recommendations for any user.  There are five functions which you will need:\n",
    "\n",
    "* **find_closest_neighbors** - this returns a list of user_ids from closest neighbor to farthest neighbor using euclidean distance\n",
    "\n",
    "\n",
    "* **movies_liked** - returns an array of movie_ids\n",
    "\n",
    "\n",
    "* **movie_names** - takes the output of movies_liked and returns a list of movie names associated with the movie_ids\n",
    "\n",
    "\n",
    "* **make_recommendations** - takes a user id and goes through closest neighbors to return a list of movie names as recommendations\n",
    "\n",
    "\n",
    "* **all_recommendations** = loops through every user and returns a dictionary of with the key as a user_id and the value as a list of movie recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_neighbors(user):\n",
    "    '''\n",
    "    INPUT:\n",
    "        user - (int) the user_id of the individual you want to find the closest users\n",
    "    OUTPUT:\n",
    "        closest_neighbors - an array of the id's of the users sorted from closest to farthest away\n",
    "    '''\n",
    "    # I treated ties as arbitrary and just kept whichever was easiest to keep using the head method\n",
    "    # You might choose to do something less hand wavy\n",
    "    \n",
    "    closest_users = df_dists[df_dists['user1']==user].sort_values(by='eucl_dist').iloc[1:]['user2']\n",
    "    closest_neighbors = np.array(closest_users)\n",
    "    \n",
    "    return closest_neighbors\n",
    "    \n",
    "    \n",
    "    \n",
    "def movies_liked(user_id, min_rating=7):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id of an individual as int\n",
    "    min_rating - the minimum rating considered while still a movie is still a \"like\" and not a \"dislike\"\n",
    "    OUTPUT:\n",
    "    movies_liked - an array of movies the user has watched and liked\n",
    "    '''\n",
    "    movies_liked = np.array(user_items.query('user_id == @user_id and rating > (@min_rating -1)')['movie_id'])\n",
    "    \n",
    "    return movies_liked\n",
    "\n",
    "\n",
    "def movie_names(movie_ids):\n",
    "    '''\n",
    "    INPUT\n",
    "    movie_ids - a list of movie_ids\n",
    "    OUTPUT\n",
    "    movies - a list of movie names associated with the movie_ids\n",
    "    \n",
    "    '''\n",
    "    movie_lst = list(movies[movies['movie_id'].isin(movie_ids)]['movie'])\n",
    "   \n",
    "    return movie_lst\n",
    "    \n",
    "    \n",
    "def make_recommendations(user, num_recs=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "        user - (int) a user_id of the individual you want to make recommendations for\n",
    "        num_recs - (int) number of movies to return\n",
    "    OUTPUT:\n",
    "        recommendations - a list of movies - if there are \"num_recs\" recommendations return this many\n",
    "                          otherwise return the total number of recommendations available for the \"user\"\n",
    "                          which may just be an empty list\n",
    "    '''\n",
    "    # I wanted to make recommendations by pulling different movies than the user has already seen\n",
    "    # Go in order from closest to farthest to find movies you would recommend\n",
    "    # I also only considered movies where the closest user rated the movie as a 9 or 10\n",
    "    \n",
    "    # movies_seen by user (we don't want to recommend these)\n",
    "    movies_seen = movies_watched(user)\n",
    "    closest_neighbors = find_closest_neighbors(user)\n",
    "    \n",
    "    # Keep the recommended movies here\n",
    "    recs = np.array([])\n",
    "    \n",
    "    # Go through the neighbors and identify movies they like the user hasn't seen\n",
    "    for neighbor in closest_neighbors:\n",
    "        neighbs_likes = movies_liked(neighbor)\n",
    "        \n",
    "        #Obtain recommendations for each neighbor\n",
    "        new_recs = np.setdiff1d(neighbs_likes, movies_seen, assume_unique=True)\n",
    "        \n",
    "        # Update recs with new recs\n",
    "        recs = np.unique(np.concatenate([new_recs, recs], axis=0))\n",
    "        \n",
    "        # If we have enough recommendations exit the loop\n",
    "        if len(recs) > num_recs-1:\n",
    "            break\n",
    "    \n",
    "    # Pull movie titles using movie ids\n",
    "    recommendations = movie_names(recs)\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def all_recommendations(num_recs=10):\n",
    "    '''\n",
    "    INPUT \n",
    "        num_recs (int) the (max) number of recommendations for each user\n",
    "    OUTPUT\n",
    "        all_recs - a dictionary where each key is a user_id and the value is an array of recommended movie titles\n",
    "    '''\n",
    "    \n",
    "    # All the users we need to make recommendations for\n",
    "    users = np.unique(df_dists['user1'])\n",
    "    n_users = len(users)\n",
    "    \n",
    "    #Store all recommendations in this dictionary\n",
    "    all_recs = dict()\n",
    "    \n",
    "    # Make the recommendations for each user\n",
    "    for user in users:\n",
    "        all_recs[user] = make_recommendations(user, num_recs)\n",
    "    \n",
    "    return all_recs\n",
    "\n",
    "all_recs = all_recommendations(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now What?\n",
    "\n",
    "If you made it this far, you have successfully implemented a solution to making recommendations using collaborative filtering. \n",
    "\n",
    "`8.` Let's do a quick recap of the steps taken to obtain recommendations using collaborative filtering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"pearson's correlation and spearman's correlation\"\n",
    "b = 'item based collaborative filtering'\n",
    "c = \"there were too many ratings to get a stable metric\"\n",
    "d = 'user based collaborative filtering'\n",
    "e = \"euclidean distance and pearson's correlation coefficient\"\n",
    "f = \"manhattan distance and euclidean distance\"\n",
    "g = \"spearman's correlation and euclidean distance\"\n",
    "h = \"the spread in some ratings was zero\"\n",
    "i = 'content based recommendation'\n",
    "\n",
    "sol_dict = {\n",
    "    'The type of recommendation system implemented here was a ...': d,\n",
    "    'The two methods used to estimate user similarity were: ': e,\n",
    "    'There was an issue with using the correlation coefficient.  What was it?': h\n",
    "}### Now What?\n",
    "\n",
    "If you made it this far, you have successfully implemented a solution to making recommendations using collaborative filtering. \n",
    "\n",
    "`8.` Let's do a quick recap of the steps taken to obtain recommendations using collaborative filtering.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, let's take a closer look at some of the results.  There are two solution files that you read in to check your results, and you created these objects\n",
    "\n",
    "* **df_dists** - a dataframe of user1, user2, euclidean distance between the two users\n",
    "* **all_recs_sol** - a dictionary of all recommendations (key = user, value = list of recommendations)  \n",
    "\n",
    "`9.` Use these two objects along with the cells below to correctly fill in the dictionary below and complete this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 567\n",
    "b = 1503\n",
    "c = 1319\n",
    "d = 1325\n",
    "e = 2526710\n",
    "f = 0\n",
    "g = 'Use another method to make recommendations - content based, knowledge based, or model based collaborative filtering'\n",
    "\n",
    "sol_dict2 = {\n",
    "    'For how many pairs of users were we not able to obtain a measure of similarity using correlation?': e,\n",
    "    'For how many pairs of users were we not able to obtain a measure of similarity using euclidean distance?': f,\n",
    "    'For how many users were we unable to make any recommendations for using collaborative filtering?': c,\n",
    "    'For how many users were we unable to make 10 recommendations for using collaborative filtering?': d,\n",
    "    'What might be a way for us to get 10 recommendations for every user?': g   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1319"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Users without recs\n",
    "users_without_recs = []\n",
    "for user, movie_recs in all_recs.items():\n",
    "    if len(movie_recs) == 0:\n",
    "        users_without_recs.append(user)\n",
    "    \n",
    "len(users_without_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaN euclidean distance values\n",
    "df_dists['eucl_dist'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1325"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Users with fewer than 10 recs\n",
    "users_with_less_than_10recs = []\n",
    "for user, movie_recs in all_recs.items():\n",
    "    if len(movie_recs) < 10:\n",
    "        users_with_less_than_10recs.append(user)\n",
    "    \n",
    "len(users_with_less_than_10recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18 Solution: Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 Solution: Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Ways to Recommend: Content Based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Based Recommendations\n",
    "In the previous notebook, you implemented collaborative filtering to make recommendations based on finding similar users. However, there were a number of users who did not receive recommendations when using this technique. For this reason, we might use other recommendation techniques to find items to recommend to users.\n",
    "\n",
    "Another popular technique for making recommendations is called content based recommendations. In this recommendation technique, we use information that is known about the user or item to make recommendations. This method of making recommendations is particularly useful when we do not have a lot of user-item connections available in our dataset.\n",
    "\n",
    "It might be the case that content based and collaborative filtering based techniques come up with similar recommendations, but the methods by which data scientists approach these recommendations are very different. In collaborative filtering, you are using the connections of users and items (as you did before). In content based techniques, you are using information about the users and items, but not connections (hence the usefulness when you do not have a lot of internal data already available to use).\n",
    "\n",
    "Let's take a look at how we might implement this method with the MovieTweetings data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21 Notebook: Content Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Content Based Recommendations\n",
    "\n",
    "In the previous notebook, you were introduced to a way to make recommendations using collaborative filtering.  However, using this technique there are a large number of users who were left without any recommendations at all.  Other users were left with fewer than the ten recommendations that were set up by our function to retrieve...\n",
    "\n",
    "In order to help these users out, let's try another technique **content based** recommendations.  Let's start off where we were in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinwebb/Desktop/env/ds/lib/python3.7/site-packages/ipykernel_launcher.py:23: ResourceWarning: unclosed file <_io.BufferedReader name='data/all_recs.p'>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from IPython.display import HTML\n",
    "import progressbar\n",
    "import tests as t\n",
    "import pickle\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"data/\"\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv(path + 'movies_clean.csv')\n",
    "reviews = pd.read_csv(path + 'reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']\n",
    "\n",
    "\n",
    "all_recs = pickle.load(open(path + \"all_recs.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "From the above, you now have access to three important items that you will be using throughout the rest of this notebook.  \n",
    "\n",
    "`a.` **movies** - a dataframe of all of the movies in the dataset along with other content related information about the movies (genre and date)\n",
    "\n",
    "\n",
    "`b.` **reviews** - this was the main dataframe used before for collaborative filtering, as it contains all of the interactions between users and movies.\n",
    "\n",
    "\n",
    "`c.` **all_recs** - a dictionary where each key is a user, and the value is a list of movie recommendations based on collaborative filtering\n",
    "\n",
    "For the individuals in **all_recs** who did recieve 10 recommendations using collaborative filtering, we don't really need to worry about them.  However, there were a number of individuals in our dataset who did not receive any recommendations.\n",
    "\n",
    "-----\n",
    "\n",
    "`1.` Let's start with finding all of the users in our dataset who didn't get all 10 ratings we would have liked them to have using collaborative filtering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22187 users with all reccomendations from collaborative filtering.\n",
      "There are 31781 users who still need recommendations.\n",
      "This means that only 41.11% of users received all 10 of their recommendations using collaborative filtering\n"
     ]
    }
   ],
   "source": [
    "users_with_all_recs = []\n",
    "for user, movie_recs in all_recs.items():\n",
    "    if len(movie_recs) > 9:\n",
    "        users_with_all_recs.append(user)\n",
    "\n",
    "print(\"There are {} users with all reccomendations from collaborative filtering.\".format(len(users_with_all_recs)))\n",
    "\n",
    "users = np.unique(reviews['user_id'])\n",
    "users_who_need_recs = np.setdiff1d(users, users_with_all_recs)\n",
    "\n",
    "print(\"There are {} users who still need recommendations.\".format(len(users_who_need_recs)))\n",
    "print(\"This means that only {}% of users received all 10 of their recommendations using collaborative filtering\".format(round(len(users_with_all_recs)/len(np.unique(reviews['user_id'])), 4)*100))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Based Recommendations\n",
    "\n",
    "You will be doing a bit of a mix of content and collaborative filtering to make recommendations for the users this time.  This will allow you to obtain recommendations in many cases where we didn't make recommendations earlier.     \n",
    "\n",
    "`2.` Before finding recommendations, rank the user's ratings from highest to lowest. You will move through the movies in this order looking for other similar movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe similar to reviews, but ranked by rating for each user\n",
    "ranked_reviews = reviews.sort_values(by=['user_id', 'rating'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities\n",
    "\n",
    "In the collaborative filtering sections, you became quite familiar with different methods of determining the similarity (or distance) of two users.  We can perform similarities based on content in much the same way.  \n",
    "\n",
    "In many cases, it turns out that one of the fastest ways we can find out how similar items are to one another (when our matrix isn't totally sparse like it was in the earlier section) is by simply using matrix multiplication.  If you are not familiar with this, an explanation is available [here by 3blue1brown](https://www.youtube.com/watch?v=LyGKycYT2v0) and another quick explanation is provided [on the post here](https://math.stackexchange.com/questions/689022/how-does-the-dot-product-determine-similarity).\n",
    "\n",
    "For us to pull out a matrix that describes the movies in our dataframe in terms of content, we might just use the indicator variables related to **year** and **genre** for our movies.  \n",
    "\n",
    "Then we can obtain a matrix of how similar movies are to one another by taking the dot product of this matrix with itself.  Notice in the below that the dot product where our 1 values overlap gives a value of 2 indicating higher similarity.  In the second dot product, the 1 values don't match up.  This leads to a dot product of 0 indicating lower similarity.\n",
    "\n",
    "<img src=\"https://view3f484599.udacity-student-workspaces.com/notebooks/images/dotprod1.png\" alt=\"Dot Product\" height=\"500\" width=\"500\">\n",
    "\n",
    "We can perform the dot product on a matrix of movies with content characteristics to provide a movie by movie matrix where each cell is an indication of how similar two movies are to one another.  In the below image, you can see that movies 1 and 8 are most similar, movies 2 and 8 are most similar and movies 3 and 9 are most similar for this subset of the data.  The diagonal elements of the matrix will contain the similarity of a movie with itself, which will be the largest possible similarity (which will also be the number of 1's in the movie row within the orginal movie content matrix.\n",
    "\n",
    "<img src=\"https://view3f484599.udacity-student-workspaces.com/notebooks/images/moviemat.png\" alt=\"Dot Product\" height=\"500\" width=\"500\">\n",
    "\n",
    "\n",
    "`3.` Create a numpy array that is a matrix of indicator variables related to year (by century) and movie genres by movie.  Perform the dot product of this matrix with itself (transposed) to obtain a similarity matrix of each movie with every other movie.  The final matrix should be 31245 x 31245."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset so movie_content is only using the dummy variables for each genre and the 3 century based year dummy columns\n",
    "movie_content = np.array(movies.iloc[:,4:])\n",
    "\n",
    "# Take the dot product to obtain a movie x movie matrix of similarities\n",
    "dot_prod_movies = movie_content.dot(np.transpose(movie_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Each User...\n",
    "\n",
    "\n",
    "Now that you have a matrix where each user has their ratings ordered.  You also have a second matrix where movies are each axis, and the matrix entries are larger where the two movies are more similar and smaller where the two movies are dissimilar.  This matrix is a measure of content similarity. Therefore, it is time to get to the fun part.\n",
    "\n",
    "For each user, we will perform the following:\n",
    "\n",
    "    i. For each movie, find the movies that are most similar that the user hasn't seen.\n",
    "\n",
    "    ii. Continue through the available, rated movies until 10 recommendations or until there are no additional movies.\n",
    "\n",
    "As a final note, you may need to adjust the criteria for 'most similar' to obtain 10 recommendations.  As a first pass, I used only movies with the highest possible similarity to one another as similar enough to add as a recommendation.\n",
    "\n",
    "`3.` In the below cell, complete each of the functions needed for making content based recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_movies(movie_id):\n",
    "    '''\n",
    "    INPUT\n",
    "    movie_id - a movie_id \n",
    "    OUTPUT\n",
    "    similar_movies - an array of the most similar movies by title\n",
    "    '''\n",
    "    # find the row of each movie id\n",
    "    movie_idx = np.where(movies['movie_id'] == movie_id)[0][0]\n",
    "    \n",
    "    # find the most similar movie indices - to start I said they need to be the same for all content\n",
    "    similar_idxs = np.where(dot_prod_movies[movie_idx] == np.max(dot_prod_movies[movie_idx]))[0]\n",
    "    \n",
    "    # pull the movie titles based on the indices\n",
    "    similar_movies = np.array(movies.iloc[similar_idxs, ]['movie'])\n",
    "    \n",
    "    return similar_movies\n",
    "    \n",
    "    \n",
    "def get_movie_names(movie_ids):\n",
    "    '''\n",
    "    INPUT\n",
    "    movie_ids - a list of movie_ids\n",
    "    OUTPUT\n",
    "    movies - a list of movie names associated with the movie_ids\n",
    "    \n",
    "    '''\n",
    "    movie_lst = list(movies[movies['movie_id'].isin(movie_ids)]['movie'])\n",
    "   \n",
    "    return movie_lst\n",
    "\n",
    "def make_recs():\n",
    "    '''\n",
    "    INPUT\n",
    "    None\n",
    "    OUTPUT\n",
    "    recs - a dictionary with keys of the user and values of the recommendations\n",
    "    '''\n",
    "    # Create dictionary to return with users and ratings\n",
    "    recs = defaultdict(set)\n",
    "    # How many users for progress bar\n",
    "    n_users = len(users)\n",
    "\n",
    "    \n",
    "    # Create the progressbar\n",
    "    cnter = 0\n",
    "    bar = progressbar.ProgressBar(maxval=n_users+1, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    # For each user\n",
    "    for user in users:\n",
    "        \n",
    "        # Update the progress bar\n",
    "        cnter+=1 \n",
    "        bar.update(cnter)\n",
    "\n",
    "        # Pull only the reviews the user has seen\n",
    "        reviews_temp = ranked_reviews[ranked_reviews['user_id'] == user]\n",
    "        movies_temp = np.array(reviews_temp['movie_id'])\n",
    "        movie_names = np.array(get_movie_names(movies_temp))\n",
    "\n",
    "        # Look at each of the movies (highest ranked first), \n",
    "        # pull the movies the user hasn't seen that are most similar\n",
    "        # These will be the recommendations - continue until 10 recs \n",
    "        # or you have depleted the movie list for the user\n",
    "        for movie in movies_temp:\n",
    "            rec_movies = find_similar_movies(movie)\n",
    "            temp_recs = np.setdiff1d(rec_movies, movie_names)\n",
    "            recs[user].update(temp_recs)\n",
    "\n",
    "            # If there are more than \n",
    "            if len(recs[user]) > 9:\n",
    "                break\n",
    "\n",
    "    bar.finish()\n",
    "    \n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "recs = make_recs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Did We Do?\n",
    "\n",
    "Now that you have made the recommendations, how did we do in providing everyone with a set of recommendations?\n",
    "\n",
    "`4.` Use the cells below to see how many individuals you were able to make recommendations for, as well as explore characteristics about individuals who you were not able to make recommendations for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore recommendations\n",
    "users_without_all_recs = []\n",
    "users_with_all_recs = []\n",
    "no_recs = []\n",
    "for user, movie_recs in recs.items():\n",
    "    if len(movie_recs) < 10:\n",
    "        users_without_all_recs.append(user)\n",
    "    if len(movie_recs) > 9:\n",
    "        users_with_all_recs.append(user)\n",
    "    if len(movie_recs) == 0:\n",
    "        no_recs.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 2179 users without all 10 recommendations we would have liked to have.\n",
      "There were 51789 users with all 10 recommendations we would like them to have.\n",
      "There were 174 users with no recommendations at all!\n"
     ]
    }
   ],
   "source": [
    "# Some characteristics of my content based recommendations\n",
    "print(\"There were {} users without all 10 recommendations we would have liked to have.\".format(len(users_without_all_recs)))\n",
    "print(\"There were {} users with all 10 recommendations we would like them to have.\".format(len(users_with_all_recs)))\n",
    "print(\"There were {} users with no recommendations at all!\".format(len(no_recs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([457430])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Closer look at individual user characteristics\n",
    "user_items = reviews[['user_id', 'movie_id', 'rating']]\n",
    "user_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "\n",
    "def movies_watched(user_id):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id of an individual as int\n",
    "    OUTPUT:\n",
    "    movies - an array of movies the user has watched\n",
    "    '''\n",
    "    movies = user_by_movie.loc[user_id][user_by_movie.loc[user_id].isnull() == False].index.values\n",
    "\n",
    "    return movies\n",
    "\n",
    "\n",
    "movies_watched(189)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the movie lists for users without any recommendations include:\n",
      "189\n",
      "['El laberinto del fauno (2006)']\n",
      "797\n",
      "['The 414s (2015)']\n",
      "1603\n",
      "['Beauty and the Beast (2017)']\n",
      "2056\n",
      "['Brimstone (2016)']\n",
      "2438\n",
      "['Baby Driver (2017)']\n",
      "3322\n",
      "['Rosenberg (2013)']\n",
      "3925\n",
      "['El laberinto del fauno (2006)']\n",
      "4325\n",
      "['Beauty and the Beast (2017)']\n",
      "4773\n",
      "['The Frozen Ground (2013)']\n",
      "4869\n",
      "['Beauty and the Beast (2017)']\n",
      "4878\n",
      "['American Made (2017)']\n"
     ]
    }
   ],
   "source": [
    "cnter = 0\n",
    "print(\"Some of the movie lists for users without any recommendations include:\")\n",
    "for user_id in no_recs:\n",
    "    print(user_id)\n",
    "    print(get_movie_names(movies_watched(user_id)))\n",
    "    cnter+=1\n",
    "    if cnter > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now What?  \n",
    "\n",
    "Well, if you were really strict with your criteria for how similar two movies are (like I was initially), then you still have some users that don't have all 10 recommendations (and a small group of users who have no recommendations at all). \n",
    "\n",
    "As stated earlier, recommendation engines are a bit of an **art** and a **science**.  There are a number of things we still could look into - how do our collaborative filtering and content based recommendations compare to one another? How could we incorporate user input along with collaborative filtering and/or content based recommendations to improve any of our recommendations?  How can we truly gain recommendations for every user?\n",
    "\n",
    "`5.` In this last step feel free to explore any last ideas you have with the recommendation techniques we have looked at so far.  You might choose to make the final needed recommendations using the first technique with just top ranked movies.  You might also loosen up the strictness in the similarity needed between movies.  Be creative and share your insights with your classmates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22 Solution: Content Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23 Three Types of Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Main Branches of Recommendations\n",
    "- Knowledge Based\n",
    "- Collaborative Filtering\n",
    "- Content Based\n",
    "\n",
    "### Knowledge Based Recommendations\n",
    "Use knowledge about items that meet user specifications to recommend items.\n",
    "\n",
    "Common for luxury items.\n",
    "\n",
    "### Collaborative Filtering\n",
    "A method of making recommendations based on using the collaboration of user-item interactions.\n",
    "\n",
    "Most common recommendation method\n",
    "\n",
    "### Content Based\n",
    "Use information about items to find ite, similarities. Often the similarities are related to item descriptions or purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24 More Recommendation Technniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Types of Recommendations\n",
    "There are three methods that you have now implemented for making recommendations. These are the three most recognized methods in industry:\n",
    "\n",
    "`1.` Knowledge Based Recommendations\n",
    "\n",
    "Knowledge based recommendations frequently are implemented using filters, and are extremely common amongst luxury based goods. Filters that you might see when purchasing items like cars or homes are examples of knowledge based recommendations. In knowledge based recommendations, users provide information about the types of recommendations they would like back.\n",
    "\n",
    "`2.` Collaborative Filtering Based Recommendations\n",
    "\n",
    "Collaborative filtering uses the connections between users and items to make recommendations. Even the content based recommendation you just implemented used some collaborative filtering techniques, as you were not treating items and users independent from one another. In this lesson, you used neighborhood based collaborative filtering to find users who were alike and then recommend new movies based on these similar users.\n",
    "\n",
    "Even in the content based recommendation, you were using collaborative filtering. You were finding items that were similar and making recommendations of new items based on the highest ratings of a user. Because you were still using the user ratings of an item, this was an example of a blend between content and collaborative filtering based techniques.\n",
    "\n",
    "`3.` Content Based Recommendations\n",
    "\n",
    "In the previous notebook, you created a matrix of similarities between items (movies) based only on the content related to those movies (year and genre). The similarity matrix that was used, was completely created using only the items (movies). There was no information used about the users implemented. For any movie, you would be able to determine the most related additional movies based only on the genre and the year of the movie. This is the premise of how a completely content based recommendation would be made.\n",
    "\n",
    "Often blended techniques of all three types are used in practice to provide the the best recommendation for a particular circumstance.\n",
    "\n",
    "There are still more advanced techniques that are related to the methods that you learned about here, and they will most likely fall in one of the three buckets below.\n",
    "\n",
    "- [AirBnB uses embeddings in their recommendation, which you can read more about here.](https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e)\n",
    "\n",
    "![Cali](https://video.udacity-data.com/topher/2018/August/5b6f3e96_screen-shot-2018-08-11-at-12.52.21-pm/screen-shot-2018-08-11-at-12.52.21-pm.png)\n",
    "\n",
    "- [As our smart phones become more addicting every day, it is easy to see why location based recommendations will be more and more popular. You can read more about these types of recommendations here.](https://link.springer.com/referenceworkentry/10.1007%2F978-3-319-17885-1_1580)\n",
    "\n",
    "![Location](https://video.udacity-data.com/topher/2018/August/5b6f3eb7_screen-shot-2018-08-11-at-12.52.03-pm/screen-shot-2018-08-11-at-12.52.03-pm.png)\n",
    "\n",
    "- [Many companies are also exploring deep learning use cases in recommendation systems.](https://ebaytech.berlin/deep-learning-for-recommender-systems-48c786a20e1a)\n",
    "\n",
    "![Usernet](https://video.udacity-data.com/topher/2018/August/5b6f3f18_screen-shot-2018-08-11-at-12.54.48-pm/screen-shot-2018-08-11-at-12.54.48-pm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25 Quiz: Recommendation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Check\n",
    "Now that you have gained exposure to each of the different types of recommendation techniques, let's see if you can tell what type of recommendation engine is being used in each of the below cases.\n",
    "\n",
    "You are interested in purchasing a new shirt. You go through and look at a number of shirts online. The next day while you are surfing the web, an advertisement pops up that says other users who liked the shirts you were looking at also liked these shirts (with new shirts shown). This is an example of `Collaborative Filtering`\n",
    "\n",
    "You are interested in purchasing a new shirt. You go through and look at a number of shirts online. The next day while you are surfing the web, an advertisement pops up that says here are some other items that were like the items you viewed yesterday. This is an example of `Content Based`\n",
    "\n",
    "You are interested in purchasing a new shirt. You know you want a red shirt. You use a filter to select that you only want red shirts. This is an example of `Knowledge Based`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26 Types of Ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Scales\n",
    "If you are in control of choosing your rating scale, think of what might be most beneficial to your scenario. If you are working alongside a team TO design the interfaces for how data will be collected, there are number of ideas to keep in mind.\n",
    "- Do you need to ask questions of your user or can you collect data about their interactions with items?\n",
    "- If you need to ask questions, how many do you ask?\n",
    "- How do you word the questions?\n",
    "- And finally, the question in the above video: what type of scale should you use?\n",
    "\n",
    "In general, I suggest using the simplest rating that allows you to get whatever questions of interest you have, but there are some important ideas to keep in mind when choosing a particular type of rating. Ratings are a necessary part of working with different recommendation systems, but they aren't a central part of our focus. A good overview of types of ratings and when to use them is also provided [here](https://conversionxl.com/blog/survey-response-scales/).\n",
    "\n",
    "Most of these ideas are specific to your use case, and are easy to notice in hindsight. It is simply important to think of this in advance and not completely gloss over possible issues with the data you are collecting and how it connects to the questions you want answered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27 Goals of Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Goals of Recommendations\n",
    "In general, recommendations are important because they are often central to driving revenue for a company. In the new world of the Internet, you don't always need a friend to personally recommend you an item you never heard of - instead, the recommendations can do this.\n",
    "\n",
    "There are ultimately 4 goals to keep in mind when performing recommendations:\n",
    "1. Relevance\n",
    "2. Novelty\n",
    "3. Serendipity\n",
    "4. Diversity\n",
    "\n",
    "Often people only think of the first goal, and it is a good reminder that focusing only on **relevance** can lead to drops in use, and therefore loss of revenue. We also need to consider the other three goals, so that recommendations can be most effective in driving sales. In this [article](https://gab41.lab41.org/recommender-systems-its-not-all-about-the-accuracy-562c7dceeaff), Lab41 shows how they worked to quantify these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28 Quiz: Types of Ratings & Goals of Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to make sure that when a user gives you an item rating you are able to clearly identify if they like the item or not, which of the following are possible rating scales for this situation?\n",
    "- 1 for like, 0 for dislike\n",
    "- Scale 1 - 4\n",
    "- With odd valued scales, a user may choose to be undecided.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29 Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Recommendations\n",
    "In this lesson, you worked with the MovieTweetings data to apply each of the three methods of recommendations:\n",
    "1. Knowledge Based Recommendations\n",
    "2. Collaborative Filtering Based Recommendations\n",
    "3. Content Based Recommendations\n",
    "\n",
    "Within Collaborative Filtering, there are two main branches:\n",
    "1. Model Based Collaborative Filtering\n",
    "2. Neighborhood Based Collaborative Filtering\n",
    "\n",
    "In this lesson, you implemented Neighborhood Based Collaborative Filtering. In the next lesson, you will implement Model Based Collaborative Filtering.\n",
    "\n",
    "### Similarity Metrics\n",
    "In order to implement Neighborhood Based Collaborative Filtering, you were introduced to and applied a few techniques to assess how similar or distant two users were from one another:\n",
    "1. Pearson's correlation coefficient\n",
    "2. Spearman's correlation coefficient\n",
    "3. Kendall's Tau\n",
    "4. Euclidean Distance\n",
    "5. Manhattan Distance\n",
    "\n",
    "### Types of Ratings\n",
    "We took a quick look at different types of ratings:\n",
    "1. Did the user interact with an item or not.\n",
    "2. Did the user like an item or not.\n",
    "3. More granular scales 1-7, 1-10, etc.\n",
    "\n",
    "It is important to understand what the data might be used for, and what type of granularity might be important for a particular case. One of the main considerations is whether you want to have neutrality available, in which case an odd number of possible values in your scale will provide a value in the middle. Another common question is, how many levels do you really need to understand how much a user likes a particular product? Again, this is largely up to individual preference and specific use cases.\n",
    "\n",
    "### Business Cases For Recommendations\n",
    "Finally, you looked at the four ideas needed for businesses to implement successful recommendations to drive revenue, which include:\n",
    "1. Relevance\n",
    "2. Novelty\n",
    "3. Serendipity\n",
    "4. Increased Diversity\n",
    "\n",
    "At the end of this lesson, you will have gained a ton of skills to build upon or to start creating your own recommendations in practice.\n",
    "\n",
    "### Next Lesson\n",
    "In the upcoming lesson, we will take a closer look at model based collaborative filtering, different methods for dealing with the cold start problem, and how to assess how well our model is performing. Then as a final touch, you will have the opportunity to deploy your recommendations to the web!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
