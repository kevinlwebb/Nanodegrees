{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Answer\n",
    "- Are our recommendations good?\n",
    "- How to use ML for recommendations?\n",
    "- How to recommend to new users?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 What's Ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics\n",
    "In this lesson, you will learn about three main topics:\n",
    "1. We will look from a high level at how you might go about validating your recommendations.\n",
    "2. We will look at matrix factorization as a method to use machine learning to make recommendations.\n",
    "3. We will look at combining recommendation techniques to make predictions to existing and new users and for existing and new items.\n",
    "\n",
    "As we go through this lesson, you will come to realize that there are a lot of difficulties in working with recommendation engines which make them still an exciting field to study! This is especially true when you combine your recommendations with a specific product type.\n",
    "\n",
    "Recommending movies, recommending restaurants, or recommending clothing might happen in a number of different ways. However, the techniques you will learn in this lesson are often extendable to any of these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 How Do We Know Our Recommendations Are Good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Data For Recommendations\n",
    "In the last lesson, you were making recommendations by providing a list of popular items, or a list of items that the user hadn't observed but that someone with similar tastes had observed. However, understanding if these recommendations are good in practice means that you have to deploy these recommendations to users and see how it impacts your metrics (sales, higher engagement, clicks, conversions, etc.).\n",
    "\n",
    "You may not want your recommendations to go live to understand how well they work. In these cases, you will want to split your data into training and testing portions. In these cases, you can train your recommendation engine on a subset of the data, then you can test how well your recommendation engine performs on a test set of data before deploying your model to the world.\n",
    "\n",
    "However, the cases you saw in the last lesson, where just a list of recommendations was provided, don't actually lend themselves very well to training and testing methods of evaluation. In the next upcoming pages, you will be introduced to matrix factorization, which actually does work quite well for these situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Validating Your Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Testing\n",
    "For online methods of testing a recommender's performance, many of the methods you saw in the previous lesson work very well - you can deploy your recommendations and just watch your metrics carefully. It is common in practice to set up online recommendations to have an \"old\" version of recommended items, which is compared to a new page that uses a new recommendation strategy.\n",
    "\n",
    "All ideas associated with A/B testing that you learned in the earlier lessons are critical to watching your metrics in online learning, and ultimately, choosing a recommendation strategy that works best for your products and customers.\n",
    "\n",
    "### Offline Testing\n",
    "In many cases, a company might not let you simply deploy your recommendations out into the real world any time you feel like it. Testing out your recommendations in a training-testing environment prior to deploying them is called **offline** testing.\n",
    "\n",
    "The recommendation methods you built in the previous lesson actually don't work very well for offline testing. In offline testing, it is ideal to not just obtain a list of recommendations for each individual, because we ultimately don't know if a user doesn't use an item because they don't like it, or because they just haven't used it yet (but would like it). Rather, it would be great if we have an idea of how much each user would like each item using a predicted rating. Then we can compare this predicted rating to the actual rating any individual gives to an item in the future.\n",
    "\n",
    "In the previous video, you saw an example of a user to whom we gave a list of movies that they still hadn't seen. Therefore, we couldn't tell how well we were doing with our recommendations. Techniques related to matrix factorization lend themselves nicely to solving this problem.\n",
    "\n",
    "### User Groups\n",
    "The final (possible) method of validating your recommendations is by having user groups give feedback on items you would recommend for them. Obtaining good user groups that are representative of your customers can be a challenge on its own. This is especially true when you have a lot of products and a very large consumer base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Metric|Reg or Class|\n",
    "|-|-|\n",
    "|Accuracy|Classification|\n",
    "|Mean-Squared Error (MSE)|Regression|\n",
    "|Precision|Classification|\n",
    "|Recall|Classification|\n",
    "|R-Squared|Regression|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Why SVD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "In the next part of this lesson, you will first get exposure to Singular Value Decomposition, or SVD. We will soon see why this technique falls short for many recommendation problems. However, understanding traditional SVD approaches to matrix factorization is useful as a start to a number of matrix factorization techniques that are possible in practice.\n",
    "\n",
    "In order to implement SVD for many recommendation engines, we will need to use a slightly modified approach known as FunkSVD. This approach proved to work incredibly well during the [Netflix competition](https://en.wikipedia.org/wiki/Netflix_Prize), and therefore, it is one of the most popular recommendation approaches in use today.\n",
    "\n",
    "Let's first take a closer look at traditional SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Latent Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Factors\n",
    "When performing SVD, we create a matrix of users by items (or customers by movies in our specific example), with user ratings for each item scattered throughout the matrix. An example is shown in the image below.\n",
    "\n",
    "![User and Items](https://video.udacity-data.com/topher/2018/September/5b9b0faf_screen-shot-2018-09-13-at-6.32.03-pm/screen-shot-2018-09-13-at-6.32.03-pm.png)\n",
    "\n",
    "You can see that this matrix doesn't have any specific information about the users or items. Rather, it just holds the ratings that each user gave to each item. Using SVD on this matrix, we can find latent features related to the movies and customers. This is amazing because the dataset doesn't contain any information about the customers or movies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Latent Factors: Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Factor\n",
    "- Values that are not directly observable in our data, but may be recognized when looking at relationships and trends that exist between observed data values.\n",
    "\n",
    "Imagine a situation in which you collect 1-5 ratings data from a bunch of people related to how they feel about lots of different animals (birds, horses, cats, dogs, etc.). What are some examples of possible latent factors you might observe in this ratings data?\n",
    "- ~~The observed rating values~~\n",
    "- The size of the animal.\n",
    "- How many legs the animal has.\n",
    "- Whether the animal can fly or not.\n",
    "\n",
    "The ratings are observed, so they are not latent! However, the other three variables are latent in that we wouldn't have raw data on these values by just collecting ratings of how a user feels about each animal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick check of understanding. If we let AA be our user-item matrix, we can write the decomposition of that matrix in the following way.\n",
    "\n",
    "$A = U \\Sigma V^T$\n",
    " \n",
    "\n",
    "$U$\n",
    "- A matrix that provides how users feel about latent features.\n",
    "\n",
    "$\\Sigma$\n",
    "- A matrix that provides weights in descending order with how much each latent feature matters towards reconstructing the original user-item matrix.\n",
    "\n",
    "$V$\n",
    "- A matrix that provides how items (movies in this case) relate to each latent feature ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 SVD Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "So far in this lesson, you have gained some exposure to Singular Value Decomposition.  In this notebook, you will get some hands on practice with this technique.\n",
    "\n",
    "Let's get started by reading in our libraries and setting up the data we will be using throughout this notebook\n",
    "\n",
    "`1.` Run the cell below to create the **user_movie_subset** dataframe.  This will be the dataframe you will be using for the first part of this notebook.\n",
    "\n",
    "**Note: Unstacking the matrix here could take ~10 mins to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_id  73486  75314  68646  99685\n",
      "user_id                             \n",
      "265        10.0   10.0   10.0   10.0\n",
      "1023       10.0    4.0    9.0   10.0\n",
      "1683        8.0    9.0   10.0    5.0\n",
      "6571        9.0    8.0   10.0   10.0\n",
      "11639      10.0    5.0    9.0    9.0\n",
      "13006       6.0    4.0   10.0    6.0\n",
      "14076       9.0    8.0   10.0    9.0\n",
      "14725      10.0    5.0    9.0    8.0\n",
      "23548       7.0    8.0   10.0    8.0\n",
      "24760       9.0    5.0    9.0    7.0\n",
      "28713       9.0    8.0   10.0    8.0\n",
      "30685       9.0   10.0   10.0    9.0\n",
      "34110      10.0    9.0   10.0    8.0\n",
      "34430       5.0    8.0    5.0    8.0\n",
      "35150      10.0    8.0   10.0   10.0\n",
      "43294       9.0    9.0   10.0   10.0\n",
      "46849       9.0    8.0    8.0    8.0\n",
      "50556      10.0    8.0    1.0   10.0\n",
      "51382       5.0    6.0   10.0   10.0\n",
      "51410       8.0    7.0   10.0    7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"4.7 Class Notebooks\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import svd_tests as t\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('data/movies_clean.csv')\n",
    "reviews = pd.read_csv('data/reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']\n",
    "\n",
    "# Create user-by-item matrix\n",
    "user_items = reviews[['user_id', 'movie_id', 'rating']]\n",
    "user_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "\n",
    "user_movie_subset = user_by_movie[[73486, 75314,  68646, 99685]].dropna(axis=0)\n",
    "print(user_movie_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have the **user_movie_subset** matrix, use this matrix to correctly match each key to the correct value in the dictionary below.  Use the cells below the dictionary as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  There are 20 users in the dataset, which is given by the number of rows. There are 4 movies in the dataset given by the number of columns.  You can find the movies or users with the highest average ratings by taking the mean of each row or column.  Using the movies table, you can find the movie names associated with each id.  This shows the top rated movie is The Godfather!\n"
     ]
    }
   ],
   "source": [
    "# match each letter to the best statement in the dictionary below - each will be used at most once\n",
    "a = 20\n",
    "b = 68646\n",
    "c = 'The Godfather'\n",
    "d = 'Goodfellas'\n",
    "e = 265\n",
    "f = 30685\n",
    "g = 4\n",
    "\n",
    "sol_1_dict = {\n",
    "    'the number of users in the user_movie_subset': a,\n",
    "    'the number of movies in the user_movie_subset': g,\n",
    "    'the user_id with the highest average ratings given': e,\n",
    "    'the movie_id with the highest average ratings received': b,\n",
    "    'the name of the movie that received the highest average rating': c\n",
    "}\n",
    "\n",
    "\n",
    "#test dictionary here\n",
    "t.test1(sol_1_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id\n",
      "265      10.00\n",
      "1023      8.25\n",
      "1683      8.00\n",
      "6571      9.25\n",
      "11639     8.25\n",
      "13006     6.50\n",
      "14076     9.00\n",
      "14725     8.00\n",
      "23548     8.25\n",
      "24760     7.50\n",
      "28713     8.75\n",
      "30685     9.50\n",
      "34110     9.25\n",
      "34430     6.50\n",
      "35150     9.50\n",
      "43294     9.50\n",
      "46849     8.25\n",
      "50556     7.25\n",
      "51382     7.75\n",
      "51410     8.00\n",
      "dtype: float64\n",
      "movie_id\n",
      "73486    8.60\n",
      "75314    7.35\n",
      "68646    9.00\n",
      "99685    8.50\n",
      "dtype: float64\n",
      "4187    One Flew Over the Cuckoo's Nest (1975)\n",
      "Name: movie, dtype: object\n",
      "4361    Taxi Driver (1976)\n",
      "Name: movie, dtype: object\n",
      "3706    The Godfather (1972)\n",
      "Name: movie, dtype: object\n",
      "6917    Goodfellas (1990)\n",
      "Name: movie, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell for work\n",
    "# user with the highest average rating\n",
    "print(user_movie_subset.mean(axis=1))\n",
    "\n",
    "# movie with highest average rating\n",
    "print(user_movie_subset.mean(axis=0))\n",
    "\n",
    "# list of movie names\n",
    "for movie_id in [73486, 75314,  68646, 99685]:\n",
    "    print(movies[movies['movie_id'] == movie_id]['movie'])\n",
    "    \n",
    "# users by movies\n",
    "user_movie_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a little more context about the matrix we will be performing Singular Value Decomposition on, we're going to do just that.  To get started, let's remind ourselves about the dimensions of each of the matrices we are going to get back.   Essentially, we are going to split the **user_movie_subset** matrix into three matrices:\n",
    "\n",
    "$$ U \\Sigma V^T $$\n",
    "\n",
    "\n",
    "`3.` Given what you learned about in the previous parts of this lesson, provide the dimensions for each of the matrices specified above using the dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  We will now put this to use, so you can see how the dot product of these matrices come together to create our user item matrix.  The number of latent features will control the sigma matrix as well, and this will a square matrix that will at most be the minimum of the number of users and number of movies (in our case the minimum is the 4 movies).\n"
     ]
    }
   ],
   "source": [
    "# match each letter in the dictionary below - a letter may appear more than once.\n",
    "a = 'a number that you can choose as the number of latent features to keep'\n",
    "b = 'the number of users'\n",
    "c = 'the number of movies'\n",
    "d = 'the sum of the number of users and movies'\n",
    "e = 'the product of the number of users and movies'\n",
    "\n",
    "sol_2_dict = {\n",
    "    'the number of rows in the U matrix': b, \n",
    "    'the number of columns in the U matrix': a, \n",
    "    'the number of rows in the V transpose matrix': a, \n",
    "    'the number of columns in the V transpose matrix': c\n",
    "}\n",
    "\n",
    "#test dictionary here\n",
    "t.test2(sol_2_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's verify the above dimensions by performing SVD on our user-movie matrix.\n",
    "\n",
    "`4.` Below you can find the code used to perform SVD in numpy.  You can see more about this functionality in the [documentation here](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.svd.html).  What do you notice about the shapes of your matrices?  If you try to take the dot product of the three objects you get back, can you directly do this to get back the user-movie matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (20, 20), (4, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, s, vt = np.linalg.svd(user_movie_subset)\n",
    "s.shape, u.shape, vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at the dimensions of the three returned objects, we can see the following:\n",
      "\n",
      " 1. The u matrix is a square matrix with the number of rows and columns equaling the number of users. \n",
      "\n",
      " 2. The v transpose matrix is also a square matrix with the number of rows and columns equaling the number of items.\n",
      "\n",
      " 3. The sigma matrix is actually returned as just an array with 4 values.  \n",
      "\n",
      " In order to set up the matrices in a way that they can be multiplied together, we have a few steps to perform: \n",
      "\n",
      " 1. Turn sigma into a square matrix with the number of latent features we would like to keep. \n",
      "\n",
      " 2. Change the columns of u and the rows of v transpose to match this number of dimensions. \n",
      "\n",
      " If we would like to exactly re-create the user-movie matrix, we could choose to keep all of the latent features.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell for our thoughts on the questions posted above\n",
    "t.question4thoughts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use the thoughts from the above question to create u, s, and vt with four latent features.  When you have all three matrices created correctly, run the test below to show that the dot product of the three matrices creates the original user-movie matrix.  The matrices should have the following dimensions:\n",
    "\n",
    "$$ U_{n x k} $$\n",
    "\n",
    "$$\\Sigma_{k x k} $$\n",
    "\n",
    "$$V^T_{k x m} $$\n",
    "\n",
    "where:\n",
    "\n",
    "1. n is the number of users\n",
    "2. k is the number of latent features to keep (4 for this case)\n",
    "3. m is the number of movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the dimensions of u, s, and vt as necessary to use four latent features\n",
    "# update the shape of u and store in u_new\n",
    "u_new = u[:, :len(s)]\n",
    "\n",
    "# update the shape of s and store in s_new\n",
    "s_new = np.zeros((len(s), len(s)))\n",
    "s_new[:len(s), :len(s)] = np.diag(s) \n",
    "\n",
    "# Because we are using 4 latent features and there are only 4 movies, \n",
    "# vt and vt_new are the same\n",
    "vt_new = vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right! The dimensions of u should be 20 x 4, and both v transpose and sigma should be 4 x 4.  The dot product of the three matrices how equals the original user-movie matrix!\n"
     ]
    }
   ],
   "source": [
    "# Check your matrices against the solution\n",
    "assert u_new.shape == (20, 4), \"Oops!  The shape of the u matrix doesn't look right. It should be 20 by 4.\"\n",
    "assert s_new.shape == (4, 4), \"Oops!  The shape of the sigma matrix doesn't look right.  It should be 4 x 4.\"\n",
    "assert vt_new.shape == (4, 4), \"Oops! The shape of the v transpose matrix doesn't look right.  It should be 4 x 4.\"\n",
    "assert np.allclose(np.dot(np.dot(u_new, s_new), vt_new), user_movie_subset), \"Oops!  Something went wrong with the dot product.  Your result didn't reproduce the original movie_user matrix.\"\n",
    "print(\"That's right! The dimensions of u should be 20 x 4, and both v transpose and sigma should be 4 x 4.  The dot product of the three matrices how equals the original user-movie matrix!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the sigma matrix can actually tell us how much of the original variability in the user-movie matrix is captured by each latent feature.  The total amount of variability to be explained is the sum of the squared diagonal elements.  The amount of variability explained by the first componenet is the square of the first value in the diagonal.  The amount of variability explained by the second componenet is the square of the second value in the diagonal.   \n",
    "\n",
    "`6.` Using the above information, can you determine the amount of variability in the original user-movie matrix that can be explained by only using the first two components? Use the cell below for your work, and then test your answer against the solution with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total variance in the original matrix is 5877.0.\n",
      "Ther percentage of variability captured by the first two components is 98.55%.\n"
     ]
    }
   ],
   "source": [
    "total_var = np.sum(s**2)\n",
    "var_exp_comp1_and_comp2 = s[0]**2 + s[1]**2\n",
    "perc_exp = round(var_exp_comp1_and_comp2/total_var*100, 2)\n",
    "print(\"The total variance in the original matrix is {}.\".format(total_var))\n",
    "print(\"Ther percentage of variability captured by the first two components is {}%.\".format(perc_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yup!  That all looks good!\n"
     ]
    }
   ],
   "source": [
    "assert np.round(perc_exp, 2) == 98.55, \"Oops!  That doesn't look quite right.  You should have total variability as the sum of all the squared elements in the sigma matrix.  Then just the sum of the squared first two elements is the amount explained by the first two latent features.  Try again.\"\n",
    "print(\"Yup!  That all looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`7.` Similar to in the previous question, change the shapes of your u, sigma, and v transpose matrices.  However, this time consider only using the first 2 components to reproduce the user-movie matrix instead of all 4. After you have your matrices set up, check your matrices against the solution by running the tests.  The matrices should have the following dimensions:\n",
    "\n",
    "$$ U_{n x k} $$\n",
    "\n",
    "$$\\Sigma_{k x k} $$\n",
    "\n",
    "$$V^T_{k x m} $$\n",
    "\n",
    "where:\n",
    "\n",
    "1. n is the number of users\n",
    "2. k is the number of latent features to keep (2 for this case)\n",
    "3. m is the number of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the dimensions of u, s, and vt as necessary to use four latent features\n",
    "# update the shape of u and store in u_new\n",
    "k = 2\n",
    "u_2 = u[:, :k]\n",
    "\n",
    "# update the shape of s and store in s_new\n",
    "s_2 = np.zeros((k, k))\n",
    "s_2[:k, :k] = np.diag(s[:k]) \n",
    "\n",
    "# Because we are using 2 latent features, we need to update vt this time\n",
    "vt_2 = vt[:k, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right! The dimensions of u should be 20 x 2, sigma should be 2 x 2, and v transpose should be 2 x 4. \n",
      "\n",
      " The question is now that we don't have all of the latent features, how well can we really re-create the original user-movie matrix?\n"
     ]
    }
   ],
   "source": [
    "# Check that your matrices are the correct shapes\n",
    "assert u_2.shape == (20, 2), \"Oops!  The shape of the u matrix doesn't look right. It should be 20 by 2.\"\n",
    "assert s_2.shape == (2, 2), \"Oops!  The shape of the sigma matrix doesn't look right.  It should be 2 x 2.\"\n",
    "assert vt_2.shape == (2, 4), \"Oops! The shape of the v transpose matrix doesn't look right.  It should be 2 x 4.\"\n",
    "print(\"That's right! The dimensions of u should be 20 x 2, sigma should be 2 x 2, and v transpose should be 2 x 4. \\n\\n The question is now that we don't have all of the latent features, how well can we really re-create the original user-movie matrix?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`8.` When using all 4 latent features, we saw that we could exactly reproduce the user-movie matrix.  Now that we only have 2 latent features, we might measure how well we are able to reproduce the original matrix by looking at the sum of squared errors from each rating produced by taking the dot product as compared to the actual rating.  Find the sum of squared error based on only the two latent features, and use the following cell to test against the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the dot product\n",
    "pred_ratings = np.dot(np.dot(u_2, s_2), vt_2)\n",
    "\n",
    "# Compute the squared error for each predicted vs. actual rating\n",
    "sum_square_errs = np.sum(np.sum((user_movie_subset - pred_ratings)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That looks right!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "# Check against the solution\n",
    "assert np.round(sum_square_errs, 2) == 85.34, \"Oops!  That doesn't look quite right.  You should return a single number for the whole matrix.\"\n",
    "print(\"That looks right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you may be thinking... why would we want to choose a k that doesn't just give us back the full user-movie matrix with all the original ratings.  This is a good question.  One reason might be for computational reasons - sure, you may want to reduce the dimensionality of the data you are keeping, but really this isn't the main reason we would want to perform reduce k to lesser than the minimum of the number of movies or users.\n",
    "\n",
    "Let's take a step back for a second.  In this example we just went through, your matrix was very clean.  That is, for every user-movie combination, we had a rating.  **There were no missing values.** But what we know from the previous lesson is that the user-movie matrix is full of missing values.  \n",
    "\n",
    "A matrix similar to the one we just performed SVD on:\n",
    "\n",
    "<img src=\"https://view9qadl8i8k4.udacity-student-workspaces.com/notebooks/imgs/nice_ex.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "The real world:\n",
    "\n",
    "<img src=\"https://view9qadl8i8k4.udacity-student-workspaces.com/notebooks/imgs/real_ex.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "\n",
    "Therefore, if we keep all k latent features it is likely that latent features with smaller values in the sigma matrix will explain variability that is probably due to noise and not signal. Furthermore, if we use these \"noisey\" latent features to assist in re-constructing the original user-movie matrix it will potentially (and likely) lead to worse ratings than if we only have latent features associated with signal.   \n",
    "\n",
    "`9.` Let's try introducing just a little of the real world into this example by performing SVD on a matrix with missing values.  Below I have added a new user to our matrix who hasn't rated all four of our movies.  Try performing SVD on the new matrix.  What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5a48ecbcb5c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Try svd with this new matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_movie_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/joshuabernhard/anaconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joshuabernhard/anaconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_svd_nonconvergence\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_svd_nonconvergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVD did not converge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: SVD did not converge"
     ]
    }
   ],
   "source": [
    "# This line adds one nan value as the very first entry in our matrix\n",
    "user_movie_subset.iloc[0, 0] = np.nan\n",
    "\n",
    "# Try svd with this new matrix\n",
    "u, s, vt = np.linalg.svd(user_movie_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Even with just one nan value we cannot perform SVD!  This is going to be a huge problem, because our real dataset has nan values everywhere!  This is where FunkSVD comes in to help.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 SVD Practice Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 SVD Practice Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition Takeaways\n",
    "Three main takeaways from the previous notebook:\n",
    "1. The latent factors retrieved from SVD aren't actually labeled.\n",
    "2. We can get an idea of how many latent factors we might want to keep by using the Sigma matrix.\n",
    "3. SVD in NumPy will not work when our matrix has missing values. **This makes this technique less than useful for our current user-movie matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 SVD Closed Form Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is A Closed Form Solution?\n",
    "A closed form solution is one where you can directly find the solution values (unlike iterative solutions, which are commonly used in practice). There isn't an iterative approach to solving a particular equation. One of the most popular examples of a closed form solution is the solution for multiple linear regression. That is if we want to find an estimate for \\betaβ in the following situation:\n",
    "\n",
    "$y = X\\beta$\n",
    "\n",
    "We can find it by computing the **Best Linear Unbiased Estimate (BLUE)**. It can be found **in closed form** using the equation:\n",
    "\n",
    "$\\hat{\\beta} = (X'X)^-X'y$\n",
    "\n",
    "where **X** is a matrix of explanatory inputs and **y** is a response vector.\n",
    "\n",
    "Another common example of a closed form solution is the quadratic equation. If we want to find **x** that solves:\n",
    "\n",
    "$ax^2 + bx + c = 0$\n",
    "\n",
    "We can find these values using the quadratic formula:\n",
    "\n",
    "$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ \n",
    "\n",
    "**Each of these is an example of a closed form solution, because in each case we have an equation that allows us to solve directly for our values of interest.**\n",
    "\n",
    "### Closed Form Solutions for SVD\n",
    "It turns out there is a closed form solution for Singular Value Decomposition that can be used to identify each of the matrices of interest (U, \\Sigma, VU,Σ,V). The most straightforward explanation of this closed form solution can be found at this [MIT link](http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm).\n",
    "\n",
    "As put in the paper -\n",
    "\n",
    "\"Calculating the SVD consists of finding the eigenvalues and eigenvectors of $AA'$ and $A'A$. The eigenvectors of $A'A$ make up the columns of $V$, the eigenvectors of $AA'$ make up the columns of $U$. Also, the singular values in $\\Sigma$ are square roots of eigenvalues from $AA'$ or $A'A$. The singular values are the diagonal entries of the $\\sigma$ matrix and are arranged in descending order. The singular values are always real numbers. If the matrix $A$ is a real matrix, then $U$ and $V$ are also real.\"\n",
    "\n",
    "Again, you can see a fully worked example of the closed form solution at the [MIT Link here](http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm).\n",
    "\n",
    "### A More Common Approach\n",
    "The main issue with the closed form solution (especially for us) is that it doesn't actually work when we have missing data. Instead, Simon Funk (and then many followers) came up with other solutions for finding our matrices of interest in these cases using **gradient descent**.\n",
    "\n",
    "So all of this is to say, people don't really use the closed form solution for SVD, and therefore, we aren't going to spend a lot of time on it either. The link above is all you need to know. Now, we are going to look at the main way that the matrices in SVD are estimated, as this is what is used for estimating values in FunkSVD.\n",
    "\n",
    "## Additional Resources\n",
    "Below are some additional resources in case you are looking for others that go beyond what was shown in the simplified MIT paper.\n",
    "- [Stanford Discussion on SVD](http://infolab.stanford.edu/~ullman/mmds/ch11.pdf)\n",
    "- [Why are Singular Values Always Positive on StackExchange](https://math.stackexchange.com/questions/2060572/why-are-singular-values-always-non-negative)\n",
    "- [An additional resource for SVD in Python](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/)\n",
    "- [Using Missing Values to Improve Recommendations in SVD](https://www.hindawi.com/journals/mpe/2015/380472/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 FunkSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funk SVD Practice\n",
    "In the notebook on the next page, you will be writing the code to implement Funk SVD. Before you dive in, let's do a practice run here.\n",
    "\n",
    "First, consider we have a user-item matrix that looks like the matrix below, where we want to make an update of **U** and **V** matrices based on the 4 highlighted.\n",
    "\n",
    "![](https://video.udacity-data.com/topher/2018/September/5b9beced_screen-shot-2018-09-14-at-10.16.10-am/screen-shot-2018-09-14-at-10.16.10-am.png)\n",
    "\n",
    "Also consider we have the following U and V matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 Implementing FunkSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing FunkSVD - Solution\n",
    "\n",
    "In this notebook we will take a look at writing our own function that performs FunkSVD, which will follow the steps you saw in the previous video.  If you find that you aren't ready to tackle this task on your own, feel free to skip to the following video where you can watch as I walk through the steps.\n",
    "\n",
    "To test our algorithm, we will run it on the subset of the data you worked with earlier.  Run the cell below to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` You will use the **user_movie_subset** matrix to show that your FunkSVD algorithm will converge.  In the below cell, use the comments and document string to assist you as you complete writing your own function to complete FunkSVD.  You may also want to try to complete the funtion on your own without the assistance of comments.  You may feel free to remove and add to the function in any way that gets you a working solution! \n",
    "\n",
    "**Notice:** There isn't a sigma matrix in this version of matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunkSVD(ratings_mat, latent_features=4, learning_rate=0.0001, iters=100):\n",
    "    '''\n",
    "    This function performs matrix factorization using a basic form of FunkSVD with no regularization\n",
    "    \n",
    "    INPUT:\n",
    "    ratings_mat - (numpy array) a matrix with users as rows, movies as columns, and ratings as values\n",
    "    latent_features - (int) the number of latent features used\n",
    "    learning_rate - (float) the learning rate \n",
    "    iters - (int) the number of iterations\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_mat - (numpy array) a user by latent feature matrix\n",
    "    movie_mat - (numpy array) a latent feature by movie matrix\n",
    "    '''\n",
    "    \n",
    "    # Set up useful values to be used through the rest of the function\n",
    "    n_users = ratings_mat.shape[0]\n",
    "    n_movies = ratings_mat.shape[1]\n",
    "    num_ratings = np.count_nonzero(~np.isnan(ratings_mat))\n",
    "    \n",
    "    # initialize the user and movie matrices with random values\n",
    "    user_mat = np.random.rand(n_users, latent_features)\n",
    "    movie_mat = np.random.rand(latent_features, n_movies)\n",
    "    \n",
    "    # initialize sse at 0 for first iteration\n",
    "    sse_accum = 0\n",
    "    \n",
    "    # header for running results\n",
    "    print(\"Optimizaiton Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "    \n",
    "    # for each iteration\n",
    "    for iteration in range(iters):\n",
    "\n",
    "        # update our sse\n",
    "        old_sse = sse_accum\n",
    "        sse_accum = 0\n",
    "        \n",
    "        # For each user-movie pair\n",
    "        for i in range(n_users):\n",
    "            for j in range(n_movies):\n",
    "                \n",
    "                # if the rating exists\n",
    "                if ratings_mat[i, j] > 0:\n",
    "                    \n",
    "                    # compute the error as the actual minus the dot product of the user and movie latent features\n",
    "                    diff = ratings_mat[i, j] - np.dot(user_mat[i, :], movie_mat[:, j])\n",
    "                    \n",
    "                    # Keep track of the sum of squared errors for the matrix\n",
    "                    sse_accum += diff**2\n",
    "                    \n",
    "                    # update the values in each matrix in the direction of the gradient\n",
    "                    for k in range(latent_features):\n",
    "                        user_mat[i, k] += learning_rate * (2*diff*movie_mat[k, j])\n",
    "                        movie_mat[k, j] += learning_rate * (2*diff*user_mat[i, k])\n",
    "\n",
    "        # print results for iteration\n",
    "        print(\"%d \\t\\t %f\" % (iteration+1, sse_accum / num_ratings))\n",
    "        \n",
    "    return user_mat, movie_mat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Try out your function on the **user_movie_subset** dataset.  First try 4 latent features, a learning rate of 0.005, and 10 iterations.  When you take the dot product of the resulting U and V matrices, how does the resulting **user_movie** matrix compare to the original subset of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "1 \t\t 48.464341\n",
      "2 \t\t 19.152872\n",
      "3 \t\t 5.072273\n",
      "4 \t\t 2.903386\n",
      "5 \t\t 2.702262\n",
      "6 \t\t 2.658502\n",
      "7 \t\t 2.630656\n",
      "8 \t\t 2.603075\n",
      "9 \t\t 2.572771\n",
      "10 \t\t 2.538804\n"
     ]
    }
   ],
   "source": [
    "user_mat, movie_mat = FunkSVD(ratings_mat, latent_features=4, learning_rate=0.005, iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.28729394   9.18159883  10.3011113   10.10854516]\n",
      " [  8.40901963   7.25897547   9.17647614   8.96236641]\n",
      " [  7.96434463   6.96572226   8.29078269   7.62868227]\n",
      " [  9.49745495   8.16283023  10.0978691    9.6359178 ]\n",
      " [  8.44627578   7.27872196   9.0380299    8.64455622]\n",
      " [  6.93472885   6.11365411   7.10425854   6.83177471]\n",
      " [  9.13015257   7.93629198   9.75497583   9.22649788]\n",
      " [  8.24952963   7.03525062   8.87468261   8.03327882]\n",
      " [  8.30034121   7.13815678   9.19308563   8.50367206]\n",
      " [  7.75389741   6.62928807   8.24367755   7.5336925 ]\n",
      " [  8.90593631   7.57443516   9.54381082   8.76562802]\n",
      " [  9.68793559   8.53054925   9.9485221    9.55102636]\n",
      " [  9.29353082   8.45644298   9.40382861   9.31821645]\n",
      " [  6.64271593   5.7035421    7.01594244   6.62622346]\n",
      " [  9.60466595   8.29849156  10.21659348   9.96836171]\n",
      " [  9.88644628   8.80495757  10.09719035   9.61751354]\n",
      " [  8.35964657   7.52516593   8.2682908    8.46863883]\n",
      " [  7.33621077   6.63868097   7.08323205   7.0246992 ]\n",
      " [  8.25256313   6.95004658   9.05355206   8.48356331]\n",
      " [  8.17132078   7.11215273   8.47818648   8.13304922]]\n",
      "[[ 10.  10.  10.  10.]\n",
      " [ 10.   4.   9.  10.]\n",
      " [  8.   9.  10.   5.]\n",
      " [  9.   8.  10.  10.]\n",
      " [ 10.   5.   9.   9.]\n",
      " [  6.   4.  10.   6.]\n",
      " [  9.   8.  10.   9.]\n",
      " [ 10.   5.   9.   8.]\n",
      " [  7.   8.  10.   8.]\n",
      " [  9.   5.   9.   7.]\n",
      " [  9.   8.  10.   8.]\n",
      " [  9.  10.  10.   9.]\n",
      " [ 10.   9.  10.   8.]\n",
      " [  5.   8.   5.   8.]\n",
      " [ 10.   8.  10.  10.]\n",
      " [  9.   9.  10.  10.]\n",
      " [  9.   8.   8.   8.]\n",
      " [ 10.   8.   1.  10.]\n",
      " [  5.   6.  10.  10.]\n",
      " [  8.   7.  10.   7.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The predicted ratings from the dot product are already starting to look a lot like the original data values even after only 10 iterations.  You can see some extreme low values that are not captured well yet.  The 5 in the second to last row in the first column is predicted as an 8, and the 4 in the second row and second column is predicted to be a 7.  Clearly the model is not done learning, but things are looking good.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Let's try out the function again on the **user_movie_subset** dataset.  This time we will again use 4 latent features and a learning rate of 0.005.  However, let's bump up the number of iterations to 250.  When you take the dot product of the resulting U and V matrices, how does the resulting **user_movie** matrix compare to the original subset of the data?  What do you notice about your error at the end of the 250 iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "1 \t\t 41.541069\n",
      "2 \t\t 13.482928\n",
      "3 \t\t 3.816577\n",
      "4 \t\t 2.809316\n",
      "5 \t\t 2.701528\n",
      "6 \t\t 2.657173\n",
      "7 \t\t 2.621357\n",
      "8 \t\t 2.583958\n",
      "9 \t\t 2.541877\n",
      "10 \t\t 2.493690\n",
      "11 \t\t 2.438410\n",
      "12 \t\t 2.375254\n",
      "13 \t\t 2.303660\n",
      "14 \t\t 2.223379\n",
      "15 \t\t 2.134590\n",
      "16 \t\t 2.038004\n",
      "17 \t\t 1.934932\n",
      "18 \t\t 1.827282\n",
      "19 \t\t 1.717461\n",
      "20 \t\t 1.608176\n",
      "21 \t\t 1.502150\n",
      "22 \t\t 1.401817\n",
      "23 \t\t 1.309049\n",
      "24 \t\t 1.224998\n",
      "25 \t\t 1.150071\n",
      "26 \t\t 1.084029\n",
      "27 \t\t 1.026167\n",
      "28 \t\t 0.975528\n",
      "29 \t\t 0.931074\n",
      "30 \t\t 0.891823\n",
      "31 \t\t 0.856916\n",
      "32 \t\t 0.825642\n",
      "33 \t\t 0.797428\n",
      "34 \t\t 0.771816\n",
      "35 \t\t 0.748433\n",
      "36 \t\t 0.726966\n",
      "37 \t\t 0.707140\n",
      "38 \t\t 0.688703\n",
      "39 \t\t 0.671425\n",
      "40 \t\t 0.655085\n",
      "41 \t\t 0.639475\n",
      "42 \t\t 0.624401\n",
      "43 \t\t 0.609680\n",
      "44 \t\t 0.595143\n",
      "45 \t\t 0.580637\n",
      "46 \t\t 0.566024\n",
      "47 \t\t 0.551182\n",
      "48 \t\t 0.536002\n",
      "49 \t\t 0.520396\n",
      "50 \t\t 0.504288\n",
      "51 \t\t 0.487622\n",
      "52 \t\t 0.470359\n",
      "53 \t\t 0.452479\n",
      "54 \t\t 0.433982\n",
      "55 \t\t 0.414890\n",
      "56 \t\t 0.395245\n",
      "57 \t\t 0.375112\n",
      "58 \t\t 0.354576\n",
      "59 \t\t 0.333746\n",
      "60 \t\t 0.312745\n",
      "61 \t\t 0.291713\n",
      "62 \t\t 0.270803\n",
      "63 \t\t 0.250170\n",
      "64 \t\t 0.229974\n",
      "65 \t\t 0.210368\n",
      "66 \t\t 0.191494\n",
      "67 \t\t 0.173479\n",
      "68 \t\t 0.156430\n",
      "69 \t\t 0.140430\n",
      "70 \t\t 0.125539\n",
      "71 \t\t 0.111789\n",
      "72 \t\t 0.099190\n",
      "73 \t\t 0.087728\n",
      "74 \t\t 0.077371\n",
      "75 \t\t 0.068071\n",
      "76 \t\t 0.059766\n",
      "77 \t\t 0.052389\n",
      "78 \t\t 0.045864\n",
      "79 \t\t 0.040116\n",
      "80 \t\t 0.035068\n",
      "81 \t\t 0.030648\n",
      "82 \t\t 0.026785\n",
      "83 \t\t 0.023415\n",
      "84 \t\t 0.020479\n",
      "85 \t\t 0.017921\n",
      "86 \t\t 0.015695\n",
      "87 \t\t 0.013757\n",
      "88 \t\t 0.012070\n",
      "89 \t\t 0.010599\n",
      "90 \t\t 0.009317\n",
      "91 \t\t 0.008199\n",
      "92 \t\t 0.007221\n",
      "93 \t\t 0.006366\n",
      "94 \t\t 0.005618\n",
      "95 \t\t 0.004962\n",
      "96 \t\t 0.004386\n",
      "97 \t\t 0.003880\n",
      "98 \t\t 0.003435\n",
      "99 \t\t 0.003043\n",
      "100 \t\t 0.002697\n",
      "101 \t\t 0.002392\n",
      "102 \t\t 0.002123\n",
      "103 \t\t 0.001885\n",
      "104 \t\t 0.001674\n",
      "105 \t\t 0.001487\n",
      "106 \t\t 0.001322\n",
      "107 \t\t 0.001176\n",
      "108 \t\t 0.001046\n",
      "109 \t\t 0.000930\n",
      "110 \t\t 0.000828\n",
      "111 \t\t 0.000737\n",
      "112 \t\t 0.000656\n",
      "113 \t\t 0.000584\n",
      "114 \t\t 0.000520\n",
      "115 \t\t 0.000463\n",
      "116 \t\t 0.000412\n",
      "117 \t\t 0.000367\n",
      "118 \t\t 0.000327\n",
      "119 \t\t 0.000292\n",
      "120 \t\t 0.000260\n",
      "121 \t\t 0.000232\n",
      "122 \t\t 0.000206\n",
      "123 \t\t 0.000184\n",
      "124 \t\t 0.000164\n",
      "125 \t\t 0.000146\n",
      "126 \t\t 0.000130\n",
      "127 \t\t 0.000116\n",
      "128 \t\t 0.000103\n",
      "129 \t\t 0.000092\n",
      "130 \t\t 0.000082\n",
      "131 \t\t 0.000073\n",
      "132 \t\t 0.000065\n",
      "133 \t\t 0.000058\n",
      "134 \t\t 0.000052\n",
      "135 \t\t 0.000046\n",
      "136 \t\t 0.000041\n",
      "137 \t\t 0.000037\n",
      "138 \t\t 0.000033\n",
      "139 \t\t 0.000029\n",
      "140 \t\t 0.000026\n",
      "141 \t\t 0.000023\n",
      "142 \t\t 0.000021\n",
      "143 \t\t 0.000018\n",
      "144 \t\t 0.000016\n",
      "145 \t\t 0.000015\n",
      "146 \t\t 0.000013\n",
      "147 \t\t 0.000012\n",
      "148 \t\t 0.000010\n",
      "149 \t\t 0.000009\n",
      "150 \t\t 0.000008\n",
      "151 \t\t 0.000007\n",
      "152 \t\t 0.000006\n",
      "153 \t\t 0.000006\n",
      "154 \t\t 0.000005\n",
      "155 \t\t 0.000005\n",
      "156 \t\t 0.000004\n",
      "157 \t\t 0.000004\n",
      "158 \t\t 0.000003\n",
      "159 \t\t 0.000003\n",
      "160 \t\t 0.000003\n",
      "161 \t\t 0.000002\n",
      "162 \t\t 0.000002\n",
      "163 \t\t 0.000002\n",
      "164 \t\t 0.000002\n",
      "165 \t\t 0.000001\n",
      "166 \t\t 0.000001\n",
      "167 \t\t 0.000001\n",
      "168 \t\t 0.000001\n",
      "169 \t\t 0.000001\n",
      "170 \t\t 0.000001\n",
      "171 \t\t 0.000001\n",
      "172 \t\t 0.000001\n",
      "173 \t\t 0.000001\n",
      "174 \t\t 0.000001\n",
      "175 \t\t 0.000000\n",
      "176 \t\t 0.000000\n",
      "177 \t\t 0.000000\n",
      "178 \t\t 0.000000\n",
      "179 \t\t 0.000000\n",
      "180 \t\t 0.000000\n",
      "181 \t\t 0.000000\n",
      "182 \t\t 0.000000\n",
      "183 \t\t 0.000000\n",
      "184 \t\t 0.000000\n",
      "185 \t\t 0.000000\n",
      "186 \t\t 0.000000\n",
      "187 \t\t 0.000000\n",
      "188 \t\t 0.000000\n",
      "189 \t\t 0.000000\n",
      "190 \t\t 0.000000\n",
      "191 \t\t 0.000000\n",
      "192 \t\t 0.000000\n",
      "193 \t\t 0.000000\n",
      "194 \t\t 0.000000\n",
      "195 \t\t 0.000000\n",
      "196 \t\t 0.000000\n",
      "197 \t\t 0.000000\n",
      "198 \t\t 0.000000\n",
      "199 \t\t 0.000000\n",
      "200 \t\t 0.000000\n",
      "201 \t\t 0.000000\n",
      "202 \t\t 0.000000\n",
      "203 \t\t 0.000000\n",
      "204 \t\t 0.000000\n",
      "205 \t\t 0.000000\n",
      "206 \t\t 0.000000\n",
      "207 \t\t 0.000000\n",
      "208 \t\t 0.000000\n",
      "209 \t\t 0.000000\n",
      "210 \t\t 0.000000\n",
      "211 \t\t 0.000000\n",
      "212 \t\t 0.000000\n",
      "213 \t\t 0.000000\n",
      "214 \t\t 0.000000\n",
      "215 \t\t 0.000000\n",
      "216 \t\t 0.000000\n",
      "217 \t\t 0.000000\n",
      "218 \t\t 0.000000\n",
      "219 \t\t 0.000000\n",
      "220 \t\t 0.000000\n",
      "221 \t\t 0.000000\n",
      "222 \t\t 0.000000\n",
      "223 \t\t 0.000000\n",
      "224 \t\t 0.000000\n",
      "225 \t\t 0.000000\n",
      "226 \t\t 0.000000\n",
      "227 \t\t 0.000000\n",
      "228 \t\t 0.000000\n",
      "229 \t\t 0.000000\n",
      "230 \t\t 0.000000\n",
      "231 \t\t 0.000000\n",
      "232 \t\t 0.000000\n",
      "233 \t\t 0.000000\n",
      "234 \t\t 0.000000\n",
      "235 \t\t 0.000000\n",
      "236 \t\t 0.000000\n",
      "237 \t\t 0.000000\n",
      "238 \t\t 0.000000\n",
      "239 \t\t 0.000000\n",
      "240 \t\t 0.000000\n",
      "241 \t\t 0.000000\n",
      "242 \t\t 0.000000\n",
      "243 \t\t 0.000000\n",
      "244 \t\t 0.000000\n",
      "245 \t\t 0.000000\n",
      "246 \t\t 0.000000\n",
      "247 \t\t 0.000000\n",
      "248 \t\t 0.000000\n",
      "249 \t\t 0.000000\n",
      "250 \t\t 0.000000\n"
     ]
    }
   ],
   "source": [
    "user_mat, movie_mat = FunkSVD(ratings_mat, latent_features=4, learning_rate=0.005, iters=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.00001034  10.00000426  10.00000266   9.99999   ]\n",
      " [ 10.000003     4.00000123   9.00000072   9.99999696]\n",
      " [  7.9999992    8.99999966   9.99999972   5.00000059]\n",
      " [  9.00002554   8.0000101   10.00000621   9.99997548]\n",
      " [ 10.00000495   5.00000157   9.00000077   8.99999507]\n",
      " [  5.99998704   3.99999452   9.99999642   6.00001218]\n",
      " [  9.00000884   8.00000318  10.0000018    8.99999138]\n",
      " [  9.99999043   4.99999585   8.99999723   8.00000891]\n",
      " [  6.99999846   7.99999908   9.99999928   8.00000132]\n",
      " [  8.99999668   4.99999857   8.99999903   7.00000298]\n",
      " [  9.00001087   8.00000417  10.00000248   7.99998945]\n",
      " [  9.00000574  10.00000183  10.00000093   8.99999434]\n",
      " [  9.99998068   8.99999188   9.99999474   8.00001823]\n",
      " [  4.9999863    7.99999421   4.99999628   8.00001303]\n",
      " [  9.99999858   7.99999952   9.99999968  10.00000119]\n",
      " [  8.99999792   8.99999925   9.99999952  10.00000186]\n",
      " [  8.99999694   7.99999892   7.99999934   8.00000279]\n",
      " [  9.99999728   7.99999889   0.99999929  10.00000257]\n",
      " [  4.99999332   5.99999739   9.99999838  10.00000631]\n",
      " [  8.00001179   7.00000504  10.0000032    6.9999886 ]]\n",
      "[[ 10.  10.  10.  10.]\n",
      " [ 10.   4.   9.  10.]\n",
      " [  8.   9.  10.   5.]\n",
      " [  9.   8.  10.  10.]\n",
      " [ 10.   5.   9.   9.]\n",
      " [  6.   4.  10.   6.]\n",
      " [  9.   8.  10.   9.]\n",
      " [ 10.   5.   9.   8.]\n",
      " [  7.   8.  10.   8.]\n",
      " [  9.   5.   9.   7.]\n",
      " [  9.   8.  10.   8.]\n",
      " [  9.  10.  10.   9.]\n",
      " [ 10.   9.  10.   8.]\n",
      " [  5.   8.   5.   8.]\n",
      " [ 10.   8.  10.  10.]\n",
      " [  9.   9.  10.  10.]\n",
      " [  9.   8.   8.   8.]\n",
      " [ 10.   8.   1.  10.]\n",
      " [  5.   6.  10.  10.]\n",
      " [  8.   7.  10.   7.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this case, we were able to completely reconstruct the item-movie matrix to obtain an essentially 0 mean squared error. I obtained 0 MSE on iteration 165.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last time we placed an **np.nan** value into this matrix the entire svd algorithm in python broke.  Let's see if that is still the case using your FunkSVD function.  In the below cell, I have placed a nan into the first cell of your numpy array.  \n",
    "\n",
    "`4.` Use 4 latent features, a learning rate of 0.005, and 250 iterations.  Are you able to run your SVD without it breaking (something that was not true about the python built in)?  Do you get a prediction for the nan value?  What is your prediction for the missing value? Use the cells below to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ nan,  10.,  10.,  10.],\n",
       "        [ 10.,   4.,   9.,  10.],\n",
       "        [  8.,   9.,  10.,   5.],\n",
       "        [  9.,   8.,  10.,  10.],\n",
       "        [ 10.,   5.,   9.,   9.],\n",
       "        [  6.,   4.,  10.,   6.],\n",
       "        [  9.,   8.,  10.,   9.],\n",
       "        [ 10.,   5.,   9.,   8.],\n",
       "        [  7.,   8.,  10.,   8.],\n",
       "        [  9.,   5.,   9.,   7.],\n",
       "        [  9.,   8.,  10.,   8.],\n",
       "        [  9.,  10.,  10.,   9.],\n",
       "        [ 10.,   9.,  10.,   8.],\n",
       "        [  5.,   8.,   5.,   8.],\n",
       "        [ 10.,   8.,  10.,  10.],\n",
       "        [  9.,   9.,  10.,  10.],\n",
       "        [  9.,   8.,   8.,   8.],\n",
       "        [ 10.,   8.,   1.,  10.],\n",
       "        [  5.,   6.,  10.,  10.],\n",
       "        [  8.,   7.,  10.,   7.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_mat[0, 0] = np.nan\n",
    "ratings_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "1 \t\t 44.864034\n",
      "2 \t\t 17.146488\n",
      "3 \t\t 4.949918\n",
      "4 \t\t 3.077611\n",
      "5 \t\t 2.845885\n",
      "6 \t\t 2.777227\n",
      "7 \t\t 2.734186\n",
      "8 \t\t 2.696149\n",
      "9 \t\t 2.657806\n",
      "10 \t\t 2.616997\n",
      "11 \t\t 2.572363\n",
      "12 \t\t 2.522794\n",
      "13 \t\t 2.467306\n",
      "14 \t\t 2.405035\n",
      "15 \t\t 2.335286\n",
      "16 \t\t 2.257608\n",
      "17 \t\t 2.171887\n",
      "18 \t\t 2.078450\n",
      "19 \t\t 1.978149\n",
      "20 \t\t 1.872406\n",
      "21 \t\t 1.763188\n",
      "22 \t\t 1.652899\n",
      "23 \t\t 1.544178\n",
      "24 \t\t 1.439634\n",
      "25 \t\t 1.341556\n",
      "26 \t\t 1.251678\n",
      "27 \t\t 1.171029\n",
      "28 \t\t 1.099913\n",
      "29 \t\t 1.038000\n",
      "30 \t\t 0.984485\n",
      "31 \t\t 0.938283\n",
      "32 \t\t 0.898195\n",
      "33 \t\t 0.863038\n",
      "34 \t\t 0.831725\n",
      "35 \t\t 0.803300\n",
      "36 \t\t 0.776947\n",
      "37 \t\t 0.751982\n",
      "38 \t\t 0.727829\n",
      "39 \t\t 0.704012\n",
      "40 \t\t 0.680129\n",
      "41 \t\t 0.655852\n",
      "42 \t\t 0.630912\n",
      "43 \t\t 0.605102\n",
      "44 \t\t 0.578273\n",
      "45 \t\t 0.550343\n",
      "46 \t\t 0.521291\n",
      "47 \t\t 0.491170\n",
      "48 \t\t 0.460100\n",
      "49 \t\t 0.428273\n",
      "50 \t\t 0.395944\n",
      "51 \t\t 0.363423\n",
      "52 \t\t 0.331062\n",
      "53 \t\t 0.299238\n",
      "54 \t\t 0.268327\n",
      "55 \t\t 0.238693\n",
      "56 \t\t 0.210656\n",
      "57 \t\t 0.184484\n",
      "58 \t\t 0.160375\n",
      "59 \t\t 0.138452\n",
      "60 \t\t 0.118764\n",
      "61 \t\t 0.101292\n",
      "62 \t\t 0.085957\n",
      "63 \t\t 0.072632\n",
      "64 \t\t 0.061159\n",
      "65 \t\t 0.051360\n",
      "66 \t\t 0.043047\n",
      "67 \t\t 0.036036\n",
      "68 \t\t 0.030151\n",
      "69 \t\t 0.025229\n",
      "70 \t\t 0.021122\n",
      "71 \t\t 0.017701\n",
      "72 \t\t 0.014855\n",
      "73 \t\t 0.012486\n",
      "74 \t\t 0.010514\n",
      "75 \t\t 0.008870\n",
      "76 \t\t 0.007498\n",
      "77 \t\t 0.006351\n",
      "78 \t\t 0.005389\n",
      "79 \t\t 0.004582\n",
      "80 \t\t 0.003902\n",
      "81 \t\t 0.003329\n",
      "82 \t\t 0.002844\n",
      "83 \t\t 0.002434\n",
      "84 \t\t 0.002085\n",
      "85 \t\t 0.001788\n",
      "86 \t\t 0.001535\n",
      "87 \t\t 0.001319\n",
      "88 \t\t 0.001135\n",
      "89 \t\t 0.000977\n",
      "90 \t\t 0.000841\n",
      "91 \t\t 0.000725\n",
      "92 \t\t 0.000625\n",
      "93 \t\t 0.000539\n",
      "94 \t\t 0.000465\n",
      "95 \t\t 0.000402\n",
      "96 \t\t 0.000347\n",
      "97 \t\t 0.000300\n",
      "98 \t\t 0.000259\n",
      "99 \t\t 0.000224\n",
      "100 \t\t 0.000193\n",
      "101 \t\t 0.000167\n",
      "102 \t\t 0.000144\n",
      "103 \t\t 0.000125\n",
      "104 \t\t 0.000108\n",
      "105 \t\t 0.000093\n",
      "106 \t\t 0.000081\n",
      "107 \t\t 0.000070\n",
      "108 \t\t 0.000060\n",
      "109 \t\t 0.000052\n",
      "110 \t\t 0.000045\n",
      "111 \t\t 0.000039\n",
      "112 \t\t 0.000034\n",
      "113 \t\t 0.000029\n",
      "114 \t\t 0.000025\n",
      "115 \t\t 0.000022\n",
      "116 \t\t 0.000019\n",
      "117 \t\t 0.000016\n",
      "118 \t\t 0.000014\n",
      "119 \t\t 0.000012\n",
      "120 \t\t 0.000011\n",
      "121 \t\t 0.000009\n",
      "122 \t\t 0.000008\n",
      "123 \t\t 0.000007\n",
      "124 \t\t 0.000006\n",
      "125 \t\t 0.000005\n",
      "126 \t\t 0.000005\n",
      "127 \t\t 0.000004\n",
      "128 \t\t 0.000003\n",
      "129 \t\t 0.000003\n",
      "130 \t\t 0.000003\n",
      "131 \t\t 0.000002\n",
      "132 \t\t 0.000002\n",
      "133 \t\t 0.000002\n",
      "134 \t\t 0.000001\n",
      "135 \t\t 0.000001\n",
      "136 \t\t 0.000001\n",
      "137 \t\t 0.000001\n",
      "138 \t\t 0.000001\n",
      "139 \t\t 0.000001\n",
      "140 \t\t 0.000001\n",
      "141 \t\t 0.000001\n",
      "142 \t\t 0.000000\n",
      "143 \t\t 0.000000\n",
      "144 \t\t 0.000000\n",
      "145 \t\t 0.000000\n",
      "146 \t\t 0.000000\n",
      "147 \t\t 0.000000\n",
      "148 \t\t 0.000000\n",
      "149 \t\t 0.000000\n",
      "150 \t\t 0.000000\n",
      "151 \t\t 0.000000\n",
      "152 \t\t 0.000000\n",
      "153 \t\t 0.000000\n",
      "154 \t\t 0.000000\n",
      "155 \t\t 0.000000\n",
      "156 \t\t 0.000000\n",
      "157 \t\t 0.000000\n",
      "158 \t\t 0.000000\n",
      "159 \t\t 0.000000\n",
      "160 \t\t 0.000000\n",
      "161 \t\t 0.000000\n",
      "162 \t\t 0.000000\n",
      "163 \t\t 0.000000\n",
      "164 \t\t 0.000000\n",
      "165 \t\t 0.000000\n",
      "166 \t\t 0.000000\n",
      "167 \t\t 0.000000\n",
      "168 \t\t 0.000000\n",
      "169 \t\t 0.000000\n",
      "170 \t\t 0.000000\n",
      "171 \t\t 0.000000\n",
      "172 \t\t 0.000000\n",
      "173 \t\t 0.000000\n",
      "174 \t\t 0.000000\n",
      "175 \t\t 0.000000\n",
      "176 \t\t 0.000000\n",
      "177 \t\t 0.000000\n",
      "178 \t\t 0.000000\n",
      "179 \t\t 0.000000\n",
      "180 \t\t 0.000000\n",
      "181 \t\t 0.000000\n",
      "182 \t\t 0.000000\n",
      "183 \t\t 0.000000\n",
      "184 \t\t 0.000000\n",
      "185 \t\t 0.000000\n",
      "186 \t\t 0.000000\n",
      "187 \t\t 0.000000\n",
      "188 \t\t 0.000000\n",
      "189 \t\t 0.000000\n",
      "190 \t\t 0.000000\n",
      "191 \t\t 0.000000\n",
      "192 \t\t 0.000000\n",
      "193 \t\t 0.000000\n",
      "194 \t\t 0.000000\n",
      "195 \t\t 0.000000\n",
      "196 \t\t 0.000000\n",
      "197 \t\t 0.000000\n",
      "198 \t\t 0.000000\n",
      "199 \t\t 0.000000\n",
      "200 \t\t 0.000000\n",
      "201 \t\t 0.000000\n",
      "202 \t\t 0.000000\n",
      "203 \t\t 0.000000\n",
      "204 \t\t 0.000000\n",
      "205 \t\t 0.000000\n",
      "206 \t\t 0.000000\n",
      "207 \t\t 0.000000\n",
      "208 \t\t 0.000000\n",
      "209 \t\t 0.000000\n",
      "210 \t\t 0.000000\n",
      "211 \t\t 0.000000\n",
      "212 \t\t 0.000000\n",
      "213 \t\t 0.000000\n",
      "214 \t\t 0.000000\n",
      "215 \t\t 0.000000\n",
      "216 \t\t 0.000000\n",
      "217 \t\t 0.000000\n",
      "218 \t\t 0.000000\n",
      "219 \t\t 0.000000\n",
      "220 \t\t 0.000000\n",
      "221 \t\t 0.000000\n",
      "222 \t\t 0.000000\n",
      "223 \t\t 0.000000\n",
      "224 \t\t 0.000000\n",
      "225 \t\t 0.000000\n",
      "226 \t\t 0.000000\n",
      "227 \t\t 0.000000\n",
      "228 \t\t 0.000000\n",
      "229 \t\t 0.000000\n",
      "230 \t\t 0.000000\n",
      "231 \t\t 0.000000\n",
      "232 \t\t 0.000000\n",
      "233 \t\t 0.000000\n",
      "234 \t\t 0.000000\n",
      "235 \t\t 0.000000\n",
      "236 \t\t 0.000000\n",
      "237 \t\t 0.000000\n",
      "238 \t\t 0.000000\n",
      "239 \t\t 0.000000\n",
      "240 \t\t 0.000000\n",
      "241 \t\t 0.000000\n",
      "242 \t\t 0.000000\n",
      "243 \t\t 0.000000\n",
      "244 \t\t 0.000000\n",
      "245 \t\t 0.000000\n",
      "246 \t\t 0.000000\n",
      "247 \t\t 0.000000\n",
      "248 \t\t 0.000000\n",
      "249 \t\t 0.000000\n",
      "250 \t\t 0.000000\n"
     ]
    }
   ],
   "source": [
    "# run SVD on the matrix with the missing value\n",
    "user_mat, movie_mat = FunkSVD(ratings_mat, latent_features=4, learning_rate=0.005, iters=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value for the missing rating is 11.252904946215294:\n",
      "\n",
      "The actual value for the missing rating is nan:\n",
      "\n",
      "That's right! You just predicted a rating for a user-movie pair that was never rated!\n",
      "But if you look in the original matrix, this was actually a value of 10. Not bad!\n"
     ]
    }
   ],
   "source": [
    "preds = np.dot(user_mat, movie_mat)\n",
    "print(\"The predicted value for the missing rating is {}:\".format(preds[0,0]))\n",
    "print()\n",
    "print(\"The actual value for the missing rating is {}:\".format(ratings_mat[0,0]))\n",
    "print()\n",
    "assert np.isnan(preds[0,0]) == False\n",
    "print(\"That's right! You just predicted a rating for a user-movie pair that was never rated!\")\n",
    "print(\"But if you look in the original matrix, this was actually a value of 10. Not bad!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extend this to a more realistic example. Unfortunately, running this function on your entire user-movie matrix is still not something you likely want to do on your local machine.  However, we can see how well this example extends to 1000 users.  In the above portion, you were using a very small subset of data with no missing values.\n",
    "\n",
    "`5.` Given the size of this matrix, this will take quite a bit of time.  Consider the following hyperparameters: 4 latent features, 0.005 learning rate, and 20 iterations.  Grab a snack, take a walk, and this should be done running in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "1 \t\t 23.043054\n",
      "2 \t\t 10.624768\n",
      "3 \t\t 7.314082\n",
      "4 \t\t 5.657515\n",
      "5 \t\t 4.612804\n",
      "6 \t\t 3.880947\n",
      "7 \t\t 3.335650\n",
      "8 \t\t 2.912864\n",
      "9 \t\t 2.575807\n",
      "10 \t\t 2.301409\n",
      "11 \t\t 2.074291\n",
      "12 \t\t 1.883753\n",
      "13 \t\t 1.722120\n",
      "14 \t\t 1.583750\n",
      "15 \t\t 1.464408\n",
      "16 \t\t 1.360851\n",
      "17 \t\t 1.270545\n",
      "18 \t\t 1.191467\n",
      "19 \t\t 1.121970\n",
      "20 \t\t 1.060690\n"
     ]
    }
   ],
   "source": [
    "# Setting up a matrix of the first 1000 users with movie ratings\n",
    "first_1000_users = np.matrix(user_by_movie.head(1000))\n",
    "\n",
    "# perform funkSVD on the matrix of the top 1000 users\n",
    "user_mat, movie_mat = FunkSVD(first_1000_users, latent_features=4, learning_rate=0.005, iters=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` Now that you have a set of predictions for each user-movie pair.  Let's answer a few questions about your results. Provide the correct values to each of the variables below, and check your solutions using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of actual ratings in the first_1000_users is 10852.\n",
      "\n",
      "The number of ratings made for user-movie pairs that didn't have ratings is 31234148\n"
     ]
    }
   ],
   "source": [
    "# How many actual ratings exist in first_1000_users\n",
    "num_ratings = np.count_nonzero(~np.isnan(first_1000_users))\n",
    "print(\"The number of actual ratings in the first_1000_users is {}.\".format(num_ratings))\n",
    "print()\n",
    "\n",
    "# How many ratings did we make for user-movie pairs that didn't have ratings\n",
    "ratings_for_missing = first_1000_users.shape[0]*first_1000_users.shape[1] - num_ratings\n",
    "print(\"The number of ratings made for user-movie pairs that didn't have ratings is {}\".format(ratings_for_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice job!  Looks like you have predictions made for all the missing user-movie pairs! But I still have one question... How good are they?\n"
     ]
    }
   ],
   "source": [
    "# Test your results against the solution\n",
    "assert num_ratings == 10852, \"Oops!  The number of actual ratings doesn't quite look right.\"\n",
    "assert ratings_for_missing == 31234148, \"Oops!  The number of movie-user pairs that you made ratings for that didn't actually have ratings doesn't look right.\"\n",
    "\n",
    "# Make sure you made predictions on all the missing user-movie pairs\n",
    "preds = np.dot(user_mat, movie_mat)\n",
    "assert np.isnan(preds).sum() == 0\n",
    "print(\"Nice job!  Looks like you have predictions made for all the missing user-movie pairs! But I still have one question... How good are they?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 Implementing FunkSVD Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17 FunkSVD Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18 How Are We Doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Are We Doing?\n",
    "\n",
    "In the last notebook, you created a working version of SVD for situations even when there are tons of missing values.  This is awesome!  The question now is how well does this solution work?\n",
    "\n",
    "In this notebook, we are going to simulate exactly what we would do in the real world to tune our recommender.  \n",
    "\n",
    "Run the cell below to read in the data and get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('data/movies_clean.csv')\n",
    "reviews = pd.read_csv('data/reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the **reviews** dataframe, perform the following tasks to create a training and validation set of data we can use to test the performance of your SVD algorithm using **off-line** validation techniques.\n",
    "\n",
    " * Order the reviews dataframe from earliest to most recent \n",
    " * Pull the first 10000 reviews from  the dataset\n",
    " * Make the first 8000/10000 reviews the training data \n",
    " * Make the last 2000/10000 the test data\n",
    " * Return the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_test(reviews, order_by, training_size, testing_size):\n",
    "    '''    \n",
    "    INPUT:\n",
    "    reviews - (pandas df) dataframe to split into train and test\n",
    "    order_by - (string) column name to sort by\n",
    "    training_size - (int) number of rows in training set\n",
    "    testing_size - (int) number of columns in the test set\n",
    "    \n",
    "    OUTPUT:\n",
    "    training_df -  (pandas df) dataframe of the training set\n",
    "    validation_df - (pandas df) dataframe of the test set\n",
    "    '''\n",
    "    reviews_new = reviews.sort_values(order_by)\n",
    "    training_df = reviews_new.head(training_size)\n",
    "    validation_df = reviews_new.iloc[training_size:training_size+testing_size]\n",
    "    \n",
    "    return training_df, validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use our function to create training and test datasets\n",
    "train_df, val_df = create_train_test(reviews, 'date', 8000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice job!  Looks like you have written a function that provides training and validation dataframes for you to use in the next steps.\n"
     ]
    }
   ],
   "source": [
    "# Make sure the dataframes we are using are the right shape\n",
    "assert train_df.shape[0] == 8000, \"The number of rows doesn't look right in the training dataset.\"\n",
    "assert val_df.shape[0] == 2000, \"The number of rows doesn't look right in the validation dataset\"\n",
    "assert str(train_df.tail(1)['date']).split()[1] == '2013-03-15', \"The last date in the training dataset doesn't look like what we expected.\"\n",
    "assert str(val_df.tail(1)['date']).split()[1] == '2013-03-18', \"The last date in the validation dataset doesn't look like what we expected.\"\n",
    "print(\"Nice job!  Looks like you have written a function that provides training and validation dataframes for you to use in the next steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, we might have all of the data up to this final date in the training data.  Then we want to see how well we are doing for each of the new ratings, which show up in the test data.\n",
    "\n",
    "Below is a working example of the function created in the previous example you can use (or you can replace with your own).\n",
    "\n",
    "`2.`  Fit the function to the training data with the following hyperparameters: 15 latent features, a learning rate of 0.005, and 250 iterations. This will take some time to run, so you may choose fewer latent features, a higher learning rate, or fewer iteratios if you want to speed up the process.  \n",
    "\n",
    "**Note:** Again, this might be a good time to take a phone call, go for a walk, or just take a little break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FunkSVD(ratings_mat, latent_features=12, learning_rate=0.0001, iters=100):\n",
    "    '''\n",
    "    This function performs matrix factorization using a basic form of FunkSVD with no regularization\n",
    "    \n",
    "    INPUT:\n",
    "    ratings_mat - (numpy array) a matrix with users as rows, movies as columns, and ratings as values\n",
    "    latent_features - (int) the number of latent features used\n",
    "    learning_rate - (float) the learning rate \n",
    "    iters - (int) the number of iterations\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_mat - (numpy array) a user by latent feature matrix\n",
    "    movie_mat - (numpy array) a latent feature by movie matrix\n",
    "    '''\n",
    "    \n",
    "    # Set up useful values to be used through the rest of the function\n",
    "    n_users = ratings_mat.shape[0]\n",
    "    n_movies = ratings_mat.shape[1]\n",
    "    num_ratings = np.count_nonzero(~np.isnan(ratings_mat))\n",
    "    \n",
    "    # initialize the user and movie matrices with random values\n",
    "    user_mat = np.random.rand(n_users, latent_features)\n",
    "    movie_mat = np.random.rand(latent_features, n_movies)\n",
    "    \n",
    "    # initialize sse at 0 for first iteration\n",
    "    sse_accum = 0\n",
    "    \n",
    "    # keep track of iteration and MSE\n",
    "    print(\"Optimizaiton Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "    \n",
    "    # for each iteration\n",
    "    for iteration in range(iters):\n",
    "\n",
    "        # update our sse\n",
    "        old_sse = sse_accum\n",
    "        sse_accum = 0\n",
    "        \n",
    "        # For each user-movie pair\n",
    "        for i in range(n_users):\n",
    "            for j in range(n_movies):\n",
    "                \n",
    "                # if the rating exists\n",
    "                if ratings_mat[i, j] > 0:\n",
    "                    \n",
    "                    # compute the error as the actual minus the dot product of the user and movie latent features\n",
    "                    diff = ratings_mat[i, j] - np.dot(user_mat[i, :], movie_mat[:, j])\n",
    "                    \n",
    "                    # Keep track of the sum of squared errors for the matrix\n",
    "                    sse_accum += diff**2\n",
    "                    \n",
    "                    # update the values in each matrix in the direction of the gradient\n",
    "                    for k in range(latent_features):\n",
    "                        user_mat[i, k] += learning_rate * (2*diff*movie_mat[k, j])\n",
    "                        movie_mat[k, j] += learning_rate * (2*diff*user_mat[i, k])\n",
    "\n",
    "        # print results\n",
    "        print(\"%d \\t\\t %f\" % (iteration+1, sse_accum / num_ratings))\n",
    "        \n",
    "    return user_mat, movie_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "1 \t\t 10.813772\n",
      "2 \t\t 5.989314\n",
      "3 \t\t 4.178734\n",
      "4 \t\t 3.127750\n",
      "5 \t\t 2.437289\n",
      "6 \t\t 1.948673\n",
      "7 \t\t 1.585911\n",
      "8 \t\t 1.307792\n",
      "9 \t\t 1.089685\n",
      "10 \t\t 0.915758\n",
      "11 \t\t 0.775258\n",
      "12 \t\t 0.660576\n",
      "13 \t\t 0.566146\n",
      "14 \t\t 0.487793\n",
      "15 \t\t 0.422326\n",
      "16 \t\t 0.367274\n",
      "17 \t\t 0.320705\n",
      "18 \t\t 0.281093\n",
      "19 \t\t 0.247230\n",
      "20 \t\t 0.218145\n",
      "21 \t\t 0.193058\n",
      "22 \t\t 0.171336\n",
      "23 \t\t 0.152458\n",
      "24 \t\t 0.135997\n",
      "25 \t\t 0.121599\n",
      "26 \t\t 0.108968\n",
      "27 \t\t 0.097856\n",
      "28 \t\t 0.088053\n",
      "29 \t\t 0.079384\n",
      "30 \t\t 0.071698\n",
      "31 \t\t 0.064869\n",
      "32 \t\t 0.058786\n",
      "33 \t\t 0.053356\n",
      "34 \t\t 0.048500\n",
      "35 \t\t 0.044149\n",
      "36 \t\t 0.040241\n",
      "37 \t\t 0.036727\n",
      "38 \t\t 0.033561\n",
      "39 \t\t 0.030703\n",
      "40 \t\t 0.028121\n",
      "41 \t\t 0.025783\n",
      "42 \t\t 0.023664\n",
      "43 \t\t 0.021741\n",
      "44 \t\t 0.019993\n",
      "45 \t\t 0.018403\n",
      "46 \t\t 0.016954\n",
      "47 \t\t 0.015633\n",
      "48 \t\t 0.014427\n",
      "49 \t\t 0.013324\n",
      "50 \t\t 0.012316\n",
      "51 \t\t 0.011393\n",
      "52 \t\t 0.010547\n",
      "53 \t\t 0.009770\n",
      "54 \t\t 0.009058\n",
      "55 \t\t 0.008403\n",
      "56 \t\t 0.007800\n",
      "57 \t\t 0.007246\n",
      "58 \t\t 0.006735\n",
      "59 \t\t 0.006264\n",
      "60 \t\t 0.005830\n",
      "61 \t\t 0.005429\n",
      "62 \t\t 0.005059\n",
      "63 \t\t 0.004716\n",
      "64 \t\t 0.004399\n",
      "65 \t\t 0.004106\n",
      "66 \t\t 0.003834\n",
      "67 \t\t 0.003582\n",
      "68 \t\t 0.003349\n",
      "69 \t\t 0.003132\n",
      "70 \t\t 0.002930\n",
      "71 \t\t 0.002743\n",
      "72 \t\t 0.002569\n",
      "73 \t\t 0.002407\n",
      "74 \t\t 0.002257\n",
      "75 \t\t 0.002116\n",
      "76 \t\t 0.001986\n",
      "77 \t\t 0.001864\n",
      "78 \t\t 0.001750\n",
      "79 \t\t 0.001644\n",
      "80 \t\t 0.001545\n",
      "81 \t\t 0.001452\n",
      "82 \t\t 0.001366\n",
      "83 \t\t 0.001285\n",
      "84 \t\t 0.001209\n",
      "85 \t\t 0.001139\n",
      "86 \t\t 0.001072\n",
      "87 \t\t 0.001010\n",
      "88 \t\t 0.000952\n",
      "89 \t\t 0.000898\n",
      "90 \t\t 0.000847\n",
      "91 \t\t 0.000799\n",
      "92 \t\t 0.000754\n",
      "93 \t\t 0.000712\n",
      "94 \t\t 0.000672\n",
      "95 \t\t 0.000635\n",
      "96 \t\t 0.000600\n",
      "97 \t\t 0.000567\n",
      "98 \t\t 0.000536\n",
      "99 \t\t 0.000507\n",
      "100 \t\t 0.000479\n",
      "101 \t\t 0.000454\n",
      "102 \t\t 0.000429\n",
      "103 \t\t 0.000406\n",
      "104 \t\t 0.000385\n",
      "105 \t\t 0.000364\n",
      "106 \t\t 0.000345\n",
      "107 \t\t 0.000327\n",
      "108 \t\t 0.000310\n",
      "109 \t\t 0.000294\n",
      "110 \t\t 0.000279\n",
      "111 \t\t 0.000265\n",
      "112 \t\t 0.000251\n",
      "113 \t\t 0.000238\n",
      "114 \t\t 0.000226\n",
      "115 \t\t 0.000215\n",
      "116 \t\t 0.000204\n",
      "117 \t\t 0.000194\n",
      "118 \t\t 0.000184\n",
      "119 \t\t 0.000175\n",
      "120 \t\t 0.000166\n",
      "121 \t\t 0.000158\n",
      "122 \t\t 0.000150\n",
      "123 \t\t 0.000143\n",
      "124 \t\t 0.000136\n",
      "125 \t\t 0.000129\n",
      "126 \t\t 0.000123\n",
      "127 \t\t 0.000117\n",
      "128 \t\t 0.000112\n",
      "129 \t\t 0.000106\n",
      "130 \t\t 0.000101\n",
      "131 \t\t 0.000096\n",
      "132 \t\t 0.000092\n",
      "133 \t\t 0.000088\n",
      "134 \t\t 0.000084\n",
      "135 \t\t 0.000080\n",
      "136 \t\t 0.000076\n",
      "137 \t\t 0.000072\n",
      "138 \t\t 0.000069\n",
      "139 \t\t 0.000066\n",
      "140 \t\t 0.000063\n",
      "141 \t\t 0.000060\n",
      "142 \t\t 0.000057\n",
      "143 \t\t 0.000055\n",
      "144 \t\t 0.000052\n",
      "145 \t\t 0.000050\n",
      "146 \t\t 0.000048\n",
      "147 \t\t 0.000046\n",
      "148 \t\t 0.000043\n",
      "149 \t\t 0.000042\n",
      "150 \t\t 0.000040\n",
      "151 \t\t 0.000038\n",
      "152 \t\t 0.000036\n",
      "153 \t\t 0.000035\n",
      "154 \t\t 0.000033\n",
      "155 \t\t 0.000032\n",
      "156 \t\t 0.000030\n",
      "157 \t\t 0.000029\n",
      "158 \t\t 0.000028\n",
      "159 \t\t 0.000027\n",
      "160 \t\t 0.000025\n",
      "161 \t\t 0.000024\n",
      "162 \t\t 0.000023\n",
      "163 \t\t 0.000022\n",
      "164 \t\t 0.000021\n",
      "165 \t\t 0.000020\n",
      "166 \t\t 0.000020\n",
      "167 \t\t 0.000019\n",
      "168 \t\t 0.000018\n",
      "169 \t\t 0.000017\n",
      "170 \t\t 0.000017\n",
      "171 \t\t 0.000016\n",
      "172 \t\t 0.000015\n",
      "173 \t\t 0.000015\n",
      "174 \t\t 0.000014\n",
      "175 \t\t 0.000013\n",
      "176 \t\t 0.000013\n",
      "177 \t\t 0.000012\n",
      "178 \t\t 0.000012\n",
      "179 \t\t 0.000011\n",
      "180 \t\t 0.000011\n",
      "181 \t\t 0.000010\n",
      "182 \t\t 0.000010\n",
      "183 \t\t 0.000010\n",
      "184 \t\t 0.000009\n",
      "185 \t\t 0.000009\n",
      "186 \t\t 0.000009\n",
      "187 \t\t 0.000008\n",
      "188 \t\t 0.000008\n",
      "189 \t\t 0.000008\n",
      "190 \t\t 0.000007\n",
      "191 \t\t 0.000007\n",
      "192 \t\t 0.000007\n",
      "193 \t\t 0.000006\n",
      "194 \t\t 0.000006\n",
      "195 \t\t 0.000006\n",
      "196 \t\t 0.000006\n",
      "197 \t\t 0.000005\n",
      "198 \t\t 0.000005\n",
      "199 \t\t 0.000005\n",
      "200 \t\t 0.000005\n",
      "201 \t\t 0.000005\n",
      "202 \t\t 0.000004\n",
      "203 \t\t 0.000004\n",
      "204 \t\t 0.000004\n",
      "205 \t\t 0.000004\n",
      "206 \t\t 0.000004\n",
      "207 \t\t 0.000004\n",
      "208 \t\t 0.000004\n",
      "209 \t\t 0.000003\n",
      "210 \t\t 0.000003\n",
      "211 \t\t 0.000003\n",
      "212 \t\t 0.000003\n",
      "213 \t\t 0.000003\n",
      "214 \t\t 0.000003\n",
      "215 \t\t 0.000003\n",
      "216 \t\t 0.000003\n",
      "217 \t\t 0.000003\n",
      "218 \t\t 0.000002\n",
      "219 \t\t 0.000002\n",
      "220 \t\t 0.000002\n",
      "221 \t\t 0.000002\n",
      "222 \t\t 0.000002\n",
      "223 \t\t 0.000002\n",
      "224 \t\t 0.000002\n",
      "225 \t\t 0.000002\n",
      "226 \t\t 0.000002\n",
      "227 \t\t 0.000002\n",
      "228 \t\t 0.000002\n",
      "229 \t\t 0.000002\n",
      "230 \t\t 0.000002\n",
      "231 \t\t 0.000001\n",
      "232 \t\t 0.000001\n",
      "233 \t\t 0.000001\n",
      "234 \t\t 0.000001\n",
      "235 \t\t 0.000001\n",
      "236 \t\t 0.000001\n",
      "237 \t\t 0.000001\n",
      "238 \t\t 0.000001\n",
      "239 \t\t 0.000001\n",
      "240 \t\t 0.000001\n",
      "241 \t\t 0.000001\n",
      "242 \t\t 0.000001\n",
      "243 \t\t 0.000001\n",
      "244 \t\t 0.000001\n",
      "245 \t\t 0.000001\n",
      "246 \t\t 0.000001\n",
      "247 \t\t 0.000001\n",
      "248 \t\t 0.000001\n",
      "249 \t\t 0.000001\n",
      "250 \t\t 0.000001\n"
     ]
    }
   ],
   "source": [
    "# Create user-by-item matrix - nothing to do here\n",
    "train_user_item = train_df[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "train_data_df = train_user_item.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "train_data_np = np.array(train_data_df)\n",
    "\n",
    "# Fit FunkSVD with the specified hyper parameters to the training data\n",
    "user_mat, movie_mat = FunkSVD(train_data_np, latent_features=15, learning_rate=0.005, iters=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created the **user_mat** and **movie_mat**, we can use this to make predictions for how users would rate movies, by just computing the dot product of the row associated with a user and the column associated with the movie.\n",
    "\n",
    "`3.` Use the comments in the function below to complete the **predict_rating** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_rating(user_matrix, movie_matrix, user_id, movie_id):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_matrix - user by latent factor matrix\n",
    "    movie_matrix - latent factor by movie matrix\n",
    "    user_id - the user_id from the reviews df\n",
    "    movie_id - the movie_id according the movies df\n",
    "    \n",
    "    OUTPUT:\n",
    "    pred - the predicted rating for user_id-movie_id according to FunkSVD\n",
    "    '''\n",
    "    # Use the training data to create a series of users and movies that matches the ordering in training data\n",
    "    user_ids_series = np.array(train_data_df.index)\n",
    "    movie_ids_series = np.array(train_data_df.columns)\n",
    "    \n",
    "    # User row and Movie Column\n",
    "    user_row = np.where(user_ids_series == user_id)[0][0]\n",
    "    movie_col = np.where(movie_ids_series == movie_id)[0][0]\n",
    "    \n",
    "    # Take dot product of that row and column in U and V to make prediction\n",
    "    pred = np.dot(user_matrix[user_row, :], movie_matrix[:, movie_col])\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.1856887918502661"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function with the first user-movie in the user-movie matrix (notice this is a nan)\n",
    "pred_val = predict_rating(user_mat, movie_mat, 8, 2844)\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is great that you now have a way to make predictions. However it might be nice to get a little phrase back about the user, movie, and rating.\n",
    "\n",
    "`4.` Use the comments in the function below to complete the **predict_rating** function.  \n",
    "\n",
    "**Note:** The movie name doesn't come back in a great format, so you can see in the solution I messed around with it a bit just to make it a little nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_prediction_summary(user_id, movie_id, prediction):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id from the reviews df\n",
    "    movie_id - the movie_id according the movies df\n",
    "    prediction - the predicted rating for user_id-movie_id\n",
    "    '''\n",
    "    movie_name = str(movies[movies['movie_id'] == movie_id]['movie']) [5:]\n",
    "    movie_name = movie_name.replace('\\nName: movie, dtype: object', '')\n",
    "    print(\"For user {} we predict a {} rating for the movie {}.\".format(user_id, round(prediction, 2), str(movie_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For user 8 we predict a 7.19 rating for the movie  Fantômas - À l'ombre de la guillotine (1913).\n"
     ]
    }
   ],
   "source": [
    "# Test your function the the results of the previous function\n",
    "print_prediction_summary(8, 2844, pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ability to make predictions, let's see how well our predictions do on the test ratings we already have.  This will give an indication of how well have captured the latent features, and our ability to use the latent features to make predictions in the future!\n",
    "\n",
    "`5.` For each of the user-movie rating in the **val_df** dataset, compare the actual rating given to the prediction you would make.  How do your predictions do?  Do you run into any problems?  If yes, what is the problem?  Use the document strings and comments below to assist as you work through these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual rating for user 49056 on movie 1598822 is 8.\n",
      " While the predicted rating is 6.0.\n",
      "The actual rating for user 49056 on movie 289879 is 9.\n",
      " While the predicted rating is 9.0.\n",
      "The actual rating for user 49056 on movie 1563738 is 9.\n",
      " While the predicted rating is 7.0.\n",
      "The actual rating for user 49056 on movie 1458175 is 4.\n",
      " While the predicted rating is 8.0.\n",
      "The actual rating for user 28599 on movie 103639 is 8.\n",
      " While the predicted rating is 8.0.\n",
      "The actual rating for user 50593 on movie 1560985 is 4.\n",
      " While the predicted rating is 3.0.\n"
     ]
    }
   ],
   "source": [
    "def validation_comparison(val_df, num_preds):\n",
    "    '''\n",
    "    INPUT:\n",
    "    val_df - the validation dataset created in the third cell above\n",
    "    num_preds - (int) the number of rows (going in order) you would like to make predictions for\n",
    "    \n",
    "    OUTPUT:\n",
    "    Nothing returned - print a statement about the prediciton made for each row of val_df from row 0 to num_preds\n",
    "    '''\n",
    "    val_users = np.array(val_df['user_id'])\n",
    "    val_movies = np.array(val_df['movie_id'])\n",
    "    val_ratings = np.array(val_df['rating'])\n",
    "    \n",
    "    \n",
    "    for idx in range(num_preds):\n",
    "        pred = predict_rating(user_mat, movie_mat, val_users[idx], val_movies[idx])\n",
    "        print(\"The actual rating for user {} on movie {} is {}.\\n While the predicted rating is {}.\".format(val_users[idx], val_movies[idx], val_ratings[idx], round(pred))) \n",
    "\n",
    "        \n",
    "# Perform the predicted vs. actual for the first 6 rows.  How does it look?\n",
    "validation_comparison(val_df, 6)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual rating for user 49056 on movie 1598822 is 8.\n",
      " While the predicted rating is 6.0.\n",
      "The actual rating for user 49056 on movie 289879 is 9.\n",
      " While the predicted rating is 9.0.\n",
      "The actual rating for user 49056 on movie 1563738 is 9.\n",
      " While the predicted rating is 7.0.\n",
      "The actual rating for user 49056 on movie 1458175 is 4.\n",
      " While the predicted rating is 8.0.\n",
      "The actual rating for user 28599 on movie 103639 is 8.\n",
      " While the predicted rating is 8.0.\n",
      "The actual rating for user 50593 on movie 1560985 is 4.\n",
      " While the predicted rating is 3.0.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-718a6e1cc824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Perform the predicted vs. actual for the first 7 rows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvalidation_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# generate a list of user-movies we cannot make predictions for - what do you notice?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-7d9210bd9add>\u001b[0m in \u001b[0;36mvalidation_comparison\u001b[0;34m(val_df, num_preds)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Perform the predicted vs. actual for the first 6 rows.  How does it look?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_movies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The actual rating for user {} on movie {} is {}.\\n While the predicted rating is {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_movies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-73511eac8db1>\u001b[0m in \u001b[0;36mpredict_rating\u001b[0;34m(user_matrix, movie_matrix, user_id, movie_id)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# User row and Movie Column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0muser_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_ids_series\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmovie_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_ids_series\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmovie_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Take dot product of that row and column in U and V to make prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# Perform the predicted vs. actual for the first 7 rows.  What happened?\n",
    "validation_comparison(val_df, 7)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The 7th movie is a movie that has no ratings.  Therefore, we are not able to make a prediction for this user-movie pair.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 How Are We Doing? Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 The Cold Start Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **cold start problem** is the problem that new users and new items to a platform don't have any ratings. Because these users and items don't have any ratings, it is impossible to use collaborative filtering methods to make recommendations.\n",
    "\n",
    "Therefore, methods you used in the previous lesson like (rank-based and content-based recommenders) are the only way to get started with making recommendations for these individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21 Notebook: The Cold Start Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cold Start Problem\n",
    "\n",
    "In the previous notebook, you learned about the **Cold Start Problem** first hand. In cases where you are introduced to a new user or new movie, collaborative flitering is not helpful as a technique to make predictions.\n",
    "\n",
    "Instead, you will need to use one of the techniques from the previous lesson like content based recommendations for new items or rank based recommendations for new users.  \n",
    "\n",
    "As a final step to completing out our recommendation system, we will build in these edge cases. Run the cell below to get started.\n",
    "\n",
    "### Matrix Factorization - Collaborative Filtering Where Possible\n",
    "\n",
    "Notice the following information is available by running the below cell:\n",
    "\n",
    "`1.` **reviews** - a dataframe of reviews\n",
    "\n",
    "`2.` **movies** - a dataframe of movies\n",
    "\n",
    "`3.` **create_train_test** - a function for creating the training and validation datasets\n",
    "\n",
    "`4.` **predict_rating** - a function that takes a user and movie and gives a prediction using FunkSVD\n",
    "\n",
    "`5.` **train_df** and **val_df** - the training and test datasets used in the previous notebook\n",
    "\n",
    "`6.` **user_mat** and **movie_mat** - the u and v matrices from FunkSVD\n",
    "\n",
    "`7.` **train_data_df** - a user-movie matrix with ratings where available.  FunkSVD was performed on this matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('data/movies_clean.csv')\n",
    "reviews = pd.read_csv('data/reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']\n",
    "\n",
    "def create_train_test(reviews, order_by, training_size, testing_size):\n",
    "    '''    \n",
    "    INPUT:\n",
    "    reviews - (pandas df) dataframe to split into train and test\n",
    "    order_by - (string) column name to sort by\n",
    "    training_size - (int) number of rows in training set\n",
    "    testing_size - (int) number of columns in the test set\n",
    "    \n",
    "    OUTPUT:\n",
    "    training_df -  (pandas df) dataframe of the training set\n",
    "    validation_df - (pandas df) dataframe of the test set\n",
    "    '''\n",
    "    reviews_new = reviews.sort_values(order_by)\n",
    "    training_df = reviews_new.head(training_size)\n",
    "    validation_df = reviews_new.iloc[training_size:training_size+testing_size]\n",
    "    \n",
    "    return training_df, validation_df\n",
    "\n",
    "def predict_rating(user_matrix, movie_matrix, user_id, movie_id):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_matrix - user by latent factor matrix\n",
    "    movie_matrix - latent factor by movie matrix\n",
    "    user_id - the user_id from the reviews df\n",
    "    movie_id - the movie_id according the movies df\n",
    "    \n",
    "    OUTPUT:\n",
    "    pred - the predicted rating for user_id-movie_id according to FunkSVD\n",
    "    '''\n",
    "    # Create series of users and movies in the right order\n",
    "    user_ids_series = np.array(train_data_df.index)\n",
    "    movie_ids_series = np.array(train_data_df.columns)\n",
    "    \n",
    "    # User row and Movie Column\n",
    "    user_row = np.where(user_ids_series == user_id)[0][0]\n",
    "    movie_col = np.where(movie_ids_series == movie_id)[0][0]\n",
    "    \n",
    "    # Take dot product of that row and column in U and V to make prediction\n",
    "    pred = np.dot(user_matrix[user_row, :], movie_matrix[:, movie_col])\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# Use our function to create training and test datasets\n",
    "train_df, val_df = create_train_test(reviews, 'date', 8000, 2000)\n",
    "\n",
    "# Create user-by-item matrix - this will keep track of order of users and movies in u and v\n",
    "train_user_item = train_df[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "train_data_df = train_user_item.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "train_data_np = np.array(train_data_df)\n",
    "\n",
    "# Read in user and movie matrices\n",
    "user_file = open(\"user_matrix\", 'rb')\n",
    "user_mat = pickle.load(user_file)\n",
    "user_file.close()\n",
    "\n",
    "movie_file = open(\"movie_matrix\", 'rb')\n",
    "movie_mat = pickle.load(movie_file)\n",
    "movie_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Predictions\n",
    "\n",
    "Unfortunately, you weren't able to make predictions on every user-movie combination in the test set, as some of these users or movies were new.  \n",
    "\n",
    "However, you can validate your predictions for the user-movie pairs that do exist in the user_mat and movie_mat matrices.  \n",
    "\n",
    "`1.` Complete the function below to see how far off we were on average across all of the predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_comparison(val_df, user_mat=user_mat, movie_mat=movie_mat):\n",
    "    '''\n",
    "    INPUT:\n",
    "    val_df - the validation dataset created in the third cell above\n",
    "    user_mat - U matrix in FunkSVD\n",
    "    movie_mat - V matrix in FunkSVD\n",
    "        \n",
    "    OUTPUT:\n",
    "    rmse - RMSE of how far off each value is from it's predicted value\n",
    "    perc_rated - percent of predictions out of all possible that could be rated\n",
    "    actual_v_pred - a 10 x 10 grid with counts for actual vs predicted values\n",
    "    '''\n",
    "        \n",
    "    val_users = np.array(val_df['user_id'])\n",
    "    val_movies = np.array(val_df['movie_id'])\n",
    "    val_ratings = np.array(val_df['rating'])\n",
    "    \n",
    "    sse = 0\n",
    "    num_rated = 0\n",
    "    preds, acts = [], []\n",
    "    actual_v_pred = np.zeros((10,10))\n",
    "    for idx in range(len(val_users)):\n",
    "        try:\n",
    "            pred = predict_rating(user_mat, movie_mat, val_users[idx], val_movies[idx])\n",
    "            sse += (val_ratings[idx] - pred)**2\n",
    "            num_rated+=1\n",
    "            preds.append(pred)\n",
    "            acts.append(val_ratings[idx])\n",
    "            actual_v_pred[11-int(val_ratings[idx]-1), int(round(pred)-1)]+=1\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    rmse = np.sqrt(sse/num_rated)\n",
    "    perc_rated = num_rated/len(val_users)\n",
    "    return rmse, perc_rated, actual_v_pred, preds, acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0486163799 0.4795\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWUAAAETCAYAAADu/hdnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cb9b6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How well did we do?\n",
    "rmse, perc_rated, actual_v_pred, preds, acts = validation_comparison(val_df)\n",
    "print(rmse, perc_rated)\n",
    "sns.heatmap(actual_v_pred);\n",
    "plt.xticks(np.arange(10), np.arange(1,11));\n",
    "plt.yticks(np.arange(10), np.arange(1,11));\n",
    "plt.xlabel(\"Predicted Values\");\n",
    "plt.ylabel(\"Actual Values\");\n",
    "plt.title(\"Actual vs. Predicted Values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHtCAYAAAAjlrYyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUmEMZ0mSCmM4S5JUGMNZkqTC/H/wGup1JLrFtgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e098e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(acts, normed=True, alpha=.5, label='actual');\n",
    "plt.hist(preds, normed=True, alpha=.5, label='predicted');\n",
    "plt.legend(loc=2, prop={'size': 15});\n",
    "plt.xlabel('Rating');\n",
    "plt.title('Predicted vs. Actual Rating');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` We didn't do so bad on making those predictions!  But, how many user-movie pairs were we unable to make predictions for?  Use the cell below to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number not rated 1041\n",
      "Number rated 959.\n"
     ]
    }
   ],
   "source": [
    "# From the above, this can be calculated as follows:\n",
    "print(\"Number not rated {}\".format(int(len(val_df['rating'])*(1-perc_rated))))\n",
    "print(\"Number rated {}.\".format(int(len(val_df['rating'])*perc_rated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Based For New Movies\n",
    "\n",
    "If all of the above went well, you will notice we still have work to do!  We need to bring in a few things we picked up from the last lesson to use for those new users and movies.  Below is the code used to make the content based recommendations, which found movies that were similar to one another.  This was from **5_Content_Based_Recommendations** in the previous lesson.\n",
    "\n",
    "The below function **find_similar_movies** will provide similar movies to any movie based only on content.  \n",
    "\n",
    "Run the cell below to gain access to the content based similarity functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subset so movie_content is only using the dummy variables for each genre and the 3 century based year dummy columns\n",
    "movie_content = np.array(movies.iloc[:,4:])\n",
    "\n",
    "# Take the dot product to obtain a movie x movie matrix of similarities\n",
    "dot_prod_movies = movie_content.dot(np.transpose(movie_content))\n",
    "\n",
    "\n",
    "def find_similar_movies(movie_id):\n",
    "    '''\n",
    "    INPUT\n",
    "    movie_id - a movie_id \n",
    "    OUTPUT\n",
    "    similar_movies - an array of the most similar movies by title\n",
    "    '''\n",
    "    # find the row of each movie id\n",
    "    movie_idx = np.where(movies['movie_id'] == movie_id)[0][0]\n",
    "    \n",
    "    # find the most similar movie indices - to start I said they need to be the same for all content\n",
    "    similar_idxs = np.where(dot_prod_movies[movie_idx] == np.max(dot_prod_movies[movie_idx]))[0]\n",
    "    \n",
    "    # pull the movie titles based on the indices\n",
    "    similar_movies = np.array(movies.iloc[similar_idxs, ]['movie'])\n",
    "    \n",
    "    return similar_movies\n",
    "    \n",
    "    \n",
    "def get_movie_names(movie_ids):\n",
    "    '''\n",
    "    INPUT\n",
    "    movie_ids - a list of movie_ids\n",
    "    OUTPUT\n",
    "    movies - a list of movie names associated with the movie_ids\n",
    "    \n",
    "    '''\n",
    "    movie_lst = list(movies[movies['movie_id'].isin(movie_ids)]['movie'])\n",
    "   \n",
    "    return movie_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Based For New Users\n",
    "\n",
    "From the above two code cells, we have a way to make recommendations for movie-user pairs that have ratings in any part of our user-movie matrix.  We also have a way to make ratings for movies that have never received a rating using movie similarities.\n",
    "\n",
    "In this last part here, we need a way to make recommendations to new users.  For this, our functions from **2_Most_Popular_Recommendations** in Lesson 1 will come in handy.  Run the cell below to have these functions available.\n",
    "\n",
    "Run the cell below to gain access to the rank based functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_ranked_df(movies, reviews):\n",
    "        '''\n",
    "        INPUT\n",
    "        movies - the movies dataframe\n",
    "        reviews - the reviews dataframe\n",
    "        \n",
    "        OUTPUT\n",
    "        ranked_movies - a dataframe with movies that are sorted by highest avg rating, more reviews, \n",
    "                        then time, and must have more than 4 ratings\n",
    "        '''\n",
    "        \n",
    "        # Pull the average ratings and number of ratings for each movie\n",
    "        movie_ratings = reviews.groupby('movie_id')['rating']\n",
    "        avg_ratings = movie_ratings.mean()\n",
    "        num_ratings = movie_ratings.count()\n",
    "        last_rating = pd.DataFrame(reviews.groupby('movie_id').max()['date'])\n",
    "        last_rating.columns = ['last_rating']\n",
    "\n",
    "        # Add Dates\n",
    "        rating_count_df = pd.DataFrame({'avg_rating': avg_ratings, 'num_ratings': num_ratings})\n",
    "        rating_count_df = rating_count_df.join(last_rating)\n",
    "\n",
    "        # merge with the movies dataset\n",
    "        movie_recs = movies.set_index('movie_id').join(rating_count_df)\n",
    "\n",
    "        # sort by top avg rating and number of ratings\n",
    "        ranked_movies = movie_recs.sort_values(['avg_rating', 'num_ratings', 'last_rating'], ascending=False)\n",
    "\n",
    "        # for edge cases - subset the movie list to those with only 5 or more reviews\n",
    "        ranked_movies = ranked_movies[ranked_movies['num_ratings'] > 4]\n",
    "        \n",
    "        return ranked_movies\n",
    "    \n",
    "\n",
    "def popular_recommendations(user_id, n_top, ranked_movies):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id (str) of the individual you are making recommendations for\n",
    "    n_top - an integer of the number recommendations you want back\n",
    "    ranked_movies - a pandas dataframe of the already ranked movies based on avg rating, count, and time\n",
    "\n",
    "    OUTPUT:\n",
    "    top_movies - a list of the n_top recommended movies by movie title in order best to worst\n",
    "    '''\n",
    "\n",
    "    top_movies = list(ranked_movies['movie'][:n_top])\n",
    "\n",
    "    return top_movies\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now For Your Task\n",
    "\n",
    "The above cells set up everything we need to use to make predictions.  Your task is to write a function, which uses the above information as necessary to provide recommendations for every user in the **val_df** dataframe.  There isn't one right way to do this, but using a blend between the three could be your best bet.  \n",
    "\n",
    "You can see the blended approach I used in the video on the next page, but feel free to be creative with your solution!\n",
    "\n",
    "`3.` Use the function below along with the document strings to assist with completing the task for this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendations(_id, _id_type='movie', train_data=train_data_df, \n",
    "                         train_df=train_df, movies=movies, rec_num=5, user_mat=user_mat):\n",
    "    '''\n",
    "    INPUT:\n",
    "    _id - either a user or movie id (int)\n",
    "    _id_type - \"movie\" or \"user\" (str)\n",
    "    train_data - dataframe of data as user-movie matrix\n",
    "    train_df - dataframe of training data reviews\n",
    "    movies - movies df\n",
    "    rec_num - number of recommendations to return (int)\n",
    "    user_mat - the U matrix of matrix factorization\n",
    "    movie_mat - the V matrix of matrix factorization\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (array) a list or numpy array of recommended movies like the \n",
    "                   given movie, or recs for a user_id given\n",
    "    '''\n",
    "    # if the user is available from the matrix factorization data, \n",
    "    # I will use this and rank movies based on the predicted values\n",
    "    # For use with user indexing\n",
    "    val_users = train_data_df.index\n",
    "    rec_ids = create_ranked_df(movies, train_df)\n",
    "    \n",
    "    if _id_type == 'user':\n",
    "        if _id in train_data.index:\n",
    "            # Get the index of which row the user is in for use in U matrix\n",
    "            idx = np.where(val_users == _id)[0][0]\n",
    "            \n",
    "            # take the dot product of that row and the V matrix\n",
    "            preds = np.dot(user_mat[idx,:],movie_mat)\n",
    "            \n",
    "            # pull the top movies according to the prediction\n",
    "            indices = preds.argsort()[-rec_num:][::-1] #indices\n",
    "            rec_ids = train_data_df.columns[indices]\n",
    "            rec_names = get_movie_names(rec_ids)\n",
    "            \n",
    "        else:\n",
    "            # if we don't have this user, give just top ratings back\n",
    "            rec_names = popular_recommendations(_id, rec_num, ranked_movies)\n",
    "            \n",
    "    # Find similar movies if it is a movie that is passed\n",
    "    else:\n",
    "        rec_ids = find_similar_movies(_id)\n",
    "        rec_names = get_movie_names(rec_ids)\n",
    "    \n",
    "    return rec_ids, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Int64Index([92965, 838283, 1659337, 1205489, 421715], dtype='int64', name='movie_id'),\n",
       " ['Empire of the Sun (1987)',\n",
       "  'The Curious Case of Benjamin Button (2008)',\n",
       "  'Step Brothers (2008)',\n",
       "  'Gran Torino (2008)',\n",
       "  'The Perks of Being a Wallflower (2012)'])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_recommendations(48, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1241"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(val_df['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make recommendations\n",
    "user_recs_dict_with_top = dict()\n",
    "for user_id in set(val_df['user_id']):\n",
    "    user_recs_dict_with_top[user_id] = make_recommendations(user_id, 'user')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For user 51201, our recommendations are: \n",
      " ['The Dark Knight (2008)', 'Argo (2012)', 'Looper (2012)', 'Stoker (2013)', 'The Sessions (2012)']\n",
      "For user 53249, our recommendations are: \n",
      " ['Empire of the Sun (1987)', 'The Curious Case of Benjamin Button (2008)', 'Step Brothers (2008)', 'Incendies (2010)', 'Shame (2011)']\n",
      "For user 20486, our recommendations are: \n",
      " ['Blood Diamond (2006)', 'There Will Be Blood (2007)', 'Silver Linings Playbook (2012)', 'Cloud Atlas (2012)', 'Searching for Sugar Man (2012)']\n",
      "For user 4103, our recommendations are: \n",
      " ['Empire of the Sun (1987)', 'Casino (1995)', 'De reis van Chihiro (2001)', 'Argo (2012)', 'Les Misérables (2012)']\n",
      "For user 34824, our recommendations are: \n",
      " ['Fight Club (1999)', 'Life of Pi (2012)', 'Incendies (2010)', 'Gangster Squad (2013)', 'Ruby Sparks (2012)']\n",
      "For user 32774, our recommendations are: \n",
      " ['Taegukgi hwinalrimyeo (2004)', 'Lincoln (2012)', 'Blood Diamond (2006)', 'The Fall (2006)', \"De rouille et d'os (2012)\"]\n",
      "For user 16395, our recommendations are: \n",
      " ['Empire of the Sun (1987)', 'Casino (1995)', 'Blood Diamond (2006)', 'Silver Linings Playbook (2012)', 'Django Unchained (2012)']\n",
      "For user 43020, our recommendations are: \n",
      " ['Empire of the Sun (1987)', 'The Fall (2006)', 'Argo (2012)', 'Shame (2011)', 'Django Unchained (2012)']\n",
      "For user 24589, our recommendations are: \n",
      " ['Empire of the Sun (1987)', 'The Shawshank Redemption (1994)', 'Silver Linings Playbook (2012)', 'Incendies (2010)', 'Django Unchained (2012)']\n",
      "For user 34829, our recommendations are: \n",
      " []\n",
      "For user 53263, our recommendations are: \n",
      " ['Empire of the Sun (1987)', 'Blood Diamond (2006)', 'Life of Pi (2012)', 'Incendies (2010)', 'Shame (2011)']\n",
      "For user 43022, our recommendations are: \n",
      " ['The Godfather (1972)', 'Empire of the Sun (1987)', 'Blood Diamond (2006)', 'Incendies (2010)', 'Django Unchained (2012)']\n",
      "For user 51201, our recommendations are: \n",
      " ['The Dark Knight (2008)', 'Argo (2012)', 'Looper (2012)', 'Stoker (2013)', 'The Sessions (2012)']\n",
      "For user 53249, our recommendations are: \n",
      " ['Empire of the Sun (1987)', 'The Curious Case of Benjamin Button (2008)', 'Step Brothers (2008)', 'Incendies (2010)', 'Shame (2011)']\n",
      "For user 20486, our recommendations are: \n",
      " ['Blood Diamond (2006)', 'There Will Be Blood (2007)', 'Silver Linings Playbook (2012)', 'Cloud Atlas (2012)', 'Searching for Sugar Man (2012)']\n",
      "For user 4103, our recommendations are: \n",
      " ['Empire of the Sun (1987)', 'Casino (1995)', 'De reis van Chihiro (2001)', 'Argo (2012)', 'Les Misérables (2012)']\n",
      "For user 34824, our recommendations are: \n",
      " ['Fight Club (1999)', 'Life of Pi (2012)', 'Incendies (2010)', 'Gangster Squad (2013)', 'Ruby Sparks (2012)']\n",
      "For user 32774, our recommendations are: \n",
      " ['Taegukgi hwinalrimyeo (2004)', 'Lincoln (2012)', 'Blood Diamond (2006)', 'The Fall (2006)', \"De rouille et d'os (2012)\"]\n",
      "For user 16395, our recommendations are: \n",
      " ['Empire of the Sun (1987)', 'Casino (1995)', 'Blood Diamond (2006)', 'Silver Linings Playbook (2012)', 'Django Unchained (2012)']\n",
      "For user 43020, our recommendations are: \n",
      " ['Empire of the Sun (1987)', 'The Fall (2006)', 'Argo (2012)', 'Shame (2011)', 'Django Unchained (2012)']\n",
      "For user 24589, our recommendations are: \n",
      " ['Empire of the Sun (1987)', 'The Shawshank Redemption (1994)', 'Silver Linings Playbook (2012)', 'Incendies (2010)', 'Django Unchained (2012)']\n",
      "For user 34829, our recommendations are: \n",
      " ['Goodfellas (1990)', 'Step Brothers (2008)', 'American Beauty (1999)', 'There Will Be Blood (2007)', 'Gran Torino (2008)']\n",
      "For user 53263, our recommendations are: \n",
      " ['Empire of the Sun (1987)', 'Blood Diamond (2006)', 'Life of Pi (2012)', 'Incendies (2010)', 'Shame (2011)']\n",
      "For user 43022, our recommendations are: \n",
      " ['The Godfather (1972)', 'Empire of the Sun (1987)', 'Blood Diamond (2006)', 'Incendies (2010)', 'Django Unchained (2012)']\n"
     ]
    }
   ],
   "source": [
    "cnter = 0\n",
    "for user, rec in user_recs_dict_with_top.items():\n",
    "    if cnter < 12:\n",
    "        print(\"For user {}, our recommendations are: \\n {}\".format(user, rec))\n",
    "        cnter+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This recommendation style looks like it may do okay with accuracy, but it seems like a lot of the same movies are showing up.  When we think back to serendipity, novelty, and diversity as means of a good recommendation system, this set of recommendations still isn't great.  We might consider providing some content based recommendations from movies an individual has watched along with these recommendations to meet those categories of a good recommender.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22 The Cold Start Problem Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23 Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations Class Should:\n",
    "- Have a fit method\n",
    "- Predict the rating for a given user-movie\n",
    "- Give a list of recommendations for a given user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24 Code Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25 Workspace: Recommender Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, you got your hands on some of the most important ideas associated with recommendation systems:\n",
    "\n",
    "### Recommender Validation\n",
    "You looked at methods for validating your recommendations (when possible) using offline methods. In these cases, you could split your data into training and testing data. Frequently this split is based on time, where events earlier in time are in the training data, and events later in time are in a testing dataset.\n",
    "\n",
    "We also quickly introduced the idea of being able to see how well your recommendation engine works by simply throwing it out into the world to directly see the impact.\n",
    "\n",
    "### Matrix Factorization with SVD\n",
    "Next, we looked at matrix factorization as a technique for making recommendations. Traditional singular value decomposition a technique can be used when your matrices have no missing values. In this decomposition technique, a user-item (A) can be decomposed as follows:\n",
    "\n",
    "$A = U\\Sigma V^T$\n",
    " \n",
    "\n",
    "Where\n",
    "- $U $gives information about how users are related to latent features.\n",
    "- $\\Sigma$ gives information about how much latent features matter towards recreating the user-item matrix.\n",
    "- $V^T$ gives information about how much each movie is related to latent features.\n",
    "\n",
    "Since this traditional decomposition doesn't actually work when our matrices have missing values, we looked at another method for decomposing matrices.\n",
    "\n",
    "### FunkSVD\n",
    "FunkSVD was a new method that you found to be useful for matrices with missing values. With this matrix factorization you decomposed a user-item (A) as follows:\n",
    "\n",
    "$A = UV^T$\n",
    " \n",
    "\n",
    "Where\n",
    "- $U$ gives information about how users are related to latent features.\n",
    "- $V^T$ gives information about how much each movie is related to latent features.\n",
    "\n",
    "You found that you could iterate to find the latent features in each of these matrices using gradient descent. You wrote a function to implement gradient descent to find the values within these two matrices.\n",
    "\n",
    "Using this method, you were able to make a prediction for any user-movie pair in your dataset. You also could use it to test how well your predictions worked on a train-test split of the data. However, this method fell short with new users or movies.\n",
    "\n",
    "### The Cold Start Problem\n",
    "Collaborative filtering using FunkSVD still wasn't helpful for new users and new movies. In order to recommend these items, you implemented content based and ranked based recommendations along with your FunkSVD implementation.\n",
    "\n",
    "### Author's Note\n",
    "There are so many ways to make recommendations, and this course provides you a very strong mind and skill set to tackle building your own recommendation systems in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
