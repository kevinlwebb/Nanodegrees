{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- 1. [NLP and Pipelines](#1.-NLP-and-Pipelines)\n",
    "- 2. [How NLP and Pipelines Work](#2.-How-NLP-and-Pipelines-Work)\n",
    "- 3. [Text Processing](#3.-Text-Processing)\n",
    "- 4. [Cleaning](#4.-Cleaning)\n",
    "- 5. [Exercise: Cleaning](#5.-Exercise:-Cleaning)\n",
    "- 6. [NLP and Pipelines](#1.-NLP-and-Pipelines)\n",
    "- 7. [NLP and Pipelines](#1.-NLP-and-Pipelines)\n",
    "- 8. [NLP and Pipelines](#1.-NLP-and-Pipelines)\n",
    "- 9. [NLP and Pipelines](#1.-NLP-and-Pipelines)\n",
    "- 10. [NLP and Pipelines](#1.-NLP-and-Pipelines)\n",
    "- 11. [NLP and Pipelines](#1.-NLP-and-Pipelines)\n",
    "- 12. [NLP and Pipelines](#1.-NLP-and-Pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLP and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing Pipelines\n",
    "In this lesson, you'll be introduced to some of the steps involved in a NLP pipeline:\n",
    "\n",
    "1. Text Processing\n",
    "  - Cleaning\n",
    "  - Normalization\n",
    "  - Tokenization\n",
    "  - Stop Word Removal\n",
    "  - Part of Speech Tagging\n",
    "  - Named Entity Recognition\n",
    "  - Stemming and Lemmatization\n",
    "2. Feature Extraction\n",
    "  - Bag of Words\n",
    "  - TF-IDF\n",
    "  - Word Embeddings\n",
    "3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. How NLP and Pipelines Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How NLP Pipelines Work\n",
    "The 3 stages of an NLP pipeline are: Text Processing > Feature Extraction > Modeling.\n",
    "\n",
    "- **Text Processing**: Take raw input text, clean it, normalize it, and convert it into a form that is suitable for feature extraction.\n",
    "- **Feature Extraction**: Extract and produce feature representations that are appropriate for the type of NLP task you are trying to accomplish and the type of model you are planning to use.\n",
    "- **Modeling**: Design a statistical or machine learning model, fit its parameters to training data, use an optimization procedure, and then use it to make predictions about unseen data.\n",
    "\n",
    "This process isn't always linear and may require additional steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Text Processing\n",
    "The first chunk of this lesson will explore the steps involved in **text processing**, the first stage of the NLP pipeline.\n",
    "\n",
    "### Why Do We Need to Process Text?\n",
    "Source: https://en.wikipedia.org/wiki/Kingfisher\n",
    "\n",
    "- **Extracting plain text**: Textual data can come from a wide variety of sources: the web, PDFs, word documents, speech recognition systems, book scans, etc. Your goal is to extract plain text that is free of any source specific markup or constructs that are not relevant to your task.\n",
    "- **Reducing complexity**: Some features of our language like capitalization, punctuation, and common words such as a, of, and the, often help provide structure, but don't add much meaning. Sometimes it's best to remove them if that helps reduce the complexity of the procedures you want to apply later.\n",
    "\n",
    "### What Text Processing Will You Do in This Lesson?\n",
    "You'll prepare text data from different sources with the following text processing steps:\n",
    "\n",
    "1. **Cleaning** to remove irrelevant items, such as HTML tags\n",
    "2. **Normalizing** by converting to all lowercase and removing punctuation\n",
    "3. Splitting text into words or **tokens**\n",
    "4. Removing words that are too common, also known as **stop words**\n",
    "5. Identifying different **parts of speech** and **named entities**\n",
    "6. Converting words into their dictionary forms, using **stemming and lemmatization**\n",
    "\n",
    "After performing these steps, your text will capture the essence of what was being conveyed in a form that is easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "Let's walk through an example of cleaning text data from a popular source - the web. You'll be introduced to helpful tools in working with this data, including the **requests** library, **regular expressions**, and **Beautiful Soup**.\n",
    "\n",
    "Note: The website used in this example has since been updated with a new layout. In the next page, you'll work through the steps shown here for the new web page.\n",
    "\n",
    "**Documentation for Python Libraries**:\n",
    "- [Requests](http://docs.python-requests.org/en/master/user/quickstart/#make-a-request)\n",
    "- [Regular Expressions](https://docs.python.org/3/library/re.html)\n",
    "- [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exercise: Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WE-5f42Lo19"
   },
   "source": [
    "## Cleaning Quiz: Udacity's Course Catalog\n",
    "It's your turn! Udacity's [course catalog page](https://www.udacity.com/courses/all) has changed since the last video was filmed. One notable change is the introduction of  _schools_.\n",
    "\n",
    "In this activity, you're going to perform similar actions with BeautifulSoup to extract the following information from each course listing on the page:\n",
    "1. The course name - e.g. \"Data Analyst\"\n",
    "2. The school the course belongs to - e.g. \"School of Data Science\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Courses: 237\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# fetch web page\n",
    "r = requests.get(\"https://www.udacity.com/courses/all\")\n",
    "\n",
    "# Use \"lxml\" rather than \"html5lib\".\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "# Find all course summaries\n",
    "summaries = soup.find_all(\"div\", {\"class\":\"course-summary-card\"})\n",
    "print('Number of Courses:', len(summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div _ngcontent-sc213=\"\" class=\"course-summary-card row row-gap-medium catalog-card nanodegree-card ng-star-inserted\">\n",
      " <ir-catalog-card _ngcontent-sc213=\"\" _nghost-sc216=\"\">\n",
      "  <div _ngcontent-sc216=\"\" class=\"card-wrapper is-collapsed\">\n",
      "   <div _ngcontent-sc216=\"\" class=\"card__inner card mb-0\">\n",
      "    <div _ngcontent-sc216=\"\" class=\"card__inner--upper\">\n",
      "     <div _ngcontent-sc216=\"\" class=\"image_wrapper hidden-md-down\">\n",
      "      <a _ngcontent-sc216=\"\" href=\"/course/product-manager-nanodegree--nd036\">\n",
      "       <!-- -->\n",
      "       <div _ngcontent-sc216=\"\" class=\"image-container ng-star-inserted\" style=\"background-image:url(https://d20vrrgs8k4bvw.cloudfront.net/images/degrees/nd036/catalog+image+nd036.jpg);\">\n",
      "        <div _ngcontent-sc216=\"\" class=\"image-overlay\">\n",
      "        </div>\n",
      "       </div>\n",
      "      </a>\n",
      "      <!-- -->\n",
      "     </div>\n",
      "     <div _ngcontent-sc216=\"\" class=\"card-content\">\n",
      "      <!-- -->\n",
      "      <span _ngcontent-sc216=\"\" class=\"tag tag--new card ng-star-inserted\">\n",
      "       New\n",
      "      </span>\n",
      "      <!-- -->\n",
      "      <div _ngcontent-sc216=\"\" class=\"category-wrapper\">\n",
      "       <span _ngcontent-sc216=\"\" class=\"mobile-icon\">\n",
      "       </span>\n",
      "       <!-- -->\n",
      "       <h4 _ngcontent-sc216=\"\" class=\"category ng-star-inserted\">\n",
      "        School of Business\n",
      "       </h4>\n",
      "      </div>\n",
      "      <h3 _ngcontent-sc216=\"\" class=\"card-heading\">\n",
      "       <a _ngcontent-sc216=\"\" class=\"capitalize\" href=\"/course/product-manager-nanodegree--nd036\">\n",
      "        Product Manager\n",
      "       </a>\n",
      "      </h3>\n",
      "      <div _ngcontent-sc216=\"\" class=\"right-sub\">\n",
      "       <!-- -->\n",
      "       <div _ngcontent-sc216=\"\" class=\"skills ng-star-inserted\">\n",
      "        <h4 _ngcontent-sc216=\"\">\n",
      "         Skills Covered\n",
      "        </h4>\n",
      "        <span _ngcontent-sc216=\"\" class=\"truncate-content\">\n",
      "         <!-- -->\n",
      "         <span _ngcontent-sc216=\"\" class=\"ng-star-inserted\">\n",
      "          Product Strategy,\n",
      "         </span>\n",
      "         <span _ngcontent-sc216=\"\" class=\"ng-star-inserted\">\n",
      "          Product Design,\n",
      "         </span>\n",
      "         <span _ngcontent-sc216=\"\" class=\"ng-star-inserted\">\n",
      "          Product Development,\n",
      "         </span>\n",
      "         <span _ngcontent-sc216=\"\" class=\"ng-star-inserted\">\n",
      "          Design Sprint,\n",
      "         </span>\n",
      "         <span _ngcontent-sc216=\"\" class=\"ng-star-inserted\">\n",
      "          Product Launch\n",
      "         </span>\n",
      "        </span>\n",
      "       </div>\n",
      "       <!-- -->\n",
      "       <div _ngcontent-sc216=\"\" class=\"hidden-md-up level\">\n",
      "        <span _ngcontent-sc216=\"\" class=\"course-level course-level-beginner\" classname=\"course-level course-level-beginner\">\n",
      "        </span>\n",
      "        <span _ngcontent-sc216=\"\" class=\"capitalize\">\n",
      "         beginner\n",
      "        </span>\n",
      "       </div>\n",
      "      </div>\n",
      "     </div>\n",
      "    </div>\n",
      "    <div _ngcontent-sc216=\"\" class=\"card__inner--lower hidden-sm-down\">\n",
      "     <div _ngcontent-sc216=\"\" class=\"left uppercase blue expander pointer\">\n",
      "      <!-- -->\n",
      "      <span _ngcontent-sc216=\"\" class=\"ng-star-inserted\">\n",
      "       Program Details\n",
      "      </span>\n",
      "      <!-- -->\n",
      "     </div>\n",
      "     <div _ngcontent-sc216=\"\" class=\"right\">\n",
      "      <!-- -->\n",
      "      <span _ngcontent-sc216=\"\" class=\"caption text-right level ng-star-inserted\">\n",
      "       <span _ngcontent-sc216=\"\" class=\"course-level course-level-beginner\" classname=\"course-level course-level-beginner\">\n",
      "       </span>\n",
      "       <span _ngcontent-sc216=\"\" class=\"capitalize\">\n",
      "        beginner\n",
      "       </span>\n",
      "      </span>\n",
      "     </div>\n",
      "    </div>\n",
      "   </div>\n",
      "   <div _ngcontent-sc216=\"\" class=\"card__expander\">\n",
      "    <div _ngcontent-sc216=\"\" class=\"card__expander--summary mb-1\">\n",
      "     <!-- -->\n",
      "     <span _ngcontent-sc216=\"\" class=\"ng-star-inserted\">\n",
      "      Envision and execute the development of industry-defining products, and learn how to successfully bring them to market.\n",
      "     </span>\n",
      "    </div>\n",
      "    <hr _ngcontent-sc216=\"\"/>\n",
      "    <div _ngcontent-sc216=\"\" class=\"card__expander--details\">\n",
      "     <div _ngcontent-sc216=\"\" class=\"rating\">\n",
      "      <!-- -->\n",
      "     </div>\n",
      "     <a _ngcontent-sc216=\"\" class=\"button--primary btn\" href=\"/course/product-manager-nanodegree--nd036\">\n",
      "      Learn More\n",
      "     </a>\n",
      "    </div>\n",
      "   </div>\n",
      "  </div>\n",
      " </ir-catalog-card>\n",
      " <!-- -->\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the first summary in summaries\n",
    "print(summaries[0].prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Manager\n",
      "School of Business\n"
     ]
    }
   ],
   "source": [
    "# Extract course title\n",
    "ct = summaries[0].select_one(\"h3\").get_text().strip()\n",
    "print(ct)\n",
    "\n",
    "# Extract school\n",
    "school = summaries[0].select_one(\"h4\").get_text().strip()\n",
    "print(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237 course summaries found. Sample:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Product Manager', 'School of Business'),\n",
       " ('AI for Business Leaders', 'School of Business'),\n",
       " ('Intro to Machine Learning with TensorFlow',\n",
       "  'School of Artificial Intelligence'),\n",
       " ('UX Designer', 'School of Business'),\n",
       " ('Data Streaming', 'School of Data Science'),\n",
       " ('Front End Web Developer', 'School of Programming'),\n",
       " ('Full Stack Web Developer', 'School of Programming'),\n",
       " ('Java Developer', 'School of Programming'),\n",
       " ('AI Product Manager', 'School of Artificial Intelligence'),\n",
       " ('Sensor Fusion Engineer', 'School of Autonomous Systems'),\n",
       " ('Data Visualization', 'School of Data Science'),\n",
       " ('Cloud Developer', 'School of Cloud Computing'),\n",
       " ('Cloud DevOps Engineer', 'School of Cloud Computing'),\n",
       " ('Intro to Machine Learning with PyTorch',\n",
       "  'School of Artificial Intelligence'),\n",
       " ('C++', 'School of Autonomous Systems'),\n",
       " ('Data Structures and Algorithms', 'School of Programming'),\n",
       " ('Programming for Data Science with R', 'School of Data Science'),\n",
       " ('Data Engineer', 'School of Data Science'),\n",
       " ('Marketing Analytics', 'School of Business'),\n",
       " ('Introduction to Programming', 'School of Programming')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses = []\n",
    "for summary in summaries:\n",
    "    # append name and school of each summary to courses list\n",
    "    title = summary.select_one(\"h3\").get_text().strip()\n",
    "    school = summary.select_one(\"h4\").get_text().strip()\n",
    "    courses.append((title, school))\n",
    "    \n",
    "# display results\n",
    "print(len(courses), \"course summaries found. Sample:\")\n",
    "courses[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lower case text\n",
    "- Replace characters that are not a-z, A-Z, or numbers with a space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Exercise: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ? \n",
      "\n",
      "the first time you see the second renaissance it may look boring. look at it at least twice and definitely watch part 2. it will change your view of the matrix. are the human people the ones who started the war ? is ai a bad thing ? \n",
      "\n",
      "the first time you see the second renaissance it may look boring  look at it at least twice and definitely watch part 2  it will change your view of the matrix  are the human people the ones who started the war   is ai a bad thing  \n"
     ]
    }
   ],
   "source": [
    "text = \"The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?\"\n",
    "print(text,\"\\n\")\n",
    "\n",
    "# Convert to lowercase\n",
    "text = text.lower() \n",
    "print(text,\"\\n\")\n",
    "\n",
    "import re\n",
    "\n",
    "# Remove punctuation characters\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text) \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Reference:\n",
    "- `nltk.tokenize` package: http://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Exercise: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kevinwebb/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers. \n",
      "\n",
      "['Dr.', 'Smith', 'graduated', 'from', 'the', 'University', 'of', 'Washington', '.', 'He', 'later', 'started', 'an', 'analytics', 'firm', 'called', 'Lux', ',', 'which', 'catered', 'to', 'enterprise', 'customers', '.'] \n",
      "\n",
      "['Dr. Smith graduated from the University of Washington.', 'He later started an analytics firm called Lux, which catered to enterprise customers.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers.\"\n",
    "print(text, \"\\n\")\n",
    "\n",
    "# Split text into words using NLTK\n",
    "words = word_tokenize(text)\n",
    "print(words, \"\\n\")\n",
    "\n",
    "# Split text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimportant, common words\n",
    "- I, me, myself, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Exercise: Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kevinwebb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kevinwebb/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ? \n",
      "\n",
      "['the', 'first', 'time', 'you', 'see', 'the', 'second', 'renaissance', 'it', 'may', 'look', 'boring', 'look', 'at', 'it', 'at', 'least', 'twice', 'and', 'definitely', 'watch', 'part', '2', 'it', 'will', 'change', 'your', 'view', 'of', 'the', 'matrix', 'are', 'the', 'human', 'people', 'the', 'ones', 'who', 'started', 'the', 'war', 'is', 'ai', 'a', 'bad', 'thing'] \n",
      "\n",
      "['first', 'time', 'see', 'second', 'renaissance', 'may', 'look', 'boring', 'look', 'least', 'twice', 'definitely', 'watch', 'part', '2', 'change', 'view', 'matrix', 'human', 'people', 'ones', 'started', 'war', 'ai', 'bad', 'thing'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?\"\n",
    "print(text, \"\\n\")\n",
    "\n",
    "# Normalize text\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "# Tokenize text\n",
    "words = word_tokenize(text)\n",
    "print(words, \"\\n\")\n",
    "\n",
    "# Remove stop words\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parts of Speech Tagging\n",
    "- Sentence Parsing (Tree)\n",
    "\n",
    "**Note**: Part-of-speech tagging using a predefined grammar like this is a simple, but limited, solution. It can be very tedious and error-prone for a large corpus of text, since you have to account for all possible sentence structures and tags!\n",
    "\n",
    "There are other more advanced forms of POS tagging that can learn sentence structures and tags from given data, including Hidden Markov Models (HMMs) and Recurrent Neural Networks (RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entities\n",
    "Noun phrases that refer to specific object, person, or place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Exercise: POS and NES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/kevinwebb/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/kevinwebb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kevinwebb/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/kevinwebb/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('always', 'RB'),\n",
       " ('lie', 'VBP'),\n",
       " ('down', 'RP'),\n",
       " ('to', 'TO'),\n",
       " ('tell', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('lie', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I always lie down to tell a lie.\"\n",
    "\n",
    "# tokenize text\n",
    "sentence = word_tokenize(text)\n",
    "\n",
    "# tag each word with part of speech\n",
    "pos_tag(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Antonio/NNP)\n",
      "  joined/VBD\n",
      "  (ORGANIZATION Udacity/NNP Inc./NNP)\n",
      "  in/IN\n",
      "  (GPE California/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"Antonio joined Udacity Inc. in California.\"\n",
    "\n",
    "# tokenize, pos tag, then recognize named entities in text\n",
    "tree = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "# Define a custom grammar\n",
    "my_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(my_grammar)\n",
    "\n",
    "# Parse a sentence\n",
    "sentence = word_tokenize(\"I shot an elephant in my pajamas\")\n",
    "for tree in parser.parse(sentence):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Process of reducing a word to its stem or root form. This eliminates the letters at the end.\n",
    "- branch: branches, branching, branched\n",
    "\n",
    "## Lemmatization\n",
    "Process of reducing words to a normalized form. This uses a dicionary of variants to roots.\n",
    "- be: is, was, were"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Exercise: Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kevinwebb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kevinwebb/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') # download for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'first', 'time', 'you', 'see', 'the', 'second', 'renaissance', 'it', 'may', 'look', 'boring', 'look', 'at', 'it', 'at', 'least', 'twice', 'and', 'definitely', 'watch', 'part', '2', 'it', 'will', 'change', 'your', 'view', 'of', 'the', 'matrix', 'are', 'the', 'human', 'people', 'the', 'ones', 'who', 'started', 'the', 'war', 'is', 'ai', 'a', 'bad', 'thing'] \n",
      "\n",
      "['first', 'time', 'see', 'second', 'renaissance', 'may', 'look', 'boring', 'look', 'least', 'twice', 'definitely', 'watch', 'part', '2', 'change', 'view', 'matrix', 'human', 'people', 'ones', 'started', 'war', 'ai', 'bad', 'thing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?\"\n",
    "\n",
    "# Normalize text\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "# Tokenize text\n",
    "words = text.split()\n",
    "print(words, \"\\n\")\n",
    "\n",
    "# Remove stop words\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'see', 'second', 'renaiss', 'may', 'look', 'bore', 'look', 'least', 'twice', 'definit', 'watch', 'part', '2', 'chang', 'view', 'matrix', 'human', 'peopl', 'one', 'start', 'war', 'ai', 'bad', 'thing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'see', 'second', 'renaissance', 'may', 'look', 'boring', 'look', 'least', 'twice', 'definitely', 'watch', 'part', '2', 'change', 'view', 'matrix', 'human', 'people', 'one', 'started', 'war', 'ai', 'bad', 'thing'] \n",
      "\n",
      "['first', 'time', 'see', 'second', 'renaissance', 'may', 'look', 'bore', 'look', 'least', 'twice', 'definitely', 'watch', 'part', '2', 'change', 'view', 'matrix', 'human', 'people', 'one', 'start', 'war', 'ai', 'bad', 'thing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmed, \"\\n\")\n",
    "\n",
    "# Lemmatize verbs by specifying pos\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Text Processing Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Workflow\n",
    "- Sentence\n",
    "  - Normalize\n",
    "  - Tokenize\n",
    "  - Remove Stop Words\n",
    "  - Stem / Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have clean text, but still can't use it for a model.\n",
    "\n",
    "## What type of model?\n",
    "For graph based models\n",
    "- represent text as symbolic nodes with relationships between them like WordNet\n",
    "\n",
    "For statistical models\n",
    "- numerical representations\n",
    "\n",
    "## What is the end goal of statistical model?\n",
    "Perform a document level task (ex: spam detection)\n",
    "- per document representations such as bag-of-words or doc2vec\n",
    "\n",
    "Individual words or phrases (ex: text generation or machine translation)\n",
    "- word level representation such as word2vec or glove\n",
    "\n",
    "\n",
    "WordNet visualization tool: http://mateogianolio.com/wordnet-visualization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature representation\n",
    "\n",
    "Treats each document as an unordered collection or bag of words\n",
    "\n",
    "Useful approach: turn each document unto a vector of numbers representing how many times each word occurs in a document\n",
    "\n",
    "A set of documents is known as a corpus\n",
    "\n",
    "1. Collect all unique words in corpus to form vocabulary\n",
    "2. Arrange words in some order\n",
    "3. Let them form the vector element positions or columns of a table (assume each document is a row)\n",
    "4. Count the number of occurences of each word in each document\n",
    "5. Enter the value in the respective column\n",
    "\n",
    "Termed **Document-Term Matrix**\n",
    "\n",
    "Using cosine similarity (vs dot product) will allow similarities to range from -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con of Bag-of-Words\n",
    "- It treats every words as being equally important\n",
    "\n",
    "Finding frequency of words (document frequency)\n",
    "\n",
    "Using the product of two weights: term frequency and inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. Notebook: Bag of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words and TF-IDF\n",
    "Below, we'll look at three useful methods of vectorizing text.\n",
    "- `CountVectorizer` - Bag of Words\n",
    "- `TfidfTransformer` - TF-IDF values\n",
    "- `TfidfVectorizer` - Bag of Words AND TF-IDF values\n",
    "\n",
    "Let's first use an example from earlier and apply the text processing steps we saw in this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kevinwebb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kevinwebb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kevinwebb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The first time you see The Second Renaissance it may look boring.\",\n",
    "        \"Look at it at least twice and definitely watch part 2.\",\n",
    "        \"It will change your view of the matrix.\",\n",
    "        \"Are the human people the ones who started the war?\",\n",
    "        \"Is AI a bad thing ?\"]\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function `tokenize` that takes in a string of text and applies the following:\n",
    "- case normalization (convert to all lowercase)\n",
    "- punctuation removal\n",
    "- tokenization, lemmatization, and stop word removal using `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize andremove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CountVectorizer` (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize count vectorizer object\n",
    "vect = CountVectorizer(tokenizer=tokenize)\n",
    "\n",
    "# get counts of each token (word) in text data\n",
    "X = vect.fit_transform(corpus)\n",
    "\n",
    "# convert sparse matrix to numpy array to view\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 6,\n",
       " 'time': 20,\n",
       " 'see': 17,\n",
       " 'second': 16,\n",
       " 'renaissance': 15,\n",
       " 'may': 11,\n",
       " 'look': 9,\n",
       " 'boring': 3,\n",
       " 'least': 8,\n",
       " 'twice': 21,\n",
       " 'definitely': 5,\n",
       " 'watch': 24,\n",
       " 'part': 13,\n",
       " '2': 0,\n",
       " 'change': 4,\n",
       " 'view': 22,\n",
       " 'matrix': 10,\n",
       " 'human': 7,\n",
       " 'people': 14,\n",
       " 'one': 12,\n",
       " 'started': 18,\n",
       " 'war': 23,\n",
       " 'ai': 1,\n",
       " 'bad': 2,\n",
       " 'thing': 19}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view token vocabulary and counts\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TfidfTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.33144579, 0.        , 0.        , 0.33144579, 0.        ,\n",
       "        0.        , 0.12851912, 0.        , 0.19637646, 0.        ,\n",
       "        0.33144579, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.33144579, 0.33144579, 0.33144579, 0.        , 0.25703823,\n",
       "        0.        , 0.33144579, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.33144579, 0.        ],\n",
       "       [0.        , 0.30858627, 0.        , 0.61717254, 0.        ,\n",
       "        0.        , 0.        , 0.30858627, 0.        , 0.        ,\n",
       "        0.        , 0.11965527, 0.30858627, 0.18283256, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30858627, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30858627, 0.        , 0.        ,\n",
       "        0.30858627, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.39838725, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.15447587, 0.        , 0.        , 0.39838725,\n",
       "        0.        , 0.39838725, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.15447587,\n",
       "        0.        , 0.        , 0.        , 0.39838725, 0.        ,\n",
       "        0.        , 0.        , 0.39838725, 0.        , 0.39838725],\n",
       "       [0.        , 0.        , 0.34599856, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.34599856,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.34599856, 0.        , 0.34599856,\n",
       "        0.        , 0.        , 0.        , 0.34599856, 0.402486  ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.34599856,\n",
       "        0.        , 0.34599856, 0.        , 0.        , 0.        ],\n",
       "       [0.5       , 0.        , 0.        , 0.        , 0.5       ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# initialize tf-idf transformer object\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "# use counts from count vectorizer results to compute tf-idf values\n",
    "tfidf = transformer.fit_transform(X)\n",
    "\n",
    "# convert sparse matrix to numpy array to view\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `TfidfVectorizer`\n",
    "`TfidfVectorizer` = `CountVectorizer` + `TfidfTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.30298183, 0.        , 0.        , 0.30298183, 0.        ,\n",
       "        0.        , 0.20291046, 0.        , 0.24444384, 0.        ,\n",
       "        0.30298183, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.30298183, 0.30298183, 0.30298183, 0.        , 0.40582093,\n",
       "        0.        , 0.30298183, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30298183, 0.        ],\n",
       "       [0.        , 0.30015782, 0.        , 0.60031564, 0.        ,\n",
       "        0.        , 0.        , 0.30015782, 0.        , 0.        ,\n",
       "        0.        , 0.20101919, 0.30015782, 0.24216544, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30015782, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30015782, 0.        , 0.        ,\n",
       "        0.30015782, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.38077552, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.25500981, 0.        , 0.        , 0.38077552,\n",
       "        0.        , 0.38077552, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.25500981,\n",
       "        0.        , 0.        , 0.        , 0.38077552, 0.        ,\n",
       "        0.        , 0.        , 0.38077552, 0.        , 0.38077552],\n",
       "       [0.        , 0.        , 0.30101067, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30101067,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30101067, 0.        , 0.30101067,\n",
       "        0.        , 0.        , 0.        , 0.30101067, 0.60477106,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30101067,\n",
       "        0.        , 0.30101067, 0.        , 0.        , 0.        ],\n",
       "       [0.5       , 0.        , 0.        , 0.        , 0.5       ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize tf-idf vectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# compute bag of word counts and tf-idf values\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# convert sparse matrix to numpy array to view\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have looked representations that characterize an entire document or collection of words as one unit. Insights are document-level.\n",
    "\n",
    "For word-level insights, we need to come up with a numerical representation for each word.\n",
    "\n",
    "Treat each word like a class / column. Similar to bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding breaks down when we have a large vocabulary.\n",
    "\n",
    "Limit word represenation to a fixed-size vector.\n",
    "\n",
    "Finding similarites in word meanings and mapping it out in 2D space.\n",
    "\n",
    "For more on word embeddings, take a look at the optional content at the end of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final stage of the NLP pipeline is **modeling**, which includes designing a statistical or machine learning model, fitting its parameters to training data, using an optimization procedure, and then using it to make predictions about unseen data.\n",
    "\n",
    "The nice thing about working with numerical features is that it allows you to choose from all machine learning models or even a combination of them.\n",
    "\n",
    "Once you have a working model, you can deploy it as a web app, mobile app, or integrate it with other products and services. The possibilities are endless!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular example of word embeddings.\n",
    "\n",
    "Transforms words to vectors.\n",
    "\n",
    "Utilizes surrounding words for context and Skip-gram Model\n",
    "\n",
    "Properties\n",
    "- Robust, distributed representation\n",
    "- Vector size independent of vocabulary\n",
    "- Train once, store in lookup table\n",
    "- Deep learning ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New example of word embedding\n",
    "\n",
    "Global Vectors for Word Representation\n",
    "\n",
    "Tries to directly optimize the vector representation of each word just using co-occurence statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27. Embeddings for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributional Hypothesis\n",
    "\n",
    "Mapped out on a 2D plane, words with common context tend to get pulled closer and closer together.\n",
    "\n",
    "Dimensions can be added depending on the different meanings of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28. t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-Distributed Stochastic Neighbor Embedding\n",
    "\n",
    "dimensionality reduction technique that can map high dimensional vectors to a lower dimensional space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
